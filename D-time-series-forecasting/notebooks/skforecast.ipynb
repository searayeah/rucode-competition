{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from skforecast.datasets import fetch_dataset\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "df[\"service_date\"] = pd.to_datetime(df[\"service_date\"])\n",
        "df = df.drop(\"index\", axis=1)\n",
        "# df = df.set_index(\"service_date\")\n",
        "\n",
        "\n",
        "df[\"sum_price\"] = df[\"sum_price\"].apply(lambda x: abs(x))\n",
        "\n",
        "\n",
        "train = df.groupby([\"service_date\"])[\"sum_price\"].sum()\n",
        "train\n",
        "\n",
        "\n",
        "train_after_covid = train[train.index > \"2022-02-24\"]\n",
        "train_after_covid = train\n",
        "train_after_covid.plot()\n",
        "\n",
        "\n",
        "train_after_covid = train_after_covid.reset_index()\n",
        "\n",
        "\n",
        "train_after_covid.columns = [\"date\", \"forecast_value\"]\n",
        "\n",
        "\n",
        "submission = pd.read_csv(\"sample_submission.csv\")\n",
        "train_after_covid = pd.concat([train_after_covid, submission], axis=0)\n",
        "\n",
        "\n",
        "train_after_covid = train_after_covid.reset_index(drop=True)\n",
        "train_after_covid[\"date\"] = pd.to_datetime(train_after_covid[\"date\"])\n",
        "\n",
        "train_after_covid = train_after_covid.sort_values(by=\"date\")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGkCAYAAADjdyVMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSMklEQVR4nO3deViU5foH8O+ALKKICwqCJFaamgouqbhbmOaSLZapiXnMTosdi985HT2l5nJEM81OWaZmmWlaprZomqK44obiLiKKIMomsg3LDDPP7w9iZJgZmBlm5p0Zvp/r4rqGd73nAea9eVaZEEKAiIiISCIuUgdAREREdRuTESIiIpIUkxEiIiKSFJMRIiIikhSTESIiIpIUkxEiIiKSFJMRIiIikhSTESIiIpIUkxEiIiKSFJMRIiIikpRDJSMHDx7EqFGjEBAQAJlMhu3bt5t8jd27d6N3797w9vZG8+bN8fzzzyM5OdnisRIREZFxHCoZkcvlCAkJwYoVK8w6/8aNGxg9ejQef/xxxMfHY/fu3cjOzsZzzz1n4UiJiIjIWDJHXShPJpNh27ZteOaZZzTbSktL8f777+OHH35Abm4uOnXqhMWLF2PQoEEAgC1btmDcuHEoLS2Fi0t5Hvbbb79h9OjRKC0thZubmwTvhIiIqG5zqJqRmkybNg2xsbHYtGkTzp07hxdeeAHDhg1DYmIiAKB79+5wcXHBN998A5VKhby8PKxfvx7h4eFMRIiIiCTiNDUjKSkpePDBB5GSkoKAgADNceHh4ejZsycWLlwIADhw4ABefPFF3L17FyqVCmFhYdi5cycaN24swbsgIiIip6kZOX/+PFQqFdq1a4eGDRtqvg4cOICkpCQAQHp6OqZOnYpJkybh5MmTOHDgANzd3TFmzBg4aE5GRETk8OpJHYClFBYWwtXVFXFxcXB1ddXa17BhQwDAihUr4OPjg48++kiz7/vvv0dQUBCOHz+O3r172zRmIiIicqJkpGvXrlCpVMjMzET//v31HlNUVKTpuFqhInFRq9VWj5GIiIh0OVQzTWFhIeLj4xEfHw+gfKhufHw8UlJS0K5dO0yYMAERERHYunUrbty4gRMnTiAqKgo7duwAAIwYMQInT57EvHnzkJiYiNOnT2Py5Mlo3bo1unbtKuE7IyIiqrscqgNrTEwMBg8erLN90qRJ+Pbbb6FUKrFgwQJ89913SEtLg6+vL3r37o25c+eic+fOAIBNmzbho48+wtWrV+Hl5YWwsDAsXrwY7du3t/XbISIiIjhYMkJERETOx6GaaYiIiMj5OEQHVrVajdu3b8Pb2xsymUzqcIiIiMgIQggUFBQgICBAZwBJZQ6RjNy+fRtBQUFSh0FERERmSE1NRatWrQzud4hkxNvbG0D5m2nUqJHE0RAREZEx8vPzERQUpHmOG+IQyUhF00yjRo2YjBARETmYmrpYsAMrERERSYrJCBEREUmKyQgRERFJiskIERERSYrJCBEREUmKyQgRERFJiskIERERSYrJCBEREUmKyQgRERFJiskIERERSYrJCBEREUmKyQgREVEdtf9KJqZtPI28IqWkcTjEQnlERERkeZO/PQkAaOLljvnPdJIsDtaMEBER1XHp+SWS3p/JCBERUR3nIpP4/tLenoiIiKQmg7TZCJMRIiKiOk7GmhEiIiKSkovE2QiTESIiorrO0WpGDh48iFGjRiEgIAAymQzbt2+v8ZyYmBh069YNHh4eePjhh/Htt9+aESoRERFZQ6ajjaaRy+UICQnBihUrjDr+xo0bGDFiBAYPHoz4+Hi88847ePXVV7F7926TgyUiIiLLO5l8T9L7mzzp2VNPPYWnnnrK6ONXrlyJNm3aYOnSpQCADh064PDhw/jkk08wdOhQU29PRERETsbqfUZiY2MRHh6utW3o0KGIjY01eE5paSny8/O1voiIiMh6Mguka6qxejKSnp4OPz8/rW1+fn7Iz89HcXGx3nOioqLg4+Oj+QoKCrJ2mERERHVacnaRZPe2y9E0M2fORF5enuYrNTVV6pCIiIjISqy+UJ6/vz8yMjK0tmVkZKBRo0aoX7++3nM8PDzg4eFh7dCIiIjoL1JONWL1mpGwsDBER0drbduzZw/CwsKsfWsiIiIykhDS3dvkZKSwsBDx8fGIj48HUD50Nz4+HikpKQDKm1giIiI0x7/++uu4fv063nvvPVy5cgVffPEFfvzxR7z77ruWeQdERERkEiEEXl13SmubQ9WMnDp1Cl27dkXXrl0BAJGRkejatStmz54NALhz544mMQGANm3aYMeOHdizZw9CQkKwdOlSrFmzhsN6iYiIJHIlvQB7L2t3oZBy5V6T+4wMGjQIopq6HH2zqw4aNAhnzpwx9VZERERkBWUq3ed4h5aNJIiknF2OpiEiIiLrEdBNRrzcrT6mxSAmI0RERHWMlJ1V9WEyQkREVMfYWS7CZISIiKiuqa7vpxSYjBAREdUx9pWKMBkhIiKqc+ysYoTJCBERUd1jX9kIkxEiIqI6hjUjREREJCk7y0WYjBAREdU1rBkhIiIiSantLBthMkJERESSYjJCRERUx9hZxQiTESIiorpG30J5UmIyQkRE5MRu3pWjTKXWfH8jW47/bD0vYUS6mIwQERE5qV/P3sbAJTH4+/o4zbZxq44h+W6RhFHpYjJCRETkpNYcug4AiL6SqdmWnl8iVTgGMRkhIiJyUjKpAzASkxEiIiJnJXOMdITJCBERkZOqmopcyyyUJI6aMBkhIiJyUpUrRo5dv4vwZQekC6YaTEaIiIiclEulbOSlVcckjKR6TEaIiIiclGP0GGEyQkRERBJjMkJEROSkHGQwDZMRIiIiZyVzkIYaJiNERETOyjFyESYjREREzsqFyQgRERFJic00REREJCl2YCUiIiJJMRkhIiIiMgKTESIiIifFPiNEREQkKTbTEBERkaRkDpKNMBkhIiJyUo6RijAZISIicloOUjHCZISIiMhZOUguwmSEiIiIpMVkhIiIyEmxAysRERFJigvlERERkcQcIxthMkJEROSkHKSVhskIERGRs3KQXITJCBERkbNizQgRERGREcxKRlasWIHg4GB4enqiV69eOHHiRLXHL1++HI888gjq16+PoKAgvPvuuygpKTErYCIiIjKO067au3nzZkRGRmLOnDk4ffo0QkJCMHToUGRmZuo9fuPGjZgxYwbmzJmDy5cv4+uvv8bmzZvxn//8p9bBExERkWEuDtL+YXKYy5Ytw9SpUzF58mR07NgRK1euhJeXF9auXav3+KNHj6Jv374YP348goOD8eSTT2LcuHE11qYQERFR7ThlzYhCoUBcXBzCw8PvX8DFBeHh4YiNjdV7Tp8+fRAXF6dJPq5fv46dO3di+PDhBu9TWlqK/Px8rS8iIiIykWPkIqhnysHZ2dlQqVTw8/PT2u7n54crV67oPWf8+PHIzs5Gv379IIRAWVkZXn/99WqbaaKiojB37lxTQiMiIqIqHCQXsf5ompiYGCxcuBBffPEFTp8+ja1bt2LHjh2YP3++wXNmzpyJvLw8zVdqaqq1wyQiInI6jrI2jUk1I76+vnB1dUVGRobW9oyMDPj7++s9Z9asWZg4cSJeffVVAEDnzp0hl8vx2muv4f3334eLnt41Hh4e8PDwMCU0IiIiclAm1Yy4u7uje/fuiI6O1mxTq9WIjo5GWFiY3nOKiop0Eg5XV1cAgBDC1HiJiIjISI5RL2JizQgAREZGYtKkSejRowd69uyJ5cuXQy6XY/LkyQCAiIgIBAYGIioqCgAwatQoLFu2DF27dkWvXr1w7do1zJo1C6NGjdIkJURERGR5jrJqr8nJyNixY5GVlYXZs2cjPT0doaGh2LVrl6ZTa0pKilZNyAcffACZTIYPPvgAaWlpaN68OUaNGoX//ve/lnsXREREpMNR+ozIhAO0leTn58PHxwd5eXlo1KiR1OEQERE5hMjN8dh6Js2oY5MXjbD4/Y19fjvI3GxERERkMseoGGEyQkRE5KyccgZWIiIichwO0mWEyQgREZGzcpTRNExGiIiInBSbaYiIiEhSeiY5t0sOEiYRERGZjjUjREREJKEfTqRIHYJRmIwQERGRpJiMEBERkaSYjBAREdkBlVqgTKWWOgxJMBkhIiKSmBACQ5cfxMAlMXUyIWEyQkREJLHSMjWuZRYiLbcYt3NLpA7H5piMEBER2REBIXUINsdkhIiISGJZBaWa16Lu5SJMRoiIiKQ24n+HNK/rYC7CZISIiEhq+SVlmteiDlaN1JM6ACIiorqiTKVGSZkaDT0MP34tkYqUKFVaTT/2jskIERGRjYz87DCupBfg1Afh8G3oofeY3CJlre/z1KeHcCNbXuvr2AqbaYiIiGzkSnoBACAmIcvgMR/tulLr+zhSIgIwGSEiIrI5tVqgoESJYoVKZ9+dvPvzjJSWqbA/IRNFijKd4wxd90JansXitBUmI0RERDZWWFqGzh/+ic4f7tbZ5yK7/zpq5xVM/uYk/vFDvFHX/fJAEkZ+dthCUdoOkxEiIiIbqFy7UdGMUqYWOqNnku8WYX9CJgDg26PJAIC9lzOMusfKA0kWiNT2mIwQERHZwO9n72heV55lVa1n+Mzkb05CrW9HDVxkspoPskNMRoiIyOkUK1RmPcytSanWvwDe1YwCvdt/O3fb5Hu4OGYuwmSEiIicy93CUnSYvQtjVh6VOhQtlXOjnefTNa/jbt7Te/zui+l6t1eHNSNERER24M9L5f0rTqfkShtIFZX7huTIFZrXagMzrqYZWL33UGIW+n+0D0euZevsyy+p/RwlUmAyQkRETsVeZ1NXGWg2MrTdUDPTxK9PIDWnGBPWHNfZp1TZ6ZuvAZMRIiIiGzDUhcVQMqJUafcx+fXsbchLjZtvxNFwOngiIiIbMLQAnrxUd+IzAOj6QBPNjK0A8I8fzuCpTv5WiU1qrBkhIiKnIiyy1JzlGeob8seFO3q3/3AiRc+xhju1GqphcQRMRoiIiGxApX9kLwIa17fI9R1pld6qmIwQERHZgKGaEUstaueoc4wATEaIiIgkZbEVdpmMEBER2Qd7HdprbTIHzkaYjBAREVmJEALxqbnIK7LOZGSV5yJx0MlXAXBoLxERORl7qhj58VQq/v3zeatdP/pKJoZ09APg0K00rBkhIiKyFmsmIgCQXKm/iaOuSwMwGSEiInJYxcr7E6Y5cC7CZISIiMhRLdtzFfsTMgGwAysREZH9qGPDaSZ/cxKA/c48awwmI0RERCQpJiNERORUHLd+oHYupOVLHYLZmIwQERE5gZe/Pi51CGZjMkJERHVOYkYBMvJLrHb9EqUK41cfs9r1nQ0nPSMiojolI78EQz45CABIXjTCKvfYfDIVR5PuWuXazog1I0RE5FSqDqYRVTZUXphOpbZOD5PK839QzcxKRlasWIHg4GB4enqiV69eOHHiRLXH5+bm4q233kLLli3h4eGBdu3aYefOnWYFTEREZKw/zt9B6Lw9OJSYpdnm6eaqea1UqTWvS8ssl0A47owf0jA5Gdm8eTMiIyMxZ84cnD59GiEhIRg6dCgyMzP1Hq9QKDBkyBAkJydjy5YtSEhIwOrVqxEYGFjr4ImIiKqqXBPyxobTyCtWYuLX9/9pTs8r1jnnZHIOHvlgF97bchaJGQW1juHrwzdqfY26xORkZNmyZZg6dSomT56Mjh07YuXKlfDy8sLatWv1Hr927Vrk5ORg+/bt6Nu3L4KDgzFw4ECEhIQYvEdpaSny8/O1voiIiCzh/W0XNK9/Pn0LADDvt0sAgB9P3cKQTw4iPc/0zq2pOUX48VQqlCo1MgtKLRNsHWFSMqJQKBAXF4fw8PD7F3BxQXh4OGJjY/We8+uvvyIsLAxvvfUW/Pz80KlTJyxcuBAqleHqsKioKPj4+Gi+goKCTAmTiIjIoLtyheZ1RWJSdV2XBDNqRwYu2Y/3tpzDmkO2rxVRW6nvi62YlIxkZ2dDpVLBz89Pa7ufnx/S09P1nnP9+nVs2bIFKpUKO3fuxKxZs7B06VIsWLDA4H1mzpyJvLw8zVdqaqopYRIREZlEViUb+c/W8zodX2tSkQ8cTcq2VFhGK1SU2fyelmT1ob1qtRotWrTAqlWr4Orqiu7duyMtLQ1LlizBnDlz9J7j4eEBDw8Pa4dGREROyJw6gqodTtNyi3HpTj4eDfAx/f4SVFLkFCpqPsiOmZSM+Pr6wtXVFRkZGVrbMzIy4O/vr/ecli1bws3NDa6u93svd+jQAenp6VAoFHB3dzcjbCIiIsup2kwDAKVlat2NRpBiwbpvjjh2h1mTmmnc3d3RvXt3REdHa7ap1WpER0cjLCxM7zl9+/bFtWvXoFbf/6FevXoVLVu2ZCJCREQWZ6hmIimr0OA5LnqyEVObaWq6Pxlm8miayMhIrF69GuvWrcPly5fxxhtvQC6XY/LkyQCAiIgIzJw5U3P8G2+8gZycHEyfPh1Xr17Fjh07sHDhQrz11luWexdEREQ1UFRT06FvXhAmFbZjcp+RsWPHIisrC7Nnz0Z6ejpCQ0Oxa9cuTafWlJQUuLjcz3GCgoKwe/duvPvuu+jSpQsCAwMxffp0/Pvf/7bcuyAiIqpBdcmFvmYac3ORxEzDNTCkn1kdWKdNm4Zp06bp3RcTE6OzLSwsDMeOccEgIiKSjrqabKTqaJrayOIcIybj2jRERORUDKUcecVKvdsPJWbhxI0c3euwmcZmmIwQEVGdMGHNceTIdYfAVp4qvjJzO7CS6ZiMEBGRU6kuieg2f4/x17FEMGQUJiNERER6sGLEdpiMEBER6VF18jI221gPkxEiIiJ9KuUec3+7iN5R0Xr7nFDtMRkhIiLSo3I9yDdHkpGRX4rvYpOlCsepMRkhIiKncvNukUWuU9Eqk55Xotkm0ztXq/Saezv24rJMRoiIyKmsP3bTItep6DPy4lexmm1XMwsscm1LCw1qInUItcJkhIiISI+KmpGUnPs1LTvO3ZEoGufGZISIiEiPHLkCSpXhxfXsicrBR/owGSEiIqehVlvuofzO5ni0ff8Pi13Pmub/fknqEGqFyQgRETkNR68hMNc1B18pmMkIERE5DZUFa0bIdpiMEBGR07BFMrJk9xWr36OuYTJCREROwxbNNCv2J+ls234mzer3dWZMRoiIyGlYsgNrdaquU/P14Rs2ua+zYjJCREROw1Z9RqrexsU+J2Z1GExGiIjIadhqNI26yn3O3sqzyX2dFZMRIiJyGrarGeGoHUtiMkJERE7DVslI5VwkI7/E8IFklHpSB0BERGQpahvN3l6R9NzIluPbI+y8WltMRoiIyGnYus/I4I9jbHI/Z8dmGiIichq2aqYpLXOMBfQcBZMRIiJyGrbqWPrvLedscp+6gskIERE5DVsNcom+kmmbG9URTEaIiIhIUkxGiIjIaQhw/g9HxGSEiIiIJMVkhIiIHN7lO/l4d3M8Uu4W2eyecTfv2exezo7zjBARkcN7+vPDUKoEtp1Js9k9DySwE6ulsGaEiIgcnlJl+74iNprSpE5gMkJERGQGLpZnOUxGiIiIzMBUxHKYjBAREZlBzXYai2EyQkREZAauT2M5TEaIiIjMwD4jlsNkhIiIyAxMRiyHyQgREZEZVGylsRgmI0RERGYQrBmxGCYjREREZmAzjeUwGSEiIjIDR/ZaDpMRIiIiM7BmxHKYjBAREZmBuYjlMBkhIiIyg4rtNBbDZISIiMgMKlaNWIxZyciKFSsQHBwMT09P9OrVCydOnDDqvE2bNkEmk+GZZ54x57ZERER2g0N7LcfkZGTz5s2IjIzEnDlzcPr0aYSEhGDo0KHIzMys9rzk5GT885//RP/+/c0OloiIyF6oOemZxZicjCxbtgxTp07F5MmT0bFjR6xcuRJeXl5Yu3atwXNUKhUmTJiAuXPn4sEHH6xVwERERPYg+a5c6hAs5t/D2kt6f5OSEYVCgbi4OISHh9+/gIsLwsPDERsba/C8efPmoUWLFpgyZYpR9yktLUV+fr7WFxERkT25kl4gdQgW8a+hj+CNQQ9JGoNJyUh2djZUKhX8/Py0tvv5+SE9PV3vOYcPH8bXX3+N1atXG32fqKgo+Pj4aL6CgoJMCZOIiIiM5CKTSR2CdUfTFBQUYOLEiVi9ejV8fX2NPm/mzJnIy8vTfKWmploxSiIiIpJSPVMO9vX1haurKzIyMrS2Z2RkwN/fX+f4pKQkJCcnY9SoUZpt6r96/NSrVw8JCQl46CHdqiEPDw94eHiYEhoRERGZQWkHyw+bVDPi7u6O7t27Izo6WrNNrVYjOjoaYWFhOse3b98e58+fR3x8vObr6aefxuDBgxEfH8/mFyIiIoklZRVKHYJpNSMAEBkZiUmTJqFHjx7o2bMnli9fDrlcjsmTJwMAIiIiEBgYiKioKHh6eqJTp05a5zdu3BgAdLYTERGR7fnUd5M6BNOTkbFjxyIrKwuzZ89Geno6QkNDsWvXLk2n1pSUFLi4cGJXIqK6KjWnCKdT7mFUlwC4uEjfOZKqZw/T2pucjADAtGnTMG3aNL37YmJiqj3322+/NeeWRETkIPp/tB8AUFqmxos92Bxv7+whGWEVBhERWcWJGzlSh0BGYDJCREROiw00jsEeFvxjMkJERFSH+DbUnjpDzZoRIiJyVnYwsSfpMa6ndj8elfS5CJMRIiKiukzNZhoiInJWP566hWuZ1U+o9UXMNbzxfZxddKKsq9hMQ0RETu317+Oq3f/RrgT8cSEd+69k2igiAoC5Tz+qeV3GZISIiJxZel6JUceVlKmsHAlVNqlPsOY1a0aIiIgA7Dx/B/LSMqnDqJM4tJeIiJyasQNqdp5Px3tbzlk1FipX9WdiD/11mIwQEZHVmDK8d8f5O9YLhAziaBoiInJqMjMnGylTqfH054cxfdMZC0dEVQU2ri91CExGiIjIevJLlCYdL/76L/1k8j2cu5WHX+JvWyMsAvDD1N54rlsgZj7VQepQzFu1l4iIyBimtgAcTMzGwHbNNUkJWU/YQ80Q9lAzqcMAwJoRIiKysjMp94w+9s+L6ciRK8BUxIrscJ5+JiNERGRVK/ZfM/rYDcdT0G3+Hq0RHiM/O4T1sclWiIzsBZMRIiKyKnM6sf529n5fkQtp+Zj1y0VLhlS32WETGJMRIiKyqj2XMkzuyJqSU6SzrVjBWVotwcWFzTRERFQHPbviiEnHu+p5YK45dN1S4dRpbq729+i3v4iIiMjpJGXJTTpeX8tOdmEpAKCwtAwHrmZBqVJbIjS78WDzBja5j75ET2pMRoiIyO646MlGKno6TPn2JCatPYFP9ybaxSJvlrLv/wbZ5D71mIwQERGVq24uEX2dXs+k5AIAjt/IAQB8vv8aus7fg0u3860Sn7Oyx2SEk54REZHNfRadiIvVJBH6npfn0/J0tuUVKzFjKxfYM0X31k2lDkEHkxEiIgndkyvQ2MvN7DVcHNXSPVer3a+vmcYQe1jozZF0buUjdQg62ExDRCSRg1ez0HX+Hry3hf/ZV3U7t9joY52sH2udxGSEiEgiy/eW1w78FHdL4kjsz5X0AqOPLVKUWTES23CvV/44jvnnIGkDkQiTESIiqrVj1+/ielah1e+jb/TMzbu6E6Q5mh+m9gYABPvaZnivvWGfESIiiThLT4drmYV4adUxAEDyohFWvdeui+lWvb5UbDXApX9bX9vcyERMRoiIJOIs/S6vpJs2tPZqhvFNMFWl6pkmnvRzkQEVFUlxH4Rjx/k7eDokQNqgDGAzDRGRDV3LLETE2hOIu3lPa3tekWlrt9iTyivsfnPkhsHjKppYhi4/aPa9ypxokjNreqVPMCKHtNN836yhByLCgtHYy13CqAxjMkJEZEOvrT+Fg1ez8PyXR7WaaaL+uCxZTLVVuYZn7m+XDB5XMQS3NjVCZSomI8b48OlH4Uh5G5MRIiIbupNbcv+bSk/lG9mmrd1iT4yd58MSD0cPN+d5bK2f0tOq11c5UDbiPD9VIiIHUHmRsjt59xMTR57zbH9CllHHWWJyskf8vWt9DXvxWLD+mVAbe7lZ5PqONBkckxEiIhuqPGoit9jx+olcvJ2Hf/xwBimVhtP+dva2Ueda4uGocpJmmtcGPGhw33d/s0yNiSMlIxxNQ0RkQ5VrRionJo5Soz7if4cBlI+I2fXOAJPOtcR7VDjodKsBPp64Xakm7IXurbT2V14OoEurxha5p6P8TgGsGSEispkSpUorGXGt9AAytILt1YwCZBWU6p3sS0pJWYVQq4VJs5/evFv7fjFKB01GVFV+vlV/mtWtYGwue/udqQ6TESIiG3hn0xm0n7UL2YUKzTa5QqV5natnaO+p5Bw8+clBPPbfvXjl25M2idNYMsjwwlex6Dh7t9HnHL+eU+v7OupomsGPtNDZZu1+QhPDWgMARofa59wilTEZISIyg7y0DB/+ehGnko17wG6Pr75fRWKm7lTqY1bGal4fvGpcJ1FbkcmgM1dKTeb9fqnWNQBlaserGflqYnfMGtlRa1s9K065+vn4rgCAVk28kLBgGJaPDbXavSyFyQgRkRk+2XMV3x5N1koYLEleat+Lv5nbOTLNhNV49XHESc+e7OiHBh7aXTTbVFmDxtWCyUlLn/qa1x71XLX6o9grJiNERGa4qqcmw5Ke//Ko3u22WpCuJkozm0v0NUeZwpHmzqigLxmous2SyYgD5B46OJqGiMgMKis2F1y8nYcr6brrt3SasxuFf9WYWHtBOmsZ+dnhWp3vqH1GamLRZMRiV7Id1owQEZmhcs1AZn4JNh5PMWlkiT6L/rgCANh2Ok3v/kI7b7qxBUcd2quPDJWHeVsuhbDktWyFyQjZpasZBVix/xpKlKqaDyaSQOXmgudXHsV/tp3H/N9rt77MygNJdt9XRGrrY29KHYJVWLJmxBExGSG79OQnB7FkdwL+F50odShEet0ruj9ENzWnvFPmvisZtb6uo86jYSu17QBrayO6tDS4T1SabcTVyNqMh5rf7/i6/a2+aOnjqXNM1TlNHAGTEbJrZ2/lSh0CkV4lCuNr7SpPnV4TRxwtQoYtfSHE4L7K3Y6MrRkJbOKled3Qox52/KO/nus63u8QkxEiIhMJIbSm9q7Jq98ZP2GZIz5IyDBPN1fN62f+mnwsvIPfX/tc0KpJffg2dNep4fhoTJcar+0iA5o2cNfZ7ogJrVnJyIoVKxAcHAxPT0/06tULJ06cMHjs6tWr0b9/fzRp0gRNmjRBeHh4tccTEdm7k8nGTfZVolRh35UMXM0wfihumVroTBVOzmHOqEex6LnOWP5SKIDy4b0x/xyE2JlPoJ6r9uN4TLdWGPqoH94a/JDWonqD2jXXvDbUUdURE1qTh/Zu3rwZkZGRWLlyJXr16oXly5dj6NChSEhIQIsWutPdxsTEYNy4cejTpw88PT2xePFiPPnkk7h48SICAwMt8ibIeTlg0yfVAfsTMo06rv2sXSZf+0p6vsnnWJoQAgkZBWjj2wAe9e7/Z1/G/iy10qSBO17q+YDWtqpJSAUXFxm+mtgDAFBapoJfI080cHfFyJAAzPv9EgDDTTt1os/IsmXLMHXqVEyePBkdO3bEypUr4eXlhbVr1+o9fsOGDXjzzTcRGhqK9u3bY82aNVCr1YiOjq518EREtiKEwL9+OovP9yVa9aH8t29PST5PxC/xtzFs+SFMXKNdi/23dackiqhu86jniin92ugkMob6vDrixHAmJSMKhQJxcXEIDw+/fwEXF4SHhyM21rgpkYuKiqBUKtG0aVODx5SWliI/P1/ri+omB0zwyUmdSc3FT3G38PGfVw32FylR3k9SrLEKq61sOF4+fPZElXV37G19nLqocqJR0UwT0spH6xhzp+qXkknJSHZ2NlQqFfz8/LS2+/n5IT093ahr/Pvf/0ZAQIBWQlNVVFQUfHx8NF9BQUGmhElOhK3nZC9KKyUaO87d0XtMXrESCX/NnHowMdsmcZlr6+lbePGrWEzbeBoXb+dp7XPAZ5lZmni5SR2Cybzc7zebVXRe3fpmX61jHHGWWpuOplm0aBE2bdqEbdu2wdNTd2x0hZkzZyIvL0/zlZqaasMoiYh0GdtPZOjygwCAdUeTzb6XLR4lkT+exYkbOfj93B2M+F/tpmi3JW8Py61isn5KL4tdy1bcXF1w+N+Dcei9wZqROlX7jjzi7y1FaLVi0k/V19cXrq6uyMjQntgnIyMD/v7+1Z778ccfY9GiRdi7dy+6dKl+yJKHhwc8PDxMCY2IyKpWHbxu9LH7rmRg3xXjkhd7pC8ZOpNi3Agiayuo5Qy1L/d+AN8fSwEAdAr0qeFo+9Sq0lwj+rRu1qDa/fbIpJoRd3d3dO/eXavzaUVn1LCwMIPnffTRR5g/fz527dqFHj16mB8tEZED+Nu3tevoKXUH1ribuonHs1/oX0XY0Rg70ynZlsn1XZGRkZg0aRJ69OiBnj17Yvny5ZDL5Zg8eTIAICIiAoGBgYiKigIALF68GLNnz8bGjRsRHBys6VvSsGFDNGzY0IJvhZxRXWm7JqpMysXgrmXqzoniyJ1xq+oY0EjqEEgPk5ORsWPHIisrC7Nnz0Z6ejpCQ0Oxa9cuTafWlJQUuLjcr3D58ssvoVAoMGbMGK3rzJkzBx9++GHtoien5zwfgeTIlu+9atP7JWUZP0maqVRqgde/jzO4PyVHrrPt68M3rBaPrfV5yBefj++Kh5rzn2F7YlZPoGnTpmHatGl698XExGh9n5ycbM4tiIjsxvK9zrNg447zd7Dnku6CfrlFCjT2coeri27r/Sd7bJuMWVOTBu4Y2SVA8/2/hj6CJbsTJIyIAK5NQ0Sk43TKPaw7mgwhBApr2WHSHDIr9hrJMDBHyupD5R106+mZ1bO+u+VGsEjlhe6tsPXNPmhYZTTOW4MfxsBKU6wb4+EWrFWxNMf/DSMisrDn/uqs2dLHU5ImCmv2sfzvzst6t6/Yn4R2ft64dEd3kkl3V8fv9BkS1BjdHmiid9+yF0PQfcFeo6/VOdBHb98aMh9rRsi+sdOIXTqbmos5v1xAXpFS6lCs6nq2HMdv5NR8oIXJJBrxMX1TPL46oDuE2dUJkpHqNGto/FQS29/qC083w4/OF7q3skRIJqu66q+jYTJCdo0zsNqn0SuOYF3sTfx35yWpQzFZjlyBXRfuQGnEiBWpBpEUlthXkqd20PXx/vlkO81rS+V3oUGN9W73qOeClS93x4yn2lvmRib67m89MeiR5tj+Vt+aD7ZDbKYhIrMlOmBV9fNfHsWNbDneDW+H6eFttfYJIbRqJX6JT7N1eACA0ym5Fr/mtcxCZObr7y9SHbVaOORaJ0B5f5CP/7RG51vdzObSvGEGV9G1hbZ+3vh2ck/J7l9brBkhojrlRnb50NU/LmivL7PrQjpC5+3BgUqLwaXmFNk0NmtRqwXClx3A+DXHTT63zIGTkcqJZU2dgs9/+KTR11XrWRVXykTEGTAZISKzCVFem/D+tvP41MGHv77+fRzyipWYtPaEZptcoZIwIst54SvjVlXXRy0EJJyDzWwv937ApOO9Pd1wfeFwo45VOmq7lR1jMkJ2zUH/IatTrqQXYMPxFHxi5YnBhBCIu5ljsU6zLnVoWnB907sbS6UWDjkD6zvh7Wo+qAoXFxk+G9cV/dv66t0f8ld/EUdcFdfesc8I2TX+yds3mQwoUdqm9mD3xQy8/n0cWnh74MT74bW+nrPnImq1wI7zd1BWy//i7xYqHLKZxtyQR4UEYFRIAIJn7NBse2vwQ2jn541hncoXhK1tmZIuJiNEhHtyBc6l5aH/w75wsdO2790Xy9e1yiwohaJMDfd6uhW7CekFaNLADS287w9zVKrUOJp0F91bN9Ga8MrZk5F1scmY+1vtRzvN33EJerpIOJzajMz711DtETJBNayaS6ZjMkJ2zRGrhx3RyM8OIy23GPOf6YSJvVtLHU6Nxq6KxXPdWmnFevOuHEOXHwQAJC8aodnebf4eFJSU6Wy35iyntlB15E9VlkhEAGDPpQw08nS8R4VvQ3et7039KGnWwB135Qp8MKKDzr63n2iLwtIyXM0owMlk3SYwFxmcIoGzJfYZISKk5RYDAP44f6eGI3UZ85mbV6REuoFpyM1xJiUXs7ZfwLlbuZpt8am5OsddyyzQJCIAUFypQ6qdVgAZ7da9Ypvdy9H+J+jf1rfWE8f98U5/rHy5G17pE6yzr6FHPfz32c7o97D+aeTb+XkD0D+1PunHZISIDDqTcg9TvzuF5Gw5jl7LxrdHblRbW2VoX8i8P9E7Khq5RQqLxnflToHmddV+DUWKMoQvO6i1rfJqtWdv5aG0zHFHy+y9rLvYnbU4WC5ikeSphbcnhnVqiXquhh+T9QzMTLs6ogee6xaI397uV/tA6gjHq3ujOsXRPgSdzbN/rdGSdq9Ys2ZJO39vg8cLUX1fjMTMQjwW3NSiMVao3KdQXlqmd6XZynOIAMAjH+zCofcGI6ip4/UBUNmwHcDRmksraiYqs8Y7cDOQjAQ19cKyF0OtcEfnxZoRIqpRRTMOUJ6YVFb547jiA/9uYSlOp9zDmkPXsePc/aafLadumR3DTj1NSO/9fA5lf02C8X8/ndVsf3TObqwxcoG7/h/tNzsmKSmrGV56/laeRe/laP0fIp80fVivOVr61LfJfeoC1oyQJOSlZTiadBf92/rC083V4HEO9g+ZwzuadFfv9rzi+3N7VG2Lr/wjUgsBoRIGV0DdfCoVi8d0MSu20jL9wykPJmbh8fZ+Zl2zQlKW401rr6pmeOmozw9b9F6OtkZU5VFTGlb4MBnRuSXOpOSiW+vGFr92XcOaEZLE9E1nMPW7U5j9ywWpQyETRVfTV2H3xXTNdOu1VVqmwqYTKVq1Mvqo1YDCQKJirNqeL4XqakYsrUTpeOVjCy4uMswe1REjuwRIHYrDY80ISWLv5UwAwI+nbuGjMSESR0Om+ONCusF90zaeMelaQgh8vu8a2laaUAoALt7Ow+jPj6BMLeDtUQ/n5w41eI1Xvztl0j31qV9N7Zy9smWfEWfA0rJvTEbIrvEDxPr0LfplK7FJd7H0r46mFXOAqNQCI/53v5mhoLRM77mW5Ii/Z1wfRb83Bj2kdzubfO0bm2mI6riq81Xczi1GQYlx67/cLVTgw18vmnS/387e1rzOKNCde+RMiu4kUoVWTkgu/zVSyJGouD6Kjqc6+ePfw9rXfCDZHdaMkH3jvzNWU1CixJ5LGYj88azW9j6L9gEALs8bVuM1UnKKkJJj2n3f/uEMHmreEB0DGmltv5Kej0aebnonq+r7V0zW8uaG01a9vjXom+StrvPQs0QAOQYmI2TXmIoY70p6Pn48eQtvDX4IzRp6IEeugLy0DJ5urmju7aFzfPiyA8jILzV4PWsOeb15V46OAY20cs1hyw8BKJ+Gu6rKo3mo3KlarMRbF7lVM3kZSY/JCJGTqHiYp+UWYeXL3dFt/h7NvulPtMW7Q+7PvVCsUFWbiABAdmH1+2vji5gkPNW5Jb6ISdLZd1du2Vlaqe7Q98/Lu+HtcCgxC891C7R5PGQ8popETubi7XwMXBKjte3T6ESt7ytPiy6F82nlk3Jdy3S8+T3snSMOU7am6eFtseWNPtXOZ0TSYzJCdi8ttxhTvj2Jo9eypQ7FIeTIFUjJKTK4Pz2vRGdadClk6um8SrX3abTuNPhE9o7JCNk1IYB/bzmH6CuZGL/muNTh2K3KE40VKapf/K13VLS1wzHK2VTLTllO5X6OS5M6BCKTMRkhuyYgcCfPdkulO6rBH8dIHYLJ0vNZM2INchvMy2KvOPjOcTEZIbunb6gnlcsrVqLLh7ulDsMss7ZzKQBrsMUkcUSWxmSELOpCWh7+s+28RUdiuDAXMaj/4n3ILzHu4RN3Mwf3OFLF6bnyD4YcEIf2kkWN/Kx8Gu+sglKsjuhhkWvKwA9XfeSlZUYnIgBwPUuO3CLO1+FMhBA6NYd1+a+FrTSOizUjZBWJGQUWuY4QAFtp9CswIREBgH9tOYcyLq7mVOR6Oiur2HGCHBCTEbJr5ckIsxF9SsuqHzWjz0+nUq0QCUnl60M3dLYxFyFHxGYaMsn+hEwUlJTh6ZAAm9zvkgMuYGYsIQSEAFyqtPFnFpQgt0iJ+NRcHEu6i8Vjuuidytqcya32Xs40O16yPxxpRs6CyQiZZPI3JwEAjwU3QUuf+gaPY21G9YQQGLf6GPKKy/D72/00nQ7LVGr0/K/2PCDXs+X4elIPNGt4f32ZEqUKm06aXsvRoWUjh1yhlvRjsxs5CzbTkNFEpfrfu4UclQEA527lYu+lDJPPKy1T49j1HFy+k49b94pQrFBBUabGtSzd6dHjU3Px2nrt6dvn/nYJXx/WraKvSWhQY5PPIfulqpKMqJmckINizQgZrXJbtJoN0wCApz8/AgDYGzkAD7fwNvq8yrOk3s4t0awl8/7wDnqPj6u0Qmt6Xgl+OJFiRrTAb2dvm3Ue2SelSrupboOZvxfOQvBzyWGxZoSMVjkBqfofmaNLzyuBvLQM644mm9UOf/Ou4bVg9Knc+XTc6mOa1//dedngOR1m7cKV9PxaTedeyAmxnEqZSvvvsK5PJOdcn0p1C2tGyGhqof+1o9t25hbe3XxW8/2cXy9ict9gzBn1aLXnVU7IytQC72w6g94PNsNLPR8weM72M2nIK1Zizq8XTY6zWKnCsOWHTD6PnNeAds01r785YnqzHZG9YDJCRqtcM1JTdai9dl8tVqhwJvUeMvNL0SnQBw+3aIionVd0jvvmSDJmj+yo6YhbplLj0p18tG7WAPN/v4RipQo7zt3RHP/x7gQkZhZie/xtqITA+J4P6HTi3XXhDt7ZHG/V90d1i5e7KwAgv0SJub9dkjgaXRN7t0bnQB+89/M5qUMhO8dkhMwiVc3IkWvZ6PNQM82D/k5eMRrXd0f9vz6U9amYpTK7sBQ9FuzV2pe8aITBSdXavv8HLs8fhi9jkrBsT/XLsidm3u94+v62C3h/2wV8NKYLXuwRhCPXsjGBKw6TFRy8moW+D/vi3K1cqUPRa9bIjjhxI8dm92vVxPAIP7Jv7DNSRxSUKHHgapZOhzdTVK4Z2XQiRZLOYhPWHMcfF9IBADfvyhEWtQ+DPt6v91ghBMpUarSZuRNjvjyKyB/P6hyz/0omMvL1r6NTphaYsu5UjYmIIe9tOQchBBMRspqtZ9Lw2H/3aiXD9sS9ngv6Ptys2mO6PdC41vfp2LIRxvUMwtuPt631tUgaTEbqiCnfnsKktSfw+b5rZl+jcm3I1jNp2Hm+PCn45sgNrDuarHXs9Wy50TOEqtUCijI1jlzLRomy5nPe3HAawP0JvPQlE0IIPP/lUTz8/h8AgFM37+Hg1Syd4yZ/e7Lae+k7xxQViRORNS36Q7ep0ZY+fSkUIa188FzXQM22iiRDJpPh6oKnEODjqXPeByM64IfXeuOP6f0129zr6T6WkhYOx6H3ButsXx3RA5+P74od/+iHqOe6oKEHK/sdFZOROuJEcnlV6afRibiQlmfy+dmFpXjq04Na206n3ENeUXlb9ZxfL+qM1Fi44zKyC0ux/thN5JcYXqBty+lbWLzrCiasOY7IH+ONimfeb5ew9M8EzffBM3YgeMYOzfe/nr2N0ym5Rl3Lmnacv1PzQUQObnRoIH6Z1g/LxoZqVtle+XJ3zX73ei44OvMJbHqtN6Ke66zZ/mr/B+FRzxUdWjbC/8Z1RXt/b+x+ZwAm9m6tOaaxlxtcXWQIauqlc98hHf0wsksAJ1l0AkwjnZBaLVBapsbNHDk+2HYBrw14UGv/yM8OY2yPIBxKzMKeyIFoUOm/idScIvj7eMLN1QVlKjXkChVC5v6p9z4lShVKKtV+lFVpAvr+eArOpObi3K08HEu6ixUTuuldZfS9Lfc7t1XUttRkrYGRA7+evY2sglLM/90+OvNV7uRKVBfEz3kShSVlaNFItyak94PN0PvBZki+K4d/lf1PhwToXWZi0XNdrBYr2Q8mI07kdMo9fHUgCbsvas8IeqrK7J0AsPmvBdMenbMbV+YPw4tfxeLcrfs1JiO6tKzxQaoW2hOhxSbd1dqvUgvNNXecv4NJN3Lw4lexNb6Pa5nmr/j7jx/OmH0uEdVeI083NPJ0q/aYmU/pn9yvwvPdW2H9sZto49sAwzr5a7a/3PsBfH+sfGK3Z0Jtsz4W2YZMOMCUdfn5+fDx8UFeXh4aNWokdTh2RQiBtNxiZBcq8MyKIza/f+zMxxEWtc/m9yVyVK4uMrz9+MNYvjfRpvdt4e2BJl7uSMi4n+w38XLDvSLtJtS2LRoa7BDbupkXVkf0QEJ6AU4m5yAjvwQjuwRglBUWzrydWwzfhh5afUg+2nUFX8QkAQCuzB8GTzfDo+jIPhj7/DarZmTFihVYsmQJ0tPTERISgs8++ww9e/Y0ePxPP/2EWbNmITk5GW3btsXixYsxfPhwc25dp6nVQmuF11v3ijDvt0v404y1USyFiQhR9dZP6Yn/+/EsMgtK8Wq/NvhgZEcAwMu9W2uGms9/phM+3p2AvGLdvlXNvT3waEAjXEjLQ7YZa0J9+lIonurUUuuhLoRAYmYhHmjqheS7cqw7ehPjez6A4zfuYkz3Vvj17G0cTszGv4Y+gn9sitcsrhg5pB3a+XmjnZ+3VRKQygIa6w7T7dDy/sOMiYhzMblmZPPmzYiIiMDKlSvRq1cvLF++HD/99BMSEhLQokULneOPHj2KAQMGICoqCiNHjsTGjRuxePFinD59Gp06dTLqnnW9ZuSeXIGu8/dovu8U2Ag/TO2Nzh/q78tBRJY17FF/7LpY3p9p02u98dKq+1P4h3dogcyCUrzSJxgPNPXSLEZ4MDELIa0aa622XFVBiRJ3CxUI9m0AIQSSsuQIX3ZAs//94R0wMay15sErhMCHv17EutibNcbsXs8FJ98Ph0/96ptMjJGUVYj4lFw82zVQ6x8iW1OrBbbE3ULoA43Rzs/4taBIOsY+v01ORnr16oXHHnsMn3/+OQBArVYjKCgIb7/9NmbMmKFz/NixYyGXy/H7779rtvXu3RuhoaFYuXKlRd+MqQpKlHa/xooQ0EpEiEi/B30b4Hq2HADQOdAH580YNVZZp8BGWB3RAy19yv9DP3ItG0lZhYgIC4ZSpca5W7noHNhY71DU2igsLUOnObsNLklQolSh/axd1V6jiZcbjsx4HF7u7BZI0rJKM41CoUBcXBxmzpyp2ebi4oLw8HDExurvmBgbG4vIyEitbUOHDsX27dsN3qe0tBSlpffnjsjPzzclTKNNWnvCLoZ/EjmaDi0baaruq6rnIkOZGUm+fyNPeLi5aBYdHPxIc7zQIwjdHmiCZ1YcQXp+CQDg1AfhyCooRTs/bxQpynAyOQd3CxUYHRqolRjsupCO/0UnYtHzndHOzxv1XGQ4mnQXsdfvYkKvB7D1dBp2nr+Dj18Iwd/XxyEttxgPNm+Aff83SG98fR/2Rd+HfQEAbq4u6N66qcnv0RgNPeohedEIg/s93VxxZMbjWPpnAraeTtPZn7BgGDzqsQmDHItJyUh2djZUKhX8/Py0tvv5+eHKFf2T7qSnp+s9Pj3d8BDOqKgozJ0715TQiGrt2a6BGNO9FY5cy0Zzbw+br/XRrIE77sq1+wT0DG4KhUqN+NRcved4urlgUlgwGnjUg7+PJ+7JFUhIL0BjL3e08fXCi48FoVihQkJ6AdYcvoE9f/UvemvwQ5jYOxiNvdyQI1fg17O30aN1EzzYvCFyi8pj2H4mDX9eysBrAx5EeEc/KMrUSLtXjC6tfDTDs1VqARcZcFeugG+l5gi1WqBYqdIaNg4AmfklaO7tgSvpBXjq00MI8PHEzun90djLvdqyOfafJ7S+r7iXt6cbHm/vp+8UDOvkrzUSAyhfWK5icbl/PNEW/3iifMbOIzMeR2FpmcNMmhXYuD6WvRiKGU+1x+9n78DTzRVe7q4YFRIAVwmbUYjMZVIzze3btxEYGIijR48iLCxMs/29997DgQMHcPy47rTX7u7uWLduHcaNG6fZ9sUXX2Du3LnIyNDf8VJfzUhQUJDFm2mqzothKZaegKfis0Umk2lNwa7vPlV/nBXH6Jvfo/I5Fdc2dM2q2yvuo297Tfes7piq+wy9l6rbqr6PmmIzlaFrEhGRYVZppvH19YWrq6tOEpGRkQF/f3+95/j7+5t0PAB4eHjAw8Nwpy9LqefqeBPQ1vQwNLS/uvMq9plyrjHHmnOMqefr225OGdSESQgRkfWY9DR2d3dH9+7dER0drdmmVqsRHR2tVVNSWVhYmNbxALBnzx6DxxMREVHdYnIDaWRkJCZNmoQePXqgZ8+eWL58OeRyOSZPngwAiIiIQGBgIKKiogAA06dPx8CBA7F06VKMGDECmzZtwqlTp7Bq1SrLvhMiIiJySCYnI2PHjkVWVhZmz56N9PR0hIaGYteuXZpOqikpKXBxuV/h0qdPH2zcuBEffPAB/vOf/6Bt27bYvn270XOMEBERkXPjdPBERERkFcY+vx2vBycRERE5FSYjREREJCkmI0RERCQpJiNEREQkKSYjREREJCkmI0RERCQpJiNEREQkKYdYorJiKpT8fP1LlhMREZH9qXhu1zSlmUMkIwUFBQCAoKAgiSMhIiIiUxUUFMDHx8fgfoeYgVWtVuP27dvw9vaWdPXU/Px8BAUFITU1lTPBVsJyMYxlYxjLxjCWjWEsG8PssWyEECgoKEBAQIDWUjFVOUTNiIuLC1q1aiV1GBqNGjWymx+0PWG5GMayMYxlYxjLxjCWjWH2VjbV1YhUYAdWIiIikhSTESIiIpIUkxETeHh4YM6cOfDw8JA6FLvCcjGMZWMYy8Ywlo1hLBvDHLlsHKIDKxERETkv1owQERGRpJiMEBERkaSYjBAREZGkmIwQERGRpJiMEBERkaSYjPylpKRE6hDsVlJSEpKSkgAAZWVlEkdjPxITE/Hxxx8jISFB6lDsTnp6Om7fvo3i4mIA5Us6ULmKMiFd/Hwx7ObNm7h16xYAQKVSSRyN5dX5ZEShUODdd9/FhAkTEBERgUOHDkkdkl3Zt28f2rZtizFjxgAA6tVziBUErEqlUuGtt95C586dcfnyZWRlZUkdkt1QKpX4+9//jrCwMIwaNQpPPfUUSkpKql2Toq5QKpV444038NxzzyEiIgLHjh2rcSXTukKhUOC9997Da6+9hsjISFy/fl3qkOzKL7/8gjZt2mDatGkAAFdXV4kjsrw6/Qmxfft2PPzww4iPj8egQYMQHx+PmTNn4ueff5Y6NLuRkJCAAQMGICsrC6tXrwbA/16WLVuGs2fP4sCBA/j666/Rr18/ADUvke3s0tLSMGDAACQmJmLjxo2YPn06UlNTMWPGDKlDk1x6ejp69eqFc+fOYdSoUTh37hxef/11LFmyBEDdrjn66aef0KZNG5w6dQqtWrXC5s2b8frrr+Po0aNSh2Y3Tpw4gV69eiE1NVXzfHK22pE6m4wkJSXh+++/x9/+9jfs378fb7/9NqKjo+Hu7o7ExESpw5NcxYP15s2baNeuHaZMmYJ58+ZBoVCgXr16dfLBK4SAXC7Htm3b8Morr6BXr16IjY3FqlWrcPjwYcjlcqlDlNShQ4dQXFyMjRs3IiwsDBEREejXrx+8vb2lDk1yR44cgUKhwI8//og333wTBw4cwLPPPos5c+bg4sWLcHFxqZN/U/Hx8fjmm2/w9ttvY9++fZg3bx6OHz+Oa9euITk5WerwJFeRpObl5eGxxx5D165d8emnn0KpVMLV1dWpfmfqXDJS8cNTKBTo0qULJk2aBKA8y2zevDlcXV01/SPqMplMBgDIysrCiBEj8MILL8DNzQ1z5swBABQVFUkZniRkMhlu376N69evY9iwYfi///s/PP/881i3bh2ef/55PPvss8jPz5c6TMnk5uYiMTER/v7+AIA7d+7g3LlzaNq0KQ4fPixxdNKoeJhkZWXh3r17CAwMBFC+iunf//539OvXD3//+98B3P+bq0sUCgU6duyIiIgIAOVNWa1atUKTJk1w+fJliaOTXkWSeu3aNbz88st49tlncffuXXz55ZcAysvLWdSZZOTEiRMA7n84dOjQAbNnz0abNm0AlLfBKRQKFBUVISwsTLI4pVC1bID7SVtubi7kcjnatWuHmTNn4ssvv8SECRMwc+ZM3L17V5J4bUVfubRq1QrNmjXDBx98gJs3byI6Ohq//voroqOjERcXhwULFjjVfyuG6CubsLAw+Pj4oFevXhgzZgweeOAB+Pj4YMeOHRg+fDjmzZvnVB+ehmzZsgV79+7FnTt3NH1lXF1d4e/vr9Unzd/fHzNmzMDJkyexZ88eAM7f1FdRNrdv3wYA9OzZEx9//DECAgIAAG5ubsjLy4NcLkffvn2lDNXmKv/eVFCpVJDJZHB1dUVpaSl69+6NZ599Fl9//TVefvllLFu2DKWlpRJGbUHCyW3btk0EBASIZs2aiRs3bgghhCgrK9PsV6vVmtcFBQWibdu24tixY7YOUxL6ykalUmn2l5SUiLZt24qMjAwhhBBz584Vnp6ewsPDQ8TFxWmVnTOp7ncmJydHTJkyRXh7e4vnnntOqFQqTZmtWbNG+Pj4iKKiIqlCtzp9ZaNUKjX7b9y4If744w/RsWNH8d1332m2f//996JBgwYiNTXV1iHbzHfffSdatGghevbsKZo3by769u0rfv75ZyGEEKdPnxYdO3YUixYtEqWlpZpz0tPTxdNPPy0mTpwoVdg2oa9stm3bJoQo/wyu/LmTnJws2rZtK65duyZRtLZVXdkIUf6Z4+/vr/m9effdd4Wnp6eoX7++OHXqlERRW55T14xs2LABCxcuxIABA9ChQwcsWrQIgHZP5MpVo0eOHEFhYSHatWun2ZaRkWG7gG3IUNlU/CenVqshhEC3bt2wceNGdO3aFZ9//jnGjh0LLy8v5OXlQSaTOV1n1pp+Z5o0aYInnngC7u7uUKlUWm39nTp1gru7u9NWLxsqm8ojrIKDg3Hv3j24urri5Zdf1tSc9OvXDwqFAufOnZMkdmsqKyvDp59+iqioKCxcuBCHDh3C9u3b8dBDD2HNmjUoLi5G165d0a9fP2zdulWrY6afnx/c3NycdrRRdWWzatUqlJaWQiaTaf0dxcTEAICmtgQAcnJypAjfqowpG6B8KPjAgQOxdetWdOnSBevXr0d4eDhat26t+ftyhs6sTvkXUPGDefjhh/HEE09g8eLFePrppxETE6P5Rdf3w9u2bRsGDRqEJk2a4MyZMxg8eDDeeOMNp+rpbmzZuLi4oLCwEL/88gtmzpyJfv364dKlS/j4448xZMgQjB8/HoDzDPU1plwUCgUA4Omnn8bEiRPx66+/Yu/evZpE5fDhwwgNDUVoaKgUb8FqTP17EkLAxcUFmZmZmofsjh070K1bN/Ts2dPm8VubXC5HVlYWJk2ahMmTJ8Pd3R19+vRBx44dkZ+fr/m9mTt3LpRKJVatWoW0tDTN+cXFxWjatKlU4VtVTWVT+Z+Zin8Mt2/fjhEjRqB+/fqIj4/Hk08+ifnz5ztdE1ZNZVPRpKlSqfDjjz8iIiJCM1pt8eLFCA4ORmRkJAAnGeorab2MhV29elWn6aCiCvnChQvi6aefFsOHD9fsq3ysSqUSo0ePFkuWLBHTpk0TLi4uIiIiQigUCtsEb2Wmlk3F+/7tt9/EyZMntc7bvXu3mD9/vlCr1Q7fVGNquVQ011y/fl1ERESIBg0aiOeee06MGzdONG3aVHz11VdCCOHw5SKE6WVTUdW+Z88eMXDgQNGpUyexcuVKMXnyZNG0aVPxySef2Cx2a6taNmfOnNH8blSUw4YNG0RoaKhWs8xPP/0k+vfvL1q3bi2WLl0qJk6cKFq0aCEOHTpk2zdgReaWjRBCFBYWiscff1z88MMP4o033hCurq5iwoQJTvs5bGzZbNq0SRw/flzrWitXrhRLlixxis9hIYRwimRk8+bNIjg4WDzyyCOiZ8+e4uuvv9bsq/xDWrt2rejYsaNYu3atEEK7f0RKSoqQyWRCJpOJPn36iEuXLtnuDViRuWVTuR9A1eOd4RffUuWycuVK8a9//UtMnjxZXLlyxTbBW5klyubIkSNi1KhRYujQoWL06NFOWzZr1qzR2l/5M2X8+PHilVdeEUIIrQfLrVu3xGuvvSaeeeYZMXz48DpfNpV/b+Lj4zWfw71793baz2Fjy0ZfElbxN1i576MzcPhk5M8//xTBwcFixYoVYteuXSIyMlK4ubmJVatWaToSVvyy37p1S0yZMkU89thjoqCgQAhx/0PiwoULYuzYsWLPnj3SvBErqG3ZOMt/I1WxXAyrbdmUlJRorqVSqURubq7t34SVVFc2xcXFQgih+S+1uLhYdOnSRaxfv97g9SrOcQaWKpuDBw+KQYMG1ZnPYVPKxtmSj6ocNhmpyA7nzp0runfvrvWAePPNN0WPHj3E1q1bdc77/fffRY8ePcScOXPE2bNnxYgRI0RKSorN4rYFS5XNyJEjnapsWC6GsWwMM6ds0tLSRHBwsLh69aoQorx6/t1337Vd0DZiqbJ55513bBe0jfD3xjQO24G1orPTpUuX8NBDD8HNzU3T4WfBggXw9PTEL7/8gvT0dAD3O9gNHjwYPXv2xLx589C9e3eUlZWhRYsW0rwJK7FU2SiVSqcqG5aLYSwbw0wtGwDYu3cvgoKC0LJlS0yfPh0dO3bEzZs3oVQqnaojpqXKJiUlBUql0qkGC/D3xkRSZ0PG+vPPP8Xbb78tPvnkE62OPKtWrRLe3t6aKqyK7HPVqlWiXbt2IiYmRnNsYWGh+OSTT4Srq6sYNGiQOHfunG3fhJWwbPRjuRjGsjHM3LLZv3+/EKL8P+IXXnhBNGnSRDRr1kw8+uijOp3AHRXLxjCWTe3YfTJy+/ZtMXLkSNGiRQsxYcIE0blzZ+Hj46P5YSckJIjAwEAxa9YsIYR2RzF/f3+tHvwXL14UvXr10pqMyZGxbPRjuRjGsjHMUmUjl8vFyJEjRatWrcSmTZts/j6sgWVjGMvGMuw6GZHL5WLSpEli7Nix4vr165rtPXv21PQ2zs/PFwsWLBD169fXtFVXtNUNHDhQvPrqq7YP3AZYNvqxXAxj2Rhm6bJxppkxWTaGsWwsx677jHh5ecHDwwOvvPIK2rRpo5kgZ/jw4bh8+TKEEPD29sb48ePRrVs3vPjii7h58yZkMhlSUlKQmZmJZ555Rto3YSUsG/1YLoaxbAyzdNl0795dondieSwbw1g2liMTwr57xSiVSri5uQEon6LcxcUFEyZMQIMGDbBq1SrNcWlpaRg0aBDKysrQo0cPHD16FO3bt8fGjRvh5+cnVfhWxbLRj+ViGMvGMJaNYSwbw1g2lmH3yYg+/fr1w9SpUzFp0iRN72sXFxdcu3YNcXFxOH78OEJCQjBp0iSJI7U9lo1+LBfDWDaGsWwMY9kYxrIxg0TNQ2ZLSkoSfn5+Wm1rVacUrqtYNvqxXAxj2RjGsjGMZWMYy8Y8dt1npDLxVwXO4cOH0bBhQ03b2ty5czF9+nRkZmZKGZ6kWDb6sVwMY9kYxrIxjGVjGMumdhxmydWKCWROnDiB559/Hnv27MFrr72GoqIirF+/3ukmWjIFy0Y/lothLBvDWDaGsWwMY9nUkoS1MiYrLi4WDz/8sJDJZMLDw0MsWrRI6pDsBstGP5aLYSwbw1g2hrFsDGPZmM/hOrAOGTIEbdu2xbJly+Dp6Sl1OHaFZaMfy8Uwlo1hLBvDWDaGsWzM43DJiEqlgqurq9Rh2CWWjX4sF8NYNoaxbAxj2RjGsjGPwyUjRERE5FwcZjQNEREROScmI0RERCQpJiNEREQkKSYjREREJCkmI0RERCQpJiNEREQkKSYjREREJCkmI0RkEcnJyZDJZIiPj6+T9yci8znMQnlEZN+CgoJw584d+Pr6Sh2K0V555RXk5uZi+/btUodCVKexZoSIaqRUKms8xtXVFf7+/qhXj//jEJFpmIwQOaktW7agc+fOqF+/Ppo1a4bw8HDI5XIAwJo1a9ChQwd4enqiffv2+OKLLzTnVTR3bN68GQMHDoSnpye+/PJL1K9fH3/88YfWPbZt2wZvb28UFRXpbSa5ePEiRo4ciUaNGsHb2xv9+/dHUlKSZn91cdTkxIkT6Nq1Kzw9PdGjRw+cOXNGa79KpcKUKVPQpk0b1K9fH4888gg+/fRTzf4PP/wQ69atwy+//AKZTAaZTIaYmBgAQGpqKl588UU0btwYTZs2xejRo5GcnGx0bERkGv4LQ+SE7ty5g3HjxuGjjz7Cs88+i4KCAhw6dAhCCGzYsAGzZ8/G559/jq5du+LMmTOYOnUqGjRogEmTJmmuMWPGDCxdulTzwD906BA2btyIp556SnPMhg0b8Mwzz8DLy0snhrS0NAwYMACDBg3Cvn370KhRIxw5cgRlZWWac42JQ5/CwkKMHDkSQ4YMwffff48bN25g+vTpWseo1Wq0atUKP/30E5o1a4ajR4/itddeQ8uWLfHiiy/in//8Jy5fvoz8/Hx88803AICmTZtCqVRi6NChCAsLw6FDh1CvXj0sWLAAw4YNw7lz5+Du7m72z4WIDBBE5HTi4uIEAJGcnKyz76GHHhIbN27U2jZ//nwRFhYmhBDixo0bAoBYvny51jHbtm0TDRs2FHK5XAghRF5envD09BR//PGH1nlnzpwRQggxc+ZM0aZNG6FQKPTGWFMc1fnqq69Es2bNRHFxsWbbl19+qXV/fd566y3x/PPPa76fNGmSGD16tNYx69evF4888ohQq9WabaWlpaJ+/fpi9+7dNcZGRKZjzQiREwoJCcETTzyBzp07Y+jQoXjyyScxZswYuLu7IykpCVOmTMHUqVM1x5eVlcHHx0frGj169ND6fvjw4XBzc8Ovv/6Kl156CT///DMaNWqE8PBwvTHEx8ejf//+cHNz09knl8uNjkOfy5cvo0uXLvD09NRsCwsL0zluxYoVWLt2LVJSUlBcXAyFQoHQ0NBqr3327Flcu3YN3t7eWttLSkq0mpiIyHKYjBA5IVdXV+zZswdHjx7Fn3/+ic8++wzvv/8+fvvtNwDA6tWr0atXL51zKmvQoIHW9+7u7hgzZgw2btyIl156CRs3bsTYsWMNdlitX7++wfgKCwuNjsNcmzZtwj//+U8sXboUYWFh8Pb2xpIlS3D8+PFqzyssLET37t2xYcMGnX3Nmze3SGxEpI3JCJGTkslk6Nu3L/r27YvZs2ejdevWOHLkCAICAnD9+nVMmDDB5GtOmDABQ4YMwcWLF7Fv3z4sWLDA4LFdunTBunXroFQqdWpH/Pz8ahVHhw4dsH79epSUlGhqR44dO6Z1zJEjR9CnTx+8+eabmm1Vazbc3d2hUqm0tnXr1g2bN29GixYt0KhRI5NjIyLTcTQNkRM6fvw4Fi5ciFOnTiElJQVbt25FVlYWOnTogLlz5yIqKgr/+9//cPXqVZw/fx7ffPMNli1bVuN1BwwYAH9/f0yYMAFt2rTRqdWobNq0acjPz8dLL72EU6dOITExEevXr0dCQgIA1CqO8ePHQyaTYerUqbh06RJ27tyJjz/+WOuYtm3b4tSpU9i9ezeuXr2KWbNm4eTJk1rHBAcH49y5c0hISEB2djaUSiUmTJgAX19fjB49GocOHcKNGzcQExODf/zjH7h161aNsRGRGaTutEJElnfp0iUxdOhQ0bx5c+Hh4SHatWsnPvvsM83+DRs2iNDQUOHu7i6aNGkiBgwYILZu3SqE0O2IWtV7770nAIjZs2drbdd33tmzZ8WTTz4pvLy8hLe3t+jfv79ISkoyKo6axMbGipCQEOHu7i5CQ0PFzz//rHX/kpIS8corrwgfHx/RuHFj8cYbb4gZM2aIkJAQzTUyMzPFkCFDRMOGDQUAsX//fiGEEHfu3BERERHC19dXeHh4iAcffFBMnTpV5OXlGRUbEZlGJoQQkmZDREREVKexmYaIiIgkxWSEiOzOwoUL0bBhQ71flSddIyLnwGYaIrI7OTk5yMnJ0buvfv36CAwMtHFERGRNTEaIiIhIUmymISIiIkkxGSEiIiJJMRkhIiIiSTEZISIiIkkxGSEiIiJJMRkhIiIiSTEZISIiIkn9PwmTfRtrqhanAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "def create_date_features(df):\n",
        "    df = df.copy()\n",
        "    df[\"month\"] = df.date.dt.month.astype(\"int8\")\n",
        "    df[\"day_of_month\"] = df.date.dt.day.astype(\"int8\")\n",
        "    df[\"day_of_year\"] = df.date.dt.dayofyear.astype(\"int16\")\n",
        "    df[\"week_of_month\"] = (df.date.apply(lambda d: (d.day - 1) // 7 + 1)).astype(\"int8\")\n",
        "    df[\"week_of_year\"] = (df.date.dt.isocalendar().week).astype(\"int8\")\n",
        "    df[\"day_of_week\"] = (df.date.dt.dayofweek + 1).astype(\"int8\")\n",
        "    df[\"year\"] = df.date.dt.year.astype(\"int32\")\n",
        "    df[\"is_wknd\"] = (df.date.dt.weekday // 4).astype(\"int8\")\n",
        "    # df[\"quarter\"] = df.date.dt.quarter.astype(\"int8\")\n",
        "    # df['is_month_start'] = df.date.dt.is_month_start.astype(\"int8\")\n",
        "    # df['is_month_end'] = df.date.dt.is_month_end.astype(\"int8\")\n",
        "    # df['is_quarter_start'] = df.date.dt.is_quarter_start.astype(\"int8\")\n",
        "    # df['is_quarter_end'] = df.date.dt.is_quarter_end.astype(\"int8\")\n",
        "    # df['is_year_start'] = df.date.dt.is_year_start.astype(\"int8\")\n",
        "    # df['is_year_end'] = df.date.dt.is_year_end.astype(\"int8\")\n",
        "    # 0: Winter - 1: Spring - 2: Summer - 3: Fall\n",
        "    # df[\"season\"] = np.where(df.month.isin([12,1,2]), 0, 1)\n",
        "    # df[\"season\"] = np.where(df.month.isin([6,7,8]), 2, df[\"season\"])\n",
        "    # df[\"season\"] = pd.Series(np.where(df.month.isin([9, 10, 11]), 3, df[\"season\"])).astype(\"int8\")\n",
        "    return df\n",
        "\n",
        "\n",
        "train_after_covid_with_features = create_date_features(train_after_covid)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "def create_rolling_features(df, column, lags, windows, ew_windows):\n",
        "    df = df.copy()\n",
        "    for lag in lags:\n",
        "        lagged_col = df[column].shift(lag)\n",
        "\n",
        "        # df[f'lag_{lag}'] = lagged_col\n",
        "\n",
        "        for window in windows:\n",
        "\n",
        "            df[f\"rolling_mean_lag{lag}_win{window}\"] = lagged_col.rolling(\n",
        "                window=window\n",
        "            ).mean()\n",
        "            df[f\"rolling_median_lag{lag}_win{window}\"] = lagged_col.rolling(\n",
        "                window=window\n",
        "            ).median()\n",
        "\n",
        "            # Min and Max\n",
        "            df[f\"rolling_min_lag{lag}_win{window}\"] = lagged_col.rolling(\n",
        "                window=window\n",
        "            ).min()\n",
        "            df[f\"rolling_max_lag{lag}_win{window}\"] = lagged_col.rolling(\n",
        "                window=window\n",
        "            ).max()\n",
        "\n",
        "            # Standard Deviation\n",
        "            df[f\"rolling_std_lag{lag}_win{window}\"] = lagged_col.rolling(\n",
        "                window=window\n",
        "            ).std()\n",
        "\n",
        "            # Skewness\n",
        "            df[f\"rolling_skew_lag{lag}_win{window}\"] = lagged_col.rolling(\n",
        "                window=window\n",
        "            ).skew()\n",
        "\n",
        "            # Percentiles\n",
        "            df[f\"rolling_10th_percentile_lag{lag}_win{window}\"] = lagged_col.rolling(\n",
        "                window=window\n",
        "            ).quantile(0.1)\n",
        "            df[f\"rolling_90th_percentile_lag{lag}_win{window}\"] = lagged_col.rolling(\n",
        "                window=window\n",
        "            ).quantile(0.9)\n",
        "\n",
        "        for ew_window in ew_windows:\n",
        "            df[f\"ewm_mean_lag{lag}_win{ew_window}\"] = lagged_col.ewm(\n",
        "                span=ew_window\n",
        "            ).mean()\n",
        "            df[f\"ewm_std_lag{lag}_win{ew_window}\"] = lagged_col.ewm(span=ew_window).std()\n",
        "            df[f\"ewm_var_lag{lag}_win{ew_window}\"] = lagged_col.ewm(span=ew_window).var()\n",
        "            df[f\"ewm_corr_lag{lag}_win{ew_window}\"] = lagged_col.ewm(\n",
        "                span=ew_window\n",
        "            ).corr()\n",
        "    return df\n",
        "\n",
        "\n",
        "lags = [31, 38, 45, 365, 366]\n",
        "windows = [7, 14, 21, 28, 35, 42, 56]\n",
        "ew_windows = [7, 14, 21, 28, 35, 42, 56]\n",
        "\n",
        "# lags = [31, 38, 45]\n",
        "# windows = [7, 14, 21, 28, 35, 42]\n",
        "# lags = [31, 38]\n",
        "# windows = [7, 14, 21, 28, 35, 42]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "catboost_train = create_rolling_features(\n",
        "    train_after_covid_with_features, \"forecast_value\", lags, windows, ew_windows\n",
        ")[max(lags) + max(windows) - 1 :]\n",
        "catboost_train"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_mean_lag{lag}_win{window}'] = lagged_col.rolling(window=window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_median_lag{lag}_win{window}'] = lagged_col.rolling(window=window).median()\n",
            "/tmp/ipykernel_179633/3572121117.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_min_lag{lag}_win{window}'] = lagged_col.rolling(window=window).min()\n",
            "/tmp/ipykernel_179633/3572121117.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_max_lag{lag}_win{window}'] = lagged_col.rolling(window=window).max()\n",
            "/tmp/ipykernel_179633/3572121117.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_std_lag{lag}_win{window}'] = lagged_col.rolling(window=window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_skew_lag{lag}_win{window}'] = lagged_col.rolling(window=window).skew()\n",
            "/tmp/ipykernel_179633/3572121117.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_10th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.1)\n",
            "/tmp/ipykernel_179633/3572121117.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'rolling_90th_percentile_lag{lag}_win{window}'] = lagged_col.rolling(window=window).quantile(0.9)\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n",
            "/tmp/ipykernel_179633/3572121117.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_mean_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).mean()\n",
            "/tmp/ipykernel_179633/3572121117.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_std_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).std()\n",
            "/tmp/ipykernel_179633/3572121117.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_var_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).var()\n",
            "/tmp/ipykernel_179633/3572121117.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'ewm_corr_lag{lag}_win{ew_window}'] = lagged_col.ewm(span=ew_window).corr()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>forecast_value</th>\n",
              "      <th>month</th>\n",
              "      <th>day_of_month</th>\n",
              "      <th>day_of_year</th>\n",
              "      <th>week_of_month</th>\n",
              "      <th>week_of_year</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>year</th>\n",
              "      <th>is_wknd</th>\n",
              "      <th>...</th>\n",
              "      <th>ewm_var_lag366_win35</th>\n",
              "      <th>ewm_corr_lag366_win35</th>\n",
              "      <th>ewm_mean_lag366_win42</th>\n",
              "      <th>ewm_std_lag366_win42</th>\n",
              "      <th>ewm_var_lag366_win42</th>\n",
              "      <th>ewm_corr_lag366_win42</th>\n",
              "      <th>ewm_mean_lag366_win56</th>\n",
              "      <th>ewm_std_lag366_win56</th>\n",
              "      <th>ewm_var_lag366_win56</th>\n",
              "      <th>ewm_corr_lag366_win56</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>2014-01-21</td>\n",
              "      <td>1.440472e+06</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2014</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.551646e+10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.212941e+05</td>\n",
              "      <td>1.921287e+05</td>\n",
              "      <td>3.691343e+10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.195403e+05</td>\n",
              "      <td>1.966855e+05</td>\n",
              "      <td>3.868519e+10</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>2014-01-22</td>\n",
              "      <td>1.148345e+06</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2014</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.510914e+10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.127146e+05</td>\n",
              "      <td>1.910806e+05</td>\n",
              "      <td>3.651180e+10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.126598e+05</td>\n",
              "      <td>1.956218e+05</td>\n",
              "      <td>3.826789e+10</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>2014-01-23</td>\n",
              "      <td>1.813668e+06</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>23</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2014</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.333527e+10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.092588e+05</td>\n",
              "      <td>1.868906e+05</td>\n",
              "      <td>3.492810e+10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.098677e+05</td>\n",
              "      <td>1.921310e+05</td>\n",
              "      <td>3.691432e+10</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>2014-01-24</td>\n",
              "      <td>1.813134e+06</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2014</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>3.199661e+10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.040754e+05</td>\n",
              "      <td>1.836418e+05</td>\n",
              "      <td>3.372432e+10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.056599e+05</td>\n",
              "      <td>1.893882e+05</td>\n",
              "      <td>3.586788e+10</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>2014-01-25</td>\n",
              "      <td>3.968600e+04</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2014</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>3.169137e+10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.957276e+05</td>\n",
              "      <td>1.828515e+05</td>\n",
              "      <td>3.343467e+10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.988721e+05</td>\n",
              "      <td>1.886106e+05</td>\n",
              "      <td>3.557394e+10</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4228</th>\n",
              "      <td>2024-06-26</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6</td>\n",
              "      <td>26</td>\n",
              "      <td>178</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "      <td>3</td>\n",
              "      <td>2024</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.289365e+14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.069019e+07</td>\n",
              "      <td>1.805547e+07</td>\n",
              "      <td>3.260000e+14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.039007e+07</td>\n",
              "      <td>1.797921e+07</td>\n",
              "      <td>3.232519e+14</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4229</th>\n",
              "      <td>2024-06-27</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6</td>\n",
              "      <td>27</td>\n",
              "      <td>179</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "      <td>4</td>\n",
              "      <td>2024</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.137278e+14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.104174e+07</td>\n",
              "      <td>1.770403e+07</td>\n",
              "      <td>3.134326e+14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.066581e+07</td>\n",
              "      <td>1.772113e+07</td>\n",
              "      <td>3.140386e+14</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4230</th>\n",
              "      <td>2024-06-28</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6</td>\n",
              "      <td>28</td>\n",
              "      <td>180</td>\n",
              "      <td>4</td>\n",
              "      <td>26</td>\n",
              "      <td>5</td>\n",
              "      <td>2024</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2.983235e+14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.133079e+07</td>\n",
              "      <td>1.733808e+07</td>\n",
              "      <td>3.006088e+14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.089705e+07</td>\n",
              "      <td>1.745041e+07</td>\n",
              "      <td>3.045170e+14</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4231</th>\n",
              "      <td>2024-06-29</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6</td>\n",
              "      <td>29</td>\n",
              "      <td>181</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>6</td>\n",
              "      <td>2024</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2.871034e+14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.180048e+07</td>\n",
              "      <td>1.706634e+07</td>\n",
              "      <td>2.912598e+14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.126660e+07</td>\n",
              "      <td>1.725271e+07</td>\n",
              "      <td>2.976560e+14</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4232</th>\n",
              "      <td>2024-06-30</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>182</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>7</td>\n",
              "      <td>2024</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2.788929e+14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.236777e+07</td>\n",
              "      <td>1.686627e+07</td>\n",
              "      <td>2.844710e+14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.171329e+07</td>\n",
              "      <td>1.711136e+07</td>\n",
              "      <td>2.927987e+14</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3812 rows \u00d7 430 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           date  forecast_value  month  day_of_month  day_of_year  \\\n",
              "421  2014-01-21    1.440472e+06      1            21           21   \n",
              "422  2014-01-22    1.148345e+06      1            22           22   \n",
              "423  2014-01-23    1.813668e+06      1            23           23   \n",
              "424  2014-01-24    1.813134e+06      1            24           24   \n",
              "425  2014-01-25    3.968600e+04      1            25           25   \n",
              "...         ...             ...    ...           ...          ...   \n",
              "4228 2024-06-26    0.000000e+00      6            26          178   \n",
              "4229 2024-06-27    0.000000e+00      6            27          179   \n",
              "4230 2024-06-28    0.000000e+00      6            28          180   \n",
              "4231 2024-06-29    0.000000e+00      6            29          181   \n",
              "4232 2024-06-30    0.000000e+00      6            30          182   \n",
              "\n",
              "      week_of_month  week_of_year  day_of_week  year  is_wknd  ...  \\\n",
              "421               3             4            2  2014        0  ...   \n",
              "422               4             4            3  2014        0  ...   \n",
              "423               4             4            4  2014        0  ...   \n",
              "424               4             4            5  2014        1  ...   \n",
              "425               4             4            6  2014        1  ...   \n",
              "...             ...           ...          ...   ...      ...  ...   \n",
              "4228              4            26            3  2024        0  ...   \n",
              "4229              4            26            4  2024        0  ...   \n",
              "4230              4            26            5  2024        1  ...   \n",
              "4231              5            26            6  2024        1  ...   \n",
              "4232              5            26            7  2024        1  ...   \n",
              "\n",
              "      ewm_var_lag366_win35  ewm_corr_lag366_win35  ewm_mean_lag366_win42  \\\n",
              "421           3.551646e+10                    1.0           2.212941e+05   \n",
              "422           3.510914e+10                    1.0           2.127146e+05   \n",
              "423           3.333527e+10                    1.0           2.092588e+05   \n",
              "424           3.199661e+10                    1.0           2.040754e+05   \n",
              "425           3.169137e+10                    1.0           1.957276e+05   \n",
              "...                    ...                    ...                    ...   \n",
              "4228          3.289365e+14                    1.0           5.069019e+07   \n",
              "4229          3.137278e+14                    1.0           5.104174e+07   \n",
              "4230          2.983235e+14                    1.0           5.133079e+07   \n",
              "4231          2.871034e+14                    1.0           5.180048e+07   \n",
              "4232          2.788929e+14                    1.0           5.236777e+07   \n",
              "\n",
              "      ewm_std_lag366_win42  ewm_var_lag366_win42  ewm_corr_lag366_win42  \\\n",
              "421           1.921287e+05          3.691343e+10                    1.0   \n",
              "422           1.910806e+05          3.651180e+10                    1.0   \n",
              "423           1.868906e+05          3.492810e+10                    1.0   \n",
              "424           1.836418e+05          3.372432e+10                    1.0   \n",
              "425           1.828515e+05          3.343467e+10                    1.0   \n",
              "...                    ...                   ...                    ...   \n",
              "4228          1.805547e+07          3.260000e+14                    1.0   \n",
              "4229          1.770403e+07          3.134326e+14                    1.0   \n",
              "4230          1.733808e+07          3.006088e+14                    1.0   \n",
              "4231          1.706634e+07          2.912598e+14                    1.0   \n",
              "4232          1.686627e+07          2.844710e+14                    1.0   \n",
              "\n",
              "      ewm_mean_lag366_win56  ewm_std_lag366_win56  ewm_var_lag366_win56  \\\n",
              "421            2.195403e+05          1.966855e+05          3.868519e+10   \n",
              "422            2.126598e+05          1.956218e+05          3.826789e+10   \n",
              "423            2.098677e+05          1.921310e+05          3.691432e+10   \n",
              "424            2.056599e+05          1.893882e+05          3.586788e+10   \n",
              "425            1.988721e+05          1.886106e+05          3.557394e+10   \n",
              "...                     ...                   ...                   ...   \n",
              "4228           5.039007e+07          1.797921e+07          3.232519e+14   \n",
              "4229           5.066581e+07          1.772113e+07          3.140386e+14   \n",
              "4230           5.089705e+07          1.745041e+07          3.045170e+14   \n",
              "4231           5.126660e+07          1.725271e+07          2.976560e+14   \n",
              "4232           5.171329e+07          1.711136e+07          2.927987e+14   \n",
              "\n",
              "      ewm_corr_lag366_win56  \n",
              "421                     1.0  \n",
              "422                     1.0  \n",
              "423                     1.0  \n",
              "424                     1.0  \n",
              "425                     1.0  \n",
              "...                     ...  \n",
              "4228                    1.0  \n",
              "4229                    1.0  \n",
              "4230                    1.0  \n",
              "4231                    1.0  \n",
              "4232                    1.0  \n",
              "\n",
              "[3812 rows x 430 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "catboost_train = catboost_train.drop(\"date\", axis=1)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "X_train = catboost_train[: -len(submission)].drop(\"forecast_value\", axis=1)\n",
        "y_train = catboost_train[: -len(submission)][\"forecast_value\"]\n",
        "X_test = catboost_train[-len(submission) :].drop(\"forecast_value\", axis=1)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "forecaster = ForecasterAutoreg(regressor=CatBoostRegressor(iterations=3000), lags=365)\n",
        "\n",
        "forecaster.fit(y=y_train, exog=X_train)"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate set to 0.020351\n",
            "0:\tlearn: 20052626.5463138\ttotal: 289ms\tremaining: 14m 27s\n",
            "1:\tlearn: 19735232.2027758\ttotal: 332ms\tremaining: 8m 17s\n",
            "2:\tlearn: 19391357.3268354\ttotal: 374ms\tremaining: 6m 13s\n",
            "3:\tlearn: 19068557.8688405\ttotal: 416ms\tremaining: 5m 11s\n",
            "4:\tlearn: 18743520.8563880\ttotal: 456ms\tremaining: 4m 32s\n",
            "5:\tlearn: 18428287.3188331\ttotal: 495ms\tremaining: 4m 6s\n",
            "6:\tlearn: 18128559.8064777\ttotal: 533ms\tremaining: 3m 47s\n",
            "7:\tlearn: 17824874.2488691\ttotal: 572ms\tremaining: 3m 33s\n",
            "8:\tlearn: 17523889.0536733\ttotal: 612ms\tremaining: 3m 23s\n",
            "9:\tlearn: 17235358.2234713\ttotal: 653ms\tremaining: 3m 15s\n",
            "10:\tlearn: 16956326.5951213\ttotal: 693ms\tremaining: 3m 8s\n",
            "11:\tlearn: 16681870.0588291\ttotal: 733ms\tremaining: 3m 2s\n",
            "12:\tlearn: 16422998.9308947\ttotal: 772ms\tremaining: 2m 57s\n",
            "13:\tlearn: 16159635.9547098\ttotal: 811ms\tremaining: 2m 52s\n",
            "14:\tlearn: 15918267.4778933\ttotal: 850ms\tremaining: 2m 49s\n",
            "15:\tlearn: 15662696.5943255\ttotal: 889ms\tremaining: 2m 45s\n",
            "16:\tlearn: 15411365.1895678\ttotal: 928ms\tremaining: 2m 42s\n",
            "17:\tlearn: 15176439.7868280\ttotal: 968ms\tremaining: 2m 40s\n",
            "18:\tlearn: 14943558.0822321\ttotal: 1.01s\tremaining: 2m 38s\n",
            "19:\tlearn: 14715104.6873465\ttotal: 1.01s\tremaining: 2m 38s\n",
            "20:\tlearn: 14478898.8147138\ttotal: 1.05s\tremaining: 2m 35s\n",
            "21:\tlearn: 14266351.5902557\ttotal: 1.08s\tremaining: 2m 33s\n",
            "22:\tlearn: 14051499.4045908\ttotal: 1.12s\tremaining: 2m 32s\n",
            "23:\tlearn: 13831612.7628864\ttotal: 1.16s\tremaining: 2m 30s\n",
            "24:\tlearn: 13617332.2487003\ttotal: 1.2s\tremaining: 2m 28s\n",
            "25:\tlearn: 13411365.7802793\ttotal: 1.24s\tremaining: 2m 27s\n",
            "26:\tlearn: 13211383.9747922\ttotal: 1.28s\tremaining: 2m 26s\n",
            "27:\tlearn: 13017760.0135158\ttotal: 3.94s\tremaining: 7m 13s\n",
            "28:\tlearn: 12837076.7908357\ttotal: 3.98s\tremaining: 7m 2s\n",
            "29:\tlearn: 12651466.3072031\ttotal: 4.02s\tremaining: 6m 51s\n",
            "30:\tlearn: 12462524.3672470\ttotal: 4.05s\tremaining: 6m 41s\n",
            "31:\tlearn: 12283246.2279849\ttotal: 4.09s\tremaining: 6m 31s\n",
            "32:\tlearn: 12107269.0619554\ttotal: 4.13s\tremaining: 6m 22s\n",
            "33:\tlearn: 11932091.2890490\ttotal: 4.17s\tremaining: 6m 14s\n",
            "34:\tlearn: 11749173.9411702\ttotal: 4.21s\tremaining: 6m 7s\n",
            "35:\tlearn: 11578977.2357699\ttotal: 4.25s\tremaining: 5m 59s\n",
            "36:\tlearn: 11421077.3193003\ttotal: 4.29s\tremaining: 5m 52s\n",
            "37:\tlearn: 11266513.6085867\ttotal: 4.33s\tremaining: 5m 46s\n",
            "38:\tlearn: 11110381.2340826\ttotal: 4.37s\tremaining: 5m 40s\n",
            "39:\tlearn: 10953452.6002628\ttotal: 4.41s\tremaining: 5m 34s\n",
            "40:\tlearn: 10809706.2195764\ttotal: 4.45s\tremaining: 5m 28s\n",
            "41:\tlearn: 10662268.3173663\ttotal: 4.49s\tremaining: 5m 23s\n",
            "42:\tlearn: 10506900.7113318\ttotal: 4.52s\tremaining: 5m 18s\n",
            "43:\tlearn: 10363683.1498193\ttotal: 4.56s\tremaining: 5m 13s\n",
            "44:\tlearn: 10226951.4914599\ttotal: 4.61s\tremaining: 5m 9s\n",
            "45:\tlearn: 10096089.1895913\ttotal: 4.61s\tremaining: 5m 9s\n",
            "46:\tlearn: 9964040.2632407\ttotal: 7.27s\tremaining: 7m 57s\n",
            "47:\tlearn: 9830203.0756439\ttotal: 7.31s\tremaining: 7m 49s\n",
            "48:\tlearn: 9707629.9399072\ttotal: 7.31s\tremaining: 7m 49s\n",
            "49:\tlearn: 9581176.0612411\ttotal: 9.97s\tremaining: 10m 25s\n",
            "50:\tlearn: 9459707.5679467\ttotal: 10s\tremaining: 10m 15s\n",
            "51:\tlearn: 9346365.3270577\ttotal: 10.1s\tremaining: 10m 4s\n",
            "52:\tlearn: 9231408.9956599\ttotal: 10.1s\tremaining: 9m 54s\n",
            "53:\tlearn: 9117830.4061653\ttotal: 10.1s\tremaining: 9m 45s\n",
            "54:\tlearn: 9005710.7393295\ttotal: 10.2s\tremaining: 9m 36s\n",
            "55:\tlearn: 8902589.6464510\ttotal: 10.2s\tremaining: 9m 27s\n",
            "56:\tlearn: 8795330.4687886\ttotal: 10.2s\tremaining: 9m 18s\n",
            "57:\tlearn: 8688358.9374860\ttotal: 10.3s\tremaining: 9m 10s\n",
            "58:\tlearn: 8585040.8163369\ttotal: 10.3s\tremaining: 9m 2s\n",
            "59:\tlearn: 8479738.4705995\ttotal: 10.4s\tremaining: 8m 54s\n",
            "60:\tlearn: 8380945.7293998\ttotal: 10.4s\tremaining: 8m 47s\n",
            "61:\tlearn: 8285770.7918251\ttotal: 10.4s\tremaining: 8m 40s\n",
            "62:\tlearn: 8198484.8015460\ttotal: 10.5s\tremaining: 8m 33s\n",
            "63:\tlearn: 8113485.4867965\ttotal: 10.5s\tremaining: 8m 26s\n",
            "64:\tlearn: 8027204.3273176\ttotal: 10.6s\tremaining: 8m 19s\n",
            "65:\tlearn: 7935348.4725526\ttotal: 10.6s\tremaining: 8m 13s\n",
            "66:\tlearn: 7852299.2953605\ttotal: 10.6s\tremaining: 8m 7s\n",
            "67:\tlearn: 7762849.6125681\ttotal: 10.7s\tremaining: 8m 1s\n",
            "68:\tlearn: 7682148.8000352\ttotal: 10.7s\tremaining: 7m 55s\n",
            "69:\tlearn: 7603680.6798295\ttotal: 10.8s\tremaining: 7m 50s\n",
            "70:\tlearn: 7524793.2647168\ttotal: 10.8s\tremaining: 7m 44s\n",
            "71:\tlearn: 7442815.0313272\ttotal: 10.8s\tremaining: 7m 39s\n",
            "72:\tlearn: 7368963.4232184\ttotal: 10.9s\tremaining: 7m 34s\n",
            "73:\tlearn: 7294853.7989535\ttotal: 10.9s\tremaining: 7m 29s\n",
            "74:\tlearn: 7223849.6870902\ttotal: 10.9s\tremaining: 7m 24s\n",
            "75:\tlearn: 7155671.5070754\ttotal: 11s\tremaining: 7m 20s\n",
            "76:\tlearn: 7088184.8415616\ttotal: 11s\tremaining: 7m 15s\n",
            "77:\tlearn: 7019646.0765600\ttotal: 11.1s\tremaining: 7m 11s\n",
            "78:\tlearn: 6950875.1675000\ttotal: 11.1s\tremaining: 7m 6s\n",
            "79:\tlearn: 6889879.0404601\ttotal: 11.1s\tremaining: 7m 2s\n",
            "80:\tlearn: 6827404.0803550\ttotal: 11.2s\tremaining: 6m 58s\n",
            "81:\tlearn: 6756199.4260608\ttotal: 11.2s\tremaining: 6m 54s\n",
            "82:\tlearn: 6701823.7729536\ttotal: 11.3s\tremaining: 6m 50s\n",
            "83:\tlearn: 6647908.0722638\ttotal: 11.3s\tremaining: 6m 47s\n",
            "84:\tlearn: 6594936.4286093\ttotal: 11.3s\tremaining: 6m 43s\n",
            "85:\tlearn: 6540844.1868022\ttotal: 11.4s\tremaining: 6m 39s\n",
            "86:\tlearn: 6484760.2039464\ttotal: 11.4s\tremaining: 6m 36s\n",
            "87:\tlearn: 6436746.2586010\ttotal: 11.5s\tremaining: 6m 32s\n",
            "88:\tlearn: 6383568.2036222\ttotal: 11.5s\tremaining: 6m 29s\n",
            "89:\tlearn: 6333231.7898373\ttotal: 11.5s\tremaining: 6m 25s\n",
            "90:\tlearn: 6280009.6623382\ttotal: 11.6s\tremaining: 6m 22s\n",
            "91:\tlearn: 6235991.0818315\ttotal: 11.6s\tremaining: 6m 19s\n",
            "92:\tlearn: 6181539.7383138\ttotal: 11.7s\tremaining: 6m 16s\n",
            "93:\tlearn: 6131952.0799060\ttotal: 11.7s\tremaining: 6m 13s\n",
            "94:\tlearn: 6086577.1329389\ttotal: 11.7s\tremaining: 6m 10s\n",
            "95:\tlearn: 6039652.5318543\ttotal: 11.8s\tremaining: 6m 7s\n",
            "96:\tlearn: 5996248.4758412\ttotal: 11.8s\tremaining: 6m 4s\n",
            "97:\tlearn: 5952792.2405794\ttotal: 11.9s\tremaining: 6m 2s\n",
            "98:\tlearn: 5910492.9913257\ttotal: 11.9s\tremaining: 5m 59s\n",
            "99:\tlearn: 5872221.5651580\ttotal: 11.9s\tremaining: 5m 57s\n",
            "100:\tlearn: 5833751.7743734\ttotal: 12s\tremaining: 5m 54s\n",
            "101:\tlearn: 5795323.3618220\ttotal: 12s\tremaining: 5m 52s\n",
            "102:\tlearn: 5754150.6984238\ttotal: 12.1s\tremaining: 5m 49s\n",
            "103:\tlearn: 5712463.1311302\ttotal: 12.1s\tremaining: 5m 47s\n",
            "104:\tlearn: 5676062.3301406\ttotal: 12.2s\tremaining: 5m 44s\n",
            "105:\tlearn: 5637763.9073502\ttotal: 12.2s\tremaining: 5m 42s\n",
            "106:\tlearn: 5598345.0625421\ttotal: 12.2s\tremaining: 5m 40s\n",
            "107:\tlearn: 5565978.8244683\ttotal: 12.3s\tremaining: 5m 38s\n",
            "108:\tlearn: 5536802.8208858\ttotal: 12.3s\tremaining: 5m 36s\n",
            "109:\tlearn: 5502241.7533236\ttotal: 12.4s\tremaining: 5m 33s\n",
            "110:\tlearn: 5470283.7472327\ttotal: 12.4s\tremaining: 5m 32s\n",
            "111:\tlearn: 5439513.7151621\ttotal: 12.5s\tremaining: 5m 30s\n",
            "112:\tlearn: 5409050.9926392\ttotal: 12.5s\tremaining: 5m 28s\n",
            "113:\tlearn: 5377848.5066547\ttotal: 12.6s\tremaining: 5m 26s\n",
            "114:\tlearn: 5351082.0604060\ttotal: 12.6s\tremaining: 5m 24s\n",
            "115:\tlearn: 5316916.2292013\ttotal: 12.6s\tremaining: 5m 22s\n",
            "116:\tlearn: 5288589.1773280\ttotal: 12.7s\tremaining: 5m 20s\n",
            "117:\tlearn: 5257445.9047053\ttotal: 12.7s\tremaining: 5m 19s\n",
            "118:\tlearn: 5228626.1029849\ttotal: 12.8s\tremaining: 5m 17s\n",
            "119:\tlearn: 5199307.4919532\ttotal: 12.8s\tremaining: 5m 15s\n",
            "120:\tlearn: 5172561.2164012\ttotal: 12.9s\tremaining: 5m 13s\n",
            "121:\tlearn: 5142599.7653544\ttotal: 12.9s\tremaining: 5m 12s\n",
            "122:\tlearn: 5115433.4124986\ttotal: 12.9s\tremaining: 5m 10s\n",
            "123:\tlearn: 5088581.4244005\ttotal: 13s\tremaining: 5m 8s\n",
            "124:\tlearn: 5063539.9484419\ttotal: 13s\tremaining: 5m 6s\n",
            "125:\tlearn: 5044308.7747751\ttotal: 13.1s\tremaining: 5m 5s\n",
            "126:\tlearn: 5023019.4928741\ttotal: 13.1s\tremaining: 5m 3s\n",
            "127:\tlearn: 4997361.8896004\ttotal: 13.1s\tremaining: 5m 1s\n",
            "128:\tlearn: 4974909.8108505\ttotal: 13.2s\tremaining: 5m\n",
            "129:\tlearn: 4950244.7899729\ttotal: 13.2s\tremaining: 4m 58s\n",
            "130:\tlearn: 4926387.4345053\ttotal: 13.3s\tremaining: 4m 57s\n",
            "131:\tlearn: 4906053.0981429\ttotal: 13.3s\tremaining: 4m 56s\n",
            "132:\tlearn: 4884511.7606407\ttotal: 13.4s\tremaining: 4m 55s\n",
            "133:\tlearn: 4861736.0512923\ttotal: 13.4s\tremaining: 4m 53s\n",
            "134:\tlearn: 4841604.5149259\ttotal: 13.5s\tremaining: 4m 52s\n",
            "135:\tlearn: 4824703.5575486\ttotal: 13.5s\tremaining: 4m 51s\n",
            "136:\tlearn: 4804033.6306684\ttotal: 13.6s\tremaining: 4m 50s\n",
            "137:\tlearn: 4785155.1399882\ttotal: 13.6s\tremaining: 4m 49s\n",
            "138:\tlearn: 4768604.8484754\ttotal: 13.7s\tremaining: 4m 47s\n",
            "139:\tlearn: 4746763.7699813\ttotal: 13.7s\tremaining: 4m 46s\n",
            "140:\tlearn: 4727001.6370554\ttotal: 13.8s\tremaining: 4m 45s\n",
            "141:\tlearn: 4709706.5025297\ttotal: 13.8s\tremaining: 4m 45s\n",
            "142:\tlearn: 4689690.6788678\ttotal: 16.5s\tremaining: 5m 38s\n",
            "143:\tlearn: 4670690.6469876\ttotal: 16.5s\tremaining: 5m 36s\n",
            "144:\tlearn: 4653950.5479265\ttotal: 16.5s\tremaining: 5m 34s\n",
            "145:\tlearn: 4639778.0329199\ttotal: 16.6s\tremaining: 5m 33s\n",
            "146:\tlearn: 4622921.0393550\ttotal: 16.6s\tremaining: 5m 31s\n",
            "147:\tlearn: 4606204.1787273\ttotal: 16.7s\tremaining: 5m 30s\n",
            "148:\tlearn: 4592148.0469946\ttotal: 16.7s\tremaining: 5m 28s\n",
            "149:\tlearn: 4576609.5582978\ttotal: 16.8s\tremaining: 5m 27s\n",
            "150:\tlearn: 4564068.8374874\ttotal: 16.8s\tremaining: 5m 25s\n",
            "151:\tlearn: 4550584.3521923\ttotal: 16.9s\tremaining: 5m 24s\n",
            "152:\tlearn: 4538633.3850615\ttotal: 16.9s\tremaining: 5m 22s\n",
            "153:\tlearn: 4521955.8160113\ttotal: 16.9s\tremaining: 5m 21s\n",
            "154:\tlearn: 4505879.5811863\ttotal: 17s\tremaining: 5m 20s\n",
            "155:\tlearn: 4493008.2373097\ttotal: 17s\tremaining: 5m 18s\n",
            "156:\tlearn: 4479743.2690737\ttotal: 17.1s\tremaining: 5m 17s\n",
            "157:\tlearn: 4467174.3095694\ttotal: 17.1s\tremaining: 5m 16s\n",
            "158:\tlearn: 4449856.4622581\ttotal: 17.2s\tremaining: 5m 14s\n",
            "159:\tlearn: 4433299.9680072\ttotal: 17.2s\tremaining: 5m 13s\n",
            "160:\tlearn: 4420373.8589630\ttotal: 17.3s\tremaining: 5m 12s\n",
            "161:\tlearn: 4408305.8708987\ttotal: 17.3s\tremaining: 5m 10s\n",
            "162:\tlearn: 4396511.9178535\ttotal: 17.4s\tremaining: 5m 9s\n",
            "163:\tlearn: 4382505.4917230\ttotal: 17.4s\tremaining: 5m 8s\n",
            "164:\tlearn: 4366685.8404227\ttotal: 17.4s\tremaining: 5m 7s\n",
            "165:\tlearn: 4352866.0547449\ttotal: 17.5s\tremaining: 5m 5s\n",
            "166:\tlearn: 4340992.0208745\ttotal: 17.5s\tremaining: 5m 4s\n",
            "167:\tlearn: 4327859.5509559\ttotal: 17.6s\tremaining: 5m 3s\n",
            "168:\tlearn: 4314964.9444407\ttotal: 17.6s\tremaining: 5m 2s\n",
            "169:\tlearn: 4300038.4858095\ttotal: 17.6s\tremaining: 5m\n",
            "170:\tlearn: 4288322.2163386\ttotal: 17.7s\tremaining: 4m 59s\n",
            "171:\tlearn: 4272454.1675854\ttotal: 17.7s\tremaining: 4m 58s\n",
            "172:\tlearn: 4262161.2842530\ttotal: 17.8s\tremaining: 4m 57s\n",
            "173:\tlearn: 4252848.4837554\ttotal: 17.8s\tremaining: 4m 56s\n",
            "174:\tlearn: 4243600.2415723\ttotal: 17.9s\tremaining: 4m 55s\n",
            "175:\tlearn: 4234054.2888742\ttotal: 17.9s\tremaining: 4m 53s\n",
            "176:\tlearn: 4223036.1828467\ttotal: 17.9s\tremaining: 4m 52s\n",
            "177:\tlearn: 4216757.6712836\ttotal: 18s\tremaining: 4m 51s\n",
            "178:\tlearn: 4207537.0951991\ttotal: 18s\tremaining: 4m 50s\n",
            "179:\tlearn: 4195096.4184041\ttotal: 18.1s\tremaining: 4m 49s\n",
            "180:\tlearn: 4187277.2363324\ttotal: 18.1s\tremaining: 4m 48s\n",
            "181:\tlearn: 4177437.7413982\ttotal: 18.2s\tremaining: 4m 47s\n",
            "182:\tlearn: 4168190.3861973\ttotal: 18.2s\tremaining: 4m 46s\n",
            "183:\tlearn: 4157863.8892416\ttotal: 18.2s\tremaining: 4m 45s\n",
            "184:\tlearn: 4149185.5265183\ttotal: 18.3s\tremaining: 4m 44s\n",
            "185:\tlearn: 4141885.8858135\ttotal: 18.3s\tremaining: 4m 43s\n",
            "186:\tlearn: 4132791.6330790\ttotal: 18.4s\tremaining: 4m 42s\n",
            "187:\tlearn: 4121647.0367086\ttotal: 18.4s\tremaining: 4m 41s\n",
            "188:\tlearn: 4113773.5118241\ttotal: 18.5s\tremaining: 4m 40s\n",
            "189:\tlearn: 4104143.9735608\ttotal: 18.5s\tremaining: 4m 39s\n",
            "190:\tlearn: 4093288.2089794\ttotal: 18.5s\tremaining: 4m 38s\n",
            "191:\tlearn: 4083793.8080944\ttotal: 18.6s\tremaining: 4m 37s\n",
            "192:\tlearn: 4076242.8512370\ttotal: 18.6s\tremaining: 4m 36s\n",
            "193:\tlearn: 4068346.7400078\ttotal: 18.7s\tremaining: 4m 35s\n",
            "194:\tlearn: 4061777.4213805\ttotal: 18.7s\tremaining: 4m 34s\n",
            "195:\tlearn: 4053050.1778067\ttotal: 18.7s\tremaining: 4m 33s\n",
            "196:\tlearn: 4046030.5392682\ttotal: 18.8s\tremaining: 4m 32s\n",
            "197:\tlearn: 4038139.3836468\ttotal: 18.8s\tremaining: 4m 31s\n",
            "198:\tlearn: 4032477.0241090\ttotal: 18.9s\tremaining: 4m 30s\n",
            "199:\tlearn: 4021949.5710093\ttotal: 18.9s\tremaining: 4m 29s\n",
            "200:\tlearn: 4014722.3379589\ttotal: 18.9s\tremaining: 4m 29s\n",
            "201:\tlearn: 4007742.2386481\ttotal: 19s\tremaining: 4m 28s\n",
            "202:\tlearn: 4000842.1611679\ttotal: 19s\tremaining: 4m 27s\n",
            "203:\tlearn: 3992673.0607766\ttotal: 19.1s\tremaining: 4m 26s\n",
            "204:\tlearn: 3984922.1256256\ttotal: 19.1s\tremaining: 4m 25s\n",
            "205:\tlearn: 3977253.7410390\ttotal: 19.1s\tremaining: 4m 24s\n",
            "206:\tlearn: 3970132.6608656\ttotal: 19.2s\tremaining: 4m 24s\n",
            "207:\tlearn: 3964597.6641629\ttotal: 19.2s\tremaining: 4m 23s\n",
            "208:\tlearn: 3958259.9502758\ttotal: 19.3s\tremaining: 4m 22s\n",
            "209:\tlearn: 3951656.3103999\ttotal: 19.3s\tremaining: 4m 21s\n",
            "210:\tlearn: 3945957.5427821\ttotal: 19.4s\tremaining: 4m 20s\n",
            "211:\tlearn: 3940412.4593898\ttotal: 19.4s\tremaining: 4m 20s\n",
            "212:\tlearn: 3934747.2043747\ttotal: 19.4s\tremaining: 4m 19s\n",
            "213:\tlearn: 3928686.9740767\ttotal: 19.5s\tremaining: 4m 18s\n",
            "214:\tlearn: 3922740.0443358\ttotal: 19.5s\tremaining: 4m 17s\n",
            "215:\tlearn: 3917513.9441790\ttotal: 19.6s\tremaining: 4m 16s\n",
            "216:\tlearn: 3908924.4331862\ttotal: 19.6s\tremaining: 4m 16s\n",
            "217:\tlearn: 3903463.1121337\ttotal: 19.6s\tremaining: 4m 15s\n",
            "218:\tlearn: 3897988.3043853\ttotal: 19.7s\tremaining: 4m 14s\n",
            "219:\tlearn: 3891189.3457039\ttotal: 19.7s\tremaining: 4m 13s\n",
            "220:\tlearn: 3886139.4103077\ttotal: 19.8s\tremaining: 4m 13s\n",
            "221:\tlearn: 3881007.7204601\ttotal: 19.8s\tremaining: 4m 12s\n",
            "222:\tlearn: 3874257.5315003\ttotal: 19.8s\tremaining: 4m 11s\n",
            "223:\tlearn: 3870187.6769771\ttotal: 19.9s\tremaining: 4m 10s\n",
            "224:\tlearn: 3864143.3289925\ttotal: 19.9s\tremaining: 4m 10s\n",
            "225:\tlearn: 3856252.9475123\ttotal: 20s\tremaining: 4m 9s\n",
            "226:\tlearn: 3851603.6981875\ttotal: 20s\tremaining: 4m 8s\n",
            "227:\tlearn: 3846753.7905588\ttotal: 20s\tremaining: 4m 7s\n",
            "228:\tlearn: 3842597.2688328\ttotal: 20.1s\tremaining: 4m 7s\n",
            "229:\tlearn: 3836779.9358384\ttotal: 20.1s\tremaining: 4m 6s\n",
            "230:\tlearn: 3830614.0031455\ttotal: 20.1s\tremaining: 4m 5s\n",
            "231:\tlearn: 3826137.6619795\ttotal: 20.2s\tremaining: 4m 5s\n",
            "232:\tlearn: 3820355.8807285\ttotal: 20.2s\tremaining: 4m 4s\n",
            "233:\tlearn: 3815130.2801606\ttotal: 20.3s\tremaining: 4m 3s\n",
            "234:\tlearn: 3808624.8127838\ttotal: 20.3s\tremaining: 4m 3s\n",
            "235:\tlearn: 3801159.1881043\ttotal: 20.3s\tremaining: 4m 2s\n",
            "236:\tlearn: 3796413.0565662\ttotal: 20.4s\tremaining: 4m 1s\n",
            "237:\tlearn: 3791598.1593778\ttotal: 20.4s\tremaining: 4m\n",
            "238:\tlearn: 3788900.7953241\ttotal: 20.5s\tremaining: 4m\n",
            "239:\tlearn: 3780797.0912371\ttotal: 20.5s\tremaining: 3m 59s\n",
            "240:\tlearn: 3777094.7095375\ttotal: 20.5s\tremaining: 3m 59s\n",
            "241:\tlearn: 3772456.0317368\ttotal: 20.6s\tremaining: 3m 58s\n",
            "242:\tlearn: 3766282.0756482\ttotal: 20.6s\tremaining: 3m 57s\n",
            "243:\tlearn: 3761875.8210058\ttotal: 20.6s\tremaining: 3m 57s\n",
            "244:\tlearn: 3758219.2952672\ttotal: 20.7s\tremaining: 3m 56s\n",
            "245:\tlearn: 3753265.2832189\ttotal: 20.7s\tremaining: 3m 55s\n",
            "246:\tlearn: 3750997.2689245\ttotal: 20.8s\tremaining: 3m 55s\n",
            "247:\tlearn: 3745170.8780019\ttotal: 20.8s\tremaining: 3m 54s\n",
            "248:\tlearn: 3739170.2810746\ttotal: 20.8s\tremaining: 3m 53s\n",
            "249:\tlearn: 3733427.2888388\ttotal: 20.9s\tremaining: 3m 53s\n",
            "250:\tlearn: 3728494.0758814\ttotal: 20.9s\tremaining: 3m 52s\n",
            "251:\tlearn: 3722220.8393225\ttotal: 20.9s\tremaining: 3m 52s\n",
            "252:\tlearn: 3718493.5923546\ttotal: 21s\tremaining: 3m 51s\n",
            "253:\tlearn: 3715217.0585744\ttotal: 21s\tremaining: 3m 50s\n",
            "254:\tlearn: 3709176.1567500\ttotal: 21.1s\tremaining: 3m 50s\n",
            "255:\tlearn: 3705438.7969104\ttotal: 21.1s\tremaining: 3m 49s\n",
            "256:\tlearn: 3700640.6545361\ttotal: 21.1s\tremaining: 3m 49s\n",
            "257:\tlearn: 3696024.2377675\ttotal: 21.2s\tremaining: 3m 48s\n",
            "258:\tlearn: 3692191.0468452\ttotal: 21.2s\tremaining: 3m 48s\n",
            "259:\tlearn: 3688759.9142550\ttotal: 21.3s\tremaining: 3m 47s\n",
            "260:\tlearn: 3684792.0896874\ttotal: 21.3s\tremaining: 3m 46s\n",
            "261:\tlearn: 3681655.2202291\ttotal: 21.3s\tremaining: 3m 46s\n",
            "262:\tlearn: 3677796.7971120\ttotal: 21.4s\tremaining: 3m 45s\n",
            "263:\tlearn: 3674174.3783743\ttotal: 21.4s\tremaining: 3m 45s\n",
            "264:\tlearn: 3670633.0185011\ttotal: 21.4s\tremaining: 3m 44s\n",
            "265:\tlearn: 3663537.7939079\ttotal: 21.5s\tremaining: 3m 44s\n",
            "266:\tlearn: 3657014.9059887\ttotal: 21.5s\tremaining: 3m 43s\n",
            "267:\tlearn: 3653023.3782331\ttotal: 21.6s\tremaining: 3m 43s\n",
            "268:\tlearn: 3649989.0456575\ttotal: 21.6s\tremaining: 3m 42s\n",
            "269:\tlearn: 3644266.3999006\ttotal: 21.6s\tremaining: 3m 42s\n",
            "270:\tlearn: 3641716.5284136\ttotal: 21.7s\tremaining: 3m 41s\n",
            "271:\tlearn: 3639599.1958850\ttotal: 21.7s\tremaining: 3m 40s\n",
            "272:\tlearn: 3635065.1414099\ttotal: 21.7s\tremaining: 3m 40s\n",
            "273:\tlearn: 3631645.1101616\ttotal: 21.8s\tremaining: 3m 39s\n",
            "274:\tlearn: 3627805.1211579\ttotal: 21.8s\tremaining: 3m 39s\n",
            "275:\tlearn: 3623360.7462269\ttotal: 21.9s\tremaining: 3m 38s\n",
            "276:\tlearn: 3618577.5807263\ttotal: 21.9s\tremaining: 3m 38s\n",
            "277:\tlearn: 3617313.2119888\ttotal: 21.9s\tremaining: 3m 37s\n",
            "278:\tlearn: 3610649.8802355\ttotal: 22s\tremaining: 3m 37s\n",
            "279:\tlearn: 3604576.7530761\ttotal: 22s\tremaining: 3m 36s\n",
            "280:\tlearn: 3600833.5527887\ttotal: 22s\tremaining: 3m 36s\n",
            "281:\tlearn: 3597842.8780021\ttotal: 22.1s\tremaining: 3m 35s\n",
            "282:\tlearn: 3590991.3005077\ttotal: 22.1s\tremaining: 3m 35s\n",
            "283:\tlearn: 3587731.5373831\ttotal: 22.2s\tremaining: 3m 35s\n",
            "284:\tlearn: 3582423.4361385\ttotal: 22.2s\tremaining: 3m 34s\n",
            "285:\tlearn: 3577796.7799716\ttotal: 22.3s\tremaining: 3m 34s\n",
            "286:\tlearn: 3575556.9405098\ttotal: 22.3s\tremaining: 3m 33s\n",
            "287:\tlearn: 3569163.1577319\ttotal: 22.3s\tremaining: 3m 33s\n",
            "288:\tlearn: 3565290.0931801\ttotal: 22.4s\tremaining: 3m 32s\n",
            "289:\tlearn: 3561818.6823540\ttotal: 22.4s\tremaining: 3m 32s\n",
            "290:\tlearn: 3558907.7568580\ttotal: 22.4s\tremaining: 3m 31s\n",
            "291:\tlearn: 3555805.3104798\ttotal: 22.5s\tremaining: 3m 31s\n",
            "292:\tlearn: 3549917.3225472\ttotal: 22.5s\tremaining: 3m 30s\n",
            "293:\tlearn: 3546491.3412469\ttotal: 22.6s\tremaining: 3m 30s\n",
            "294:\tlearn: 3542261.2574495\ttotal: 22.6s\tremaining: 3m 30s\n",
            "295:\tlearn: 3538320.2961270\ttotal: 22.6s\tremaining: 3m 30s\n",
            "296:\tlearn: 3534184.3584931\ttotal: 22.6s\tremaining: 3m 29s\n",
            "297:\tlearn: 3530595.8698128\ttotal: 25.3s\tremaining: 3m 53s\n",
            "298:\tlearn: 3525872.7826016\ttotal: 25.3s\tremaining: 3m 52s\n",
            "299:\tlearn: 3522564.1263958\ttotal: 25.4s\tremaining: 3m 52s\n",
            "300:\tlearn: 3519450.5627285\ttotal: 25.4s\tremaining: 3m 51s\n",
            "301:\tlearn: 3515206.5510518\ttotal: 25.5s\tremaining: 3m 51s\n",
            "302:\tlearn: 3512724.2388181\ttotal: 25.5s\tremaining: 3m 50s\n",
            "303:\tlearn: 3510197.9474985\ttotal: 25.5s\tremaining: 3m 50s\n",
            "304:\tlearn: 3506969.8124476\ttotal: 25.6s\tremaining: 3m 49s\n",
            "305:\tlearn: 3501932.4965510\ttotal: 25.6s\tremaining: 3m 49s\n",
            "306:\tlearn: 3499536.8040819\ttotal: 25.6s\tremaining: 3m 48s\n",
            "307:\tlearn: 3494680.0785394\ttotal: 25.7s\tremaining: 3m 48s\n",
            "308:\tlearn: 3491778.5044226\ttotal: 25.7s\tremaining: 3m 47s\n",
            "309:\tlearn: 3487436.4927595\ttotal: 25.8s\tremaining: 3m 47s\n",
            "310:\tlearn: 3482900.6898321\ttotal: 25.8s\tremaining: 3m 46s\n",
            "311:\tlearn: 3480827.7157935\ttotal: 25.8s\tremaining: 3m 46s\n",
            "312:\tlearn: 3476065.2026374\ttotal: 25.9s\tremaining: 3m 45s\n",
            "313:\tlearn: 3472892.7653441\ttotal: 25.9s\tremaining: 3m 45s\n",
            "314:\tlearn: 3468817.9952780\ttotal: 26s\tremaining: 3m 44s\n",
            "315:\tlearn: 3465154.2835521\ttotal: 26s\tremaining: 3m 44s\n",
            "316:\tlearn: 3460172.6480075\ttotal: 26s\tremaining: 3m 43s\n",
            "317:\tlearn: 3457304.4909280\ttotal: 26.1s\tremaining: 3m 43s\n",
            "318:\tlearn: 3455694.1069303\ttotal: 26.1s\tremaining: 3m 42s\n",
            "319:\tlearn: 3451046.1064846\ttotal: 26.1s\tremaining: 3m 42s\n",
            "320:\tlearn: 3445793.4043509\ttotal: 26.2s\tremaining: 3m 41s\n",
            "321:\tlearn: 3440052.1901748\ttotal: 26.2s\tremaining: 3m 41s\n",
            "322:\tlearn: 3435619.6508142\ttotal: 26.3s\tremaining: 3m 41s\n",
            "323:\tlearn: 3431540.2355135\ttotal: 26.3s\tremaining: 3m 40s\n",
            "324:\tlearn: 3429498.1283592\ttotal: 26.3s\tremaining: 3m 40s\n",
            "325:\tlearn: 3426923.0550684\ttotal: 26.4s\tremaining: 3m 39s\n",
            "326:\tlearn: 3421595.5287053\ttotal: 26.4s\tremaining: 3m 39s\n",
            "327:\tlearn: 3416765.3056315\ttotal: 26.4s\tremaining: 3m 38s\n",
            "328:\tlearn: 3414503.7286245\ttotal: 26.5s\tremaining: 3m 38s\n",
            "329:\tlearn: 3409213.8446268\ttotal: 26.5s\tremaining: 3m 37s\n",
            "330:\tlearn: 3407583.5898276\ttotal: 26.6s\tremaining: 3m 37s\n",
            "331:\tlearn: 3402636.4018423\ttotal: 26.6s\tremaining: 3m 37s\n",
            "332:\tlearn: 3400297.7668291\ttotal: 26.6s\tremaining: 3m 36s\n",
            "333:\tlearn: 3395833.4437102\ttotal: 26.7s\tremaining: 3m 36s\n",
            "334:\tlearn: 3391915.9231043\ttotal: 26.7s\tremaining: 3m 35s\n",
            "335:\tlearn: 3388466.6468473\ttotal: 26.8s\tremaining: 3m 35s\n",
            "336:\tlearn: 3387341.0070003\ttotal: 26.8s\tremaining: 3m 35s\n",
            "337:\tlearn: 3384063.9799856\ttotal: 26.8s\tremaining: 3m 34s\n",
            "338:\tlearn: 3379851.2972835\ttotal: 26.9s\tremaining: 3m 34s\n",
            "339:\tlearn: 3378662.8039842\ttotal: 26.9s\tremaining: 3m 33s\n",
            "340:\tlearn: 3375838.2089848\ttotal: 27s\tremaining: 3m 33s\n",
            "341:\tlearn: 3372999.7787725\ttotal: 27s\tremaining: 3m 32s\n",
            "342:\tlearn: 3370225.2060839\ttotal: 27s\tremaining: 3m 32s\n",
            "343:\tlearn: 3367484.6748627\ttotal: 27.1s\tremaining: 3m 32s\n",
            "344:\tlearn: 3364816.8442560\ttotal: 27.1s\tremaining: 3m 31s\n",
            "345:\tlearn: 3361868.8604324\ttotal: 27.2s\tremaining: 3m 31s\n",
            "346:\tlearn: 3357648.2441528\ttotal: 27.2s\tremaining: 3m 30s\n",
            "347:\tlearn: 3354404.8501028\ttotal: 27.2s\tremaining: 3m 30s\n",
            "348:\tlearn: 3352253.7393406\ttotal: 27.3s\tremaining: 3m 30s\n",
            "349:\tlearn: 3348730.0448946\ttotal: 27.3s\tremaining: 3m 29s\n",
            "350:\tlearn: 3345561.0741200\ttotal: 27.4s\tremaining: 3m 29s\n",
            "351:\tlearn: 3342202.3748257\ttotal: 27.4s\tremaining: 3m 28s\n",
            "352:\tlearn: 3337722.6266276\ttotal: 27.4s\tremaining: 3m 28s\n",
            "353:\tlearn: 3334183.0407640\ttotal: 27.5s\tremaining: 3m 28s\n",
            "354:\tlearn: 3330676.5546941\ttotal: 27.5s\tremaining: 3m 27s\n",
            "355:\tlearn: 3327495.4844994\ttotal: 27.5s\tremaining: 3m 27s\n",
            "356:\tlearn: 3323152.4687322\ttotal: 27.6s\tremaining: 3m 26s\n",
            "357:\tlearn: 3321795.8169768\ttotal: 27.6s\tremaining: 3m 26s\n",
            "358:\tlearn: 3319621.1404328\ttotal: 27.6s\tremaining: 3m 26s\n",
            "359:\tlearn: 3315513.9198166\ttotal: 27.7s\tremaining: 3m 25s\n",
            "360:\tlearn: 3311930.1199834\ttotal: 27.7s\tremaining: 3m 25s\n",
            "361:\tlearn: 3307336.9237580\ttotal: 27.8s\tremaining: 3m 25s\n",
            "362:\tlearn: 3303621.0231755\ttotal: 27.8s\tremaining: 3m 24s\n",
            "363:\tlearn: 3298357.2110783\ttotal: 27.8s\tremaining: 3m 24s\n",
            "364:\tlearn: 3295990.2739903\ttotal: 27.9s\tremaining: 3m 23s\n",
            "365:\tlearn: 3292138.0820508\ttotal: 27.9s\tremaining: 3m 23s\n",
            "366:\tlearn: 3289201.2812414\ttotal: 27.9s\tremaining: 3m 23s\n",
            "367:\tlearn: 3285498.2265416\ttotal: 28s\tremaining: 3m 22s\n",
            "368:\tlearn: 3283382.3342263\ttotal: 28s\tremaining: 3m 22s\n",
            "369:\tlearn: 3280758.4660047\ttotal: 28s\tremaining: 3m 22s\n",
            "370:\tlearn: 3278874.4690968\ttotal: 28.1s\tremaining: 3m 21s\n",
            "371:\tlearn: 3274368.2058497\ttotal: 28.1s\tremaining: 3m 21s\n",
            "372:\tlearn: 3270522.4407476\ttotal: 28.2s\tremaining: 3m 20s\n",
            "373:\tlearn: 3265267.1079075\ttotal: 28.2s\tremaining: 3m 20s\n",
            "374:\tlearn: 3259829.1440037\ttotal: 28.2s\tremaining: 3m 20s\n",
            "375:\tlearn: 3256831.0906301\ttotal: 28.3s\tremaining: 3m 19s\n",
            "376:\tlearn: 3253810.5850321\ttotal: 28.3s\tremaining: 3m 19s\n",
            "377:\tlearn: 3251155.7322528\ttotal: 28.3s\tremaining: 3m 19s\n",
            "378:\tlearn: 3247887.0061928\ttotal: 28.4s\tremaining: 3m 18s\n",
            "379:\tlearn: 3245836.4629924\ttotal: 28.4s\tremaining: 3m 18s\n",
            "380:\tlearn: 3241884.6941970\ttotal: 28.4s\tremaining: 3m 18s\n",
            "381:\tlearn: 3240568.4550661\ttotal: 28.5s\tremaining: 3m 17s\n",
            "382:\tlearn: 3238146.2697518\ttotal: 28.5s\tremaining: 3m 17s\n",
            "383:\tlearn: 3234758.2752061\ttotal: 28.5s\tremaining: 3m 17s\n",
            "384:\tlearn: 3232446.8051619\ttotal: 28.6s\tremaining: 3m 16s\n",
            "385:\tlearn: 3228950.5797567\ttotal: 28.6s\tremaining: 3m 16s\n",
            "386:\tlearn: 3225847.8376996\ttotal: 28.7s\tremaining: 3m 16s\n",
            "387:\tlearn: 3222034.1828337\ttotal: 28.7s\tremaining: 3m 15s\n",
            "388:\tlearn: 3220107.0856728\ttotal: 28.7s\tremaining: 3m 15s\n",
            "389:\tlearn: 3217193.8020171\ttotal: 28.8s\tremaining: 3m 15s\n",
            "390:\tlearn: 3213307.4239295\ttotal: 28.8s\tremaining: 3m 14s\n",
            "391:\tlearn: 3210975.4217628\ttotal: 28.8s\tremaining: 3m 14s\n",
            "392:\tlearn: 3209407.3528051\ttotal: 28.9s\tremaining: 3m 14s\n",
            "393:\tlearn: 3205261.8078841\ttotal: 28.9s\tremaining: 3m 13s\n",
            "394:\tlearn: 3201849.6776750\ttotal: 29s\tremaining: 3m 13s\n",
            "395:\tlearn: 3199966.5515995\ttotal: 29s\tremaining: 3m 13s\n",
            "396:\tlearn: 3197984.0630806\ttotal: 29s\tremaining: 3m 12s\n",
            "397:\tlearn: 3194156.6671251\ttotal: 29.1s\tremaining: 3m 12s\n",
            "398:\tlearn: 3192357.5583855\ttotal: 29.1s\tremaining: 3m 12s\n",
            "399:\tlearn: 3189165.5342067\ttotal: 29.1s\tremaining: 3m 11s\n",
            "400:\tlearn: 3185276.6897185\ttotal: 29.2s\tremaining: 3m 11s\n",
            "401:\tlearn: 3181153.1059461\ttotal: 29.2s\tremaining: 3m 11s\n",
            "402:\tlearn: 3178717.1488085\ttotal: 29.2s\tremaining: 3m 10s\n",
            "403:\tlearn: 3177570.7316318\ttotal: 29.3s\tremaining: 3m 10s\n",
            "404:\tlearn: 3175666.7180674\ttotal: 29.3s\tremaining: 3m 10s\n",
            "405:\tlearn: 3171455.3352451\ttotal: 29.3s\tremaining: 3m 9s\n",
            "406:\tlearn: 3166913.8835382\ttotal: 29.4s\tremaining: 3m 9s\n",
            "407:\tlearn: 3161967.0651463\ttotal: 29.4s\tremaining: 3m 9s\n",
            "408:\tlearn: 3158405.4117637\ttotal: 29.5s\tremaining: 3m 8s\n",
            "409:\tlearn: 3156355.4157434\ttotal: 29.5s\tremaining: 3m 8s\n",
            "410:\tlearn: 3152536.4849350\ttotal: 29.5s\tremaining: 3m 8s\n",
            "411:\tlearn: 3151108.5431847\ttotal: 29.6s\tremaining: 3m 8s\n",
            "412:\tlearn: 3148346.0337563\ttotal: 29.6s\tremaining: 3m 7s\n",
            "413:\tlearn: 3145922.3750533\ttotal: 29.6s\tremaining: 3m 7s\n",
            "414:\tlearn: 3143055.5874010\ttotal: 29.7s\tremaining: 3m 7s\n",
            "415:\tlearn: 3140496.5155451\ttotal: 29.7s\tremaining: 3m 6s\n",
            "416:\tlearn: 3138987.4907338\ttotal: 29.8s\tremaining: 3m 6s\n",
            "417:\tlearn: 3137925.7070240\ttotal: 29.8s\tremaining: 3m 6s\n",
            "418:\tlearn: 3135769.9093002\ttotal: 29.8s\tremaining: 3m 5s\n",
            "419:\tlearn: 3131231.1659113\ttotal: 29.9s\tremaining: 3m 5s\n",
            "420:\tlearn: 3128048.4992419\ttotal: 29.9s\tremaining: 3m 5s\n",
            "421:\tlearn: 3124992.1661743\ttotal: 29.9s\tremaining: 3m 5s\n",
            "422:\tlearn: 3120344.1861179\ttotal: 30s\tremaining: 3m 4s\n",
            "423:\tlearn: 3115804.8878420\ttotal: 30s\tremaining: 3m 4s\n",
            "424:\tlearn: 3111548.6674630\ttotal: 30s\tremaining: 3m 4s\n",
            "425:\tlearn: 3107607.7177589\ttotal: 30.1s\tremaining: 3m 3s\n",
            "426:\tlearn: 3105448.9333981\ttotal: 30.1s\tremaining: 3m 3s\n",
            "427:\tlearn: 3104284.0324380\ttotal: 30.2s\tremaining: 3m 3s\n",
            "428:\tlearn: 3103315.3635014\ttotal: 30.2s\tremaining: 3m 3s\n",
            "429:\tlearn: 3099993.7570606\ttotal: 30.2s\tremaining: 3m 2s\n",
            "430:\tlearn: 3096907.4464793\ttotal: 30.3s\tremaining: 3m 2s\n",
            "431:\tlearn: 3091696.8323056\ttotal: 30.3s\tremaining: 3m 2s\n",
            "432:\tlearn: 3090038.5485649\ttotal: 30.3s\tremaining: 3m 1s\n",
            "433:\tlearn: 3088018.4136895\ttotal: 30.4s\tremaining: 3m 1s\n",
            "434:\tlearn: 3085593.8375707\ttotal: 30.4s\tremaining: 3m 1s\n",
            "435:\tlearn: 3082763.7644003\ttotal: 30.4s\tremaining: 3m 1s\n",
            "436:\tlearn: 3079877.9412979\ttotal: 30.5s\tremaining: 3m\n",
            "437:\tlearn: 3074231.1586563\ttotal: 30.5s\tremaining: 3m\n",
            "438:\tlearn: 3072384.7221296\ttotal: 30.6s\tremaining: 3m\n",
            "439:\tlearn: 3066212.0411196\ttotal: 30.6s\tremaining: 3m\n",
            "440:\tlearn: 3065148.4987402\ttotal: 30.6s\tremaining: 2m 59s\n",
            "441:\tlearn: 3060116.0345522\ttotal: 30.7s\tremaining: 2m 59s\n",
            "442:\tlearn: 3056604.5099264\ttotal: 30.7s\tremaining: 2m 59s\n",
            "443:\tlearn: 3052499.7862065\ttotal: 30.8s\tremaining: 2m 59s\n",
            "444:\tlearn: 3050987.0119525\ttotal: 30.8s\tremaining: 2m 58s\n",
            "445:\tlearn: 3048915.7282253\ttotal: 30.8s\tremaining: 2m 58s\n",
            "446:\tlearn: 3046306.9768369\ttotal: 30.9s\tremaining: 2m 58s\n",
            "447:\tlearn: 3044434.8787046\ttotal: 30.9s\tremaining: 2m 58s\n",
            "448:\tlearn: 3041306.5441846\ttotal: 30.9s\tremaining: 2m 57s\n",
            "449:\tlearn: 3039259.5292275\ttotal: 31s\tremaining: 2m 57s\n",
            "450:\tlearn: 3034658.9257532\ttotal: 31s\tremaining: 2m 57s\n",
            "451:\tlearn: 3030775.3599088\ttotal: 31.1s\tremaining: 2m 57s\n",
            "452:\tlearn: 3028562.6244357\ttotal: 31.1s\tremaining: 2m 56s\n",
            "453:\tlearn: 3024853.5070059\ttotal: 31.1s\tremaining: 2m 56s\n",
            "454:\tlearn: 3020997.5204693\ttotal: 31.2s\tremaining: 2m 56s\n",
            "455:\tlearn: 3018300.3298156\ttotal: 31.2s\tremaining: 2m 56s\n",
            "456:\tlearn: 3016791.5984810\ttotal: 31.3s\tremaining: 2m 55s\n",
            "457:\tlearn: 3015023.1020821\ttotal: 31.3s\tremaining: 2m 55s\n",
            "458:\tlearn: 3010806.9360163\ttotal: 31.3s\tremaining: 2m 55s\n",
            "459:\tlearn: 3007376.6233747\ttotal: 31.4s\tremaining: 2m 55s\n",
            "460:\tlearn: 3003415.2170132\ttotal: 31.4s\tremaining: 2m 54s\n",
            "461:\tlearn: 2998341.7676218\ttotal: 31.5s\tremaining: 2m 54s\n",
            "462:\tlearn: 2994465.1161123\ttotal: 31.5s\tremaining: 2m 54s\n",
            "463:\tlearn: 2992421.9745655\ttotal: 31.5s\tremaining: 2m 54s\n",
            "464:\tlearn: 2989274.3619687\ttotal: 31.6s\tremaining: 2m 54s\n",
            "465:\tlearn: 2987561.1632823\ttotal: 31.6s\tremaining: 2m 53s\n",
            "466:\tlearn: 2986048.0019772\ttotal: 31.7s\tremaining: 2m 53s\n",
            "467:\tlearn: 2983243.4505100\ttotal: 31.7s\tremaining: 2m 53s\n",
            "468:\tlearn: 2980872.3544725\ttotal: 31.7s\tremaining: 2m 53s\n",
            "469:\tlearn: 2978223.3914298\ttotal: 31.8s\tremaining: 2m 52s\n",
            "470:\tlearn: 2974916.4084990\ttotal: 31.8s\tremaining: 2m 52s\n",
            "471:\tlearn: 2972333.8697039\ttotal: 31.8s\tremaining: 2m 52s\n",
            "472:\tlearn: 2970052.6666041\ttotal: 31.9s\tremaining: 2m 52s\n",
            "473:\tlearn: 2965791.0051541\ttotal: 31.9s\tremaining: 2m 51s\n",
            "474:\tlearn: 2964437.5491296\ttotal: 32s\tremaining: 2m 51s\n",
            "475:\tlearn: 2960963.5366025\ttotal: 32s\tremaining: 2m 51s\n",
            "476:\tlearn: 2958932.7085446\ttotal: 32s\tremaining: 2m 51s\n",
            "477:\tlearn: 2957418.2672566\ttotal: 32.1s\tremaining: 2m 50s\n",
            "478:\tlearn: 2956175.8890425\ttotal: 32.1s\tremaining: 2m 50s\n",
            "479:\tlearn: 2953908.6892621\ttotal: 32.1s\tremaining: 2m 50s\n",
            "480:\tlearn: 2949439.6134055\ttotal: 32.2s\tremaining: 2m 50s\n",
            "481:\tlearn: 2945027.0207169\ttotal: 32.2s\tremaining: 2m 50s\n",
            "482:\tlearn: 2942946.6353345\ttotal: 32.2s\tremaining: 2m 49s\n",
            "483:\tlearn: 2937431.6288711\ttotal: 32.3s\tremaining: 2m 49s\n",
            "484:\tlearn: 2935951.3108568\ttotal: 32.3s\tremaining: 2m 49s\n",
            "485:\tlearn: 2931370.2132546\ttotal: 32.4s\tremaining: 2m 49s\n",
            "486:\tlearn: 2928322.1354396\ttotal: 32.4s\tremaining: 2m 48s\n",
            "487:\tlearn: 2925411.2873722\ttotal: 32.4s\tremaining: 2m 48s\n",
            "488:\tlearn: 2924197.9311610\ttotal: 32.5s\tremaining: 2m 48s\n",
            "489:\tlearn: 2921206.6295078\ttotal: 32.5s\tremaining: 2m 48s\n",
            "490:\tlearn: 2916336.2204319\ttotal: 32.5s\tremaining: 2m 47s\n",
            "491:\tlearn: 2913928.7282144\ttotal: 32.6s\tremaining: 2m 47s\n",
            "492:\tlearn: 2911828.7344288\ttotal: 32.6s\tremaining: 2m 47s\n",
            "493:\tlearn: 2910976.2784926\ttotal: 32.6s\tremaining: 2m 47s\n",
            "494:\tlearn: 2907333.5889162\ttotal: 32.7s\tremaining: 2m 47s\n",
            "495:\tlearn: 2905192.0030786\ttotal: 32.7s\tremaining: 2m 46s\n",
            "496:\tlearn: 2901854.5888866\ttotal: 32.8s\tremaining: 2m 46s\n",
            "497:\tlearn: 2898217.5275011\ttotal: 32.8s\tremaining: 2m 46s\n",
            "498:\tlearn: 2896852.9299855\ttotal: 32.8s\tremaining: 2m 46s\n",
            "499:\tlearn: 2892872.4891100\ttotal: 32.9s\tremaining: 2m 46s\n",
            "500:\tlearn: 2889400.8298071\ttotal: 32.9s\tremaining: 2m 45s\n",
            "501:\tlearn: 2886560.0149745\ttotal: 33s\tremaining: 2m 45s\n",
            "502:\tlearn: 2882018.4264152\ttotal: 33s\tremaining: 2m 45s\n",
            "503:\tlearn: 2879360.4603641\ttotal: 33s\tremaining: 2m 45s\n",
            "504:\tlearn: 2877455.0494790\ttotal: 35.7s\tremaining: 2m 58s\n",
            "505:\tlearn: 2876254.7867644\ttotal: 35.7s\tremaining: 2m 58s\n",
            "506:\tlearn: 2871640.3783204\ttotal: 35.7s\tremaining: 2m 57s\n",
            "507:\tlearn: 2867979.1129172\ttotal: 35.8s\tremaining: 2m 57s\n",
            "508:\tlearn: 2865833.4081287\ttotal: 35.8s\tremaining: 2m 57s\n",
            "509:\tlearn: 2861686.2362344\ttotal: 35.8s\tremaining: 2m 57s\n",
            "510:\tlearn: 2860188.7777807\ttotal: 35.9s\tremaining: 2m 56s\n",
            "511:\tlearn: 2857041.5492205\ttotal: 35.9s\tremaining: 2m 56s\n",
            "512:\tlearn: 2854164.5410576\ttotal: 35.9s\tremaining: 2m 56s\n",
            "513:\tlearn: 2849320.2473116\ttotal: 36s\tremaining: 2m 56s\n",
            "514:\tlearn: 2847663.0537423\ttotal: 36s\tremaining: 2m 55s\n",
            "515:\tlearn: 2846193.3743883\ttotal: 36.1s\tremaining: 2m 55s\n",
            "516:\tlearn: 2842314.8673565\ttotal: 36.1s\tremaining: 2m 55s\n",
            "517:\tlearn: 2840977.3908406\ttotal: 36.1s\tremaining: 2m 55s\n",
            "518:\tlearn: 2837234.7982757\ttotal: 36.2s\tremaining: 2m 54s\n",
            "519:\tlearn: 2832465.8243436\ttotal: 36.2s\tremaining: 2m 54s\n",
            "520:\tlearn: 2830670.5614584\ttotal: 36.3s\tremaining: 2m 54s\n",
            "521:\tlearn: 2828832.7275172\ttotal: 36.3s\tremaining: 2m 54s\n",
            "522:\tlearn: 2826981.2139585\ttotal: 36.3s\tremaining: 2m 54s\n",
            "523:\tlearn: 2823920.1921699\ttotal: 36.4s\tremaining: 2m 53s\n",
            "524:\tlearn: 2822146.8589675\ttotal: 36.4s\tremaining: 2m 53s\n",
            "525:\tlearn: 2820243.6357504\ttotal: 36.4s\tremaining: 2m 53s\n",
            "526:\tlearn: 2817386.7891848\ttotal: 36.5s\tremaining: 2m 53s\n",
            "527:\tlearn: 2812661.2193546\ttotal: 36.5s\tremaining: 2m 52s\n",
            "528:\tlearn: 2808057.4441747\ttotal: 36.6s\tremaining: 2m 52s\n",
            "529:\tlearn: 2806611.7249312\ttotal: 36.6s\tremaining: 2m 52s\n",
            "530:\tlearn: 2805940.4571027\ttotal: 36.6s\tremaining: 2m 52s\n",
            "531:\tlearn: 2804995.6091446\ttotal: 36.7s\tremaining: 2m 52s\n",
            "532:\tlearn: 2803523.6844609\ttotal: 36.7s\tremaining: 2m 51s\n",
            "533:\tlearn: 2799191.4290330\ttotal: 36.8s\tremaining: 2m 51s\n",
            "534:\tlearn: 2795868.8937845\ttotal: 36.8s\tremaining: 2m 51s\n",
            "535:\tlearn: 2791181.8111650\ttotal: 36.8s\tremaining: 2m 51s\n",
            "536:\tlearn: 2787168.5640870\ttotal: 36.9s\tremaining: 2m 51s\n",
            "537:\tlearn: 2783872.1468955\ttotal: 36.9s\tremaining: 2m 50s\n",
            "538:\tlearn: 2782166.3895369\ttotal: 36.9s\tremaining: 2m 50s\n",
            "539:\tlearn: 2780612.7823197\ttotal: 37s\tremaining: 2m 50s\n",
            "540:\tlearn: 2779216.5302030\ttotal: 37s\tremaining: 2m 50s\n",
            "541:\tlearn: 2775261.5437426\ttotal: 37.1s\tremaining: 2m 49s\n",
            "542:\tlearn: 2771823.7658166\ttotal: 37.1s\tremaining: 2m 49s\n",
            "543:\tlearn: 2768785.6388655\ttotal: 37.1s\tremaining: 2m 49s\n",
            "544:\tlearn: 2768153.0259922\ttotal: 37.2s\tremaining: 2m 49s\n",
            "545:\tlearn: 2764973.4703095\ttotal: 37.2s\tremaining: 2m 49s\n",
            "546:\tlearn: 2760468.8127006\ttotal: 37.2s\tremaining: 2m 48s\n",
            "547:\tlearn: 2759953.5985804\ttotal: 37.3s\tremaining: 2m 48s\n",
            "548:\tlearn: 2758938.3375080\ttotal: 37.3s\tremaining: 2m 48s\n",
            "549:\tlearn: 2757531.1022561\ttotal: 37.4s\tremaining: 2m 48s\n",
            "550:\tlearn: 2755466.7660822\ttotal: 37.4s\tremaining: 2m 48s\n",
            "551:\tlearn: 2752758.5210292\ttotal: 37.4s\tremaining: 2m 47s\n",
            "552:\tlearn: 2751703.4335649\ttotal: 37.5s\tremaining: 2m 47s\n",
            "553:\tlearn: 2749040.9901490\ttotal: 37.5s\tremaining: 2m 47s\n",
            "554:\tlearn: 2744174.6693081\ttotal: 37.5s\tremaining: 2m 47s\n",
            "555:\tlearn: 2743620.3417723\ttotal: 37.6s\tremaining: 2m 47s\n",
            "556:\tlearn: 2742910.7728732\ttotal: 37.6s\tremaining: 2m 46s\n",
            "557:\tlearn: 2741532.7817902\ttotal: 37.7s\tremaining: 2m 46s\n",
            "558:\tlearn: 2738712.3634062\ttotal: 37.7s\tremaining: 2m 46s\n",
            "559:\tlearn: 2734238.6940302\ttotal: 37.7s\tremaining: 2m 46s\n",
            "560:\tlearn: 2732610.2724211\ttotal: 37.8s\tremaining: 2m 45s\n",
            "561:\tlearn: 2730859.0716623\ttotal: 37.8s\tremaining: 2m 45s\n",
            "562:\tlearn: 2726559.2264635\ttotal: 37.8s\tremaining: 2m 45s\n",
            "563:\tlearn: 2725158.6527143\ttotal: 37.9s\tremaining: 2m 45s\n",
            "564:\tlearn: 2720973.1103097\ttotal: 37.9s\tremaining: 2m 45s\n",
            "565:\tlearn: 2716797.5588758\ttotal: 37.9s\tremaining: 2m 44s\n",
            "566:\tlearn: 2714897.2085468\ttotal: 38s\tremaining: 2m 44s\n",
            "567:\tlearn: 2711834.8097262\ttotal: 38s\tremaining: 2m 44s\n",
            "568:\tlearn: 2709885.3737364\ttotal: 38.1s\tremaining: 2m 44s\n",
            "569:\tlearn: 2708451.3070319\ttotal: 38.1s\tremaining: 2m 44s\n",
            "570:\tlearn: 2704792.1213669\ttotal: 38.1s\tremaining: 2m 43s\n",
            "571:\tlearn: 2702419.3879302\ttotal: 38.2s\tremaining: 2m 43s\n",
            "572:\tlearn: 2700390.8012538\ttotal: 38.2s\tremaining: 2m 43s\n",
            "573:\tlearn: 2697423.9605498\ttotal: 38.2s\tremaining: 2m 43s\n",
            "574:\tlearn: 2694738.2271118\ttotal: 38.3s\tremaining: 2m 43s\n",
            "575:\tlearn: 2692883.6705563\ttotal: 38.3s\tremaining: 2m 42s\n",
            "576:\tlearn: 2691327.2871432\ttotal: 38.3s\tremaining: 2m 42s\n",
            "577:\tlearn: 2688444.1174111\ttotal: 38.4s\tremaining: 2m 42s\n",
            "578:\tlearn: 2686883.7755109\ttotal: 38.4s\tremaining: 2m 42s\n",
            "579:\tlearn: 2683270.5234499\ttotal: 38.5s\tremaining: 2m 42s\n",
            "580:\tlearn: 2680269.0079047\ttotal: 38.5s\tremaining: 2m 41s\n",
            "581:\tlearn: 2678667.8675278\ttotal: 38.5s\tremaining: 2m 41s\n",
            "582:\tlearn: 2675921.9771622\ttotal: 38.6s\tremaining: 2m 41s\n",
            "583:\tlearn: 2673806.1469751\ttotal: 38.6s\tremaining: 2m 41s\n",
            "584:\tlearn: 2670261.9757451\ttotal: 38.6s\tremaining: 2m 41s\n",
            "585:\tlearn: 2667963.2143017\ttotal: 38.7s\tremaining: 2m 40s\n",
            "586:\tlearn: 2667449.3106936\ttotal: 38.7s\tremaining: 2m 40s\n",
            "587:\tlearn: 2665274.7497331\ttotal: 38.8s\tremaining: 2m 40s\n",
            "588:\tlearn: 2663545.1375675\ttotal: 38.8s\tremaining: 2m 40s\n",
            "589:\tlearn: 2659848.7470506\ttotal: 38.8s\tremaining: 2m 40s\n",
            "590:\tlearn: 2655979.7615592\ttotal: 38.9s\tremaining: 2m 40s\n",
            "591:\tlearn: 2652947.1003989\ttotal: 38.9s\tremaining: 2m 39s\n",
            "592:\tlearn: 2651443.6480636\ttotal: 38.9s\tremaining: 2m 39s\n",
            "593:\tlearn: 2649950.8236865\ttotal: 39s\tremaining: 2m 39s\n",
            "594:\tlearn: 2646935.1206444\ttotal: 39s\tremaining: 2m 39s\n",
            "595:\tlearn: 2644609.6219780\ttotal: 39s\tremaining: 2m 39s\n",
            "596:\tlearn: 2641066.6966183\ttotal: 39.1s\tremaining: 2m 38s\n",
            "597:\tlearn: 2637673.6496416\ttotal: 39.1s\tremaining: 2m 38s\n",
            "598:\tlearn: 2635950.7469331\ttotal: 39.2s\tremaining: 2m 38s\n",
            "599:\tlearn: 2632057.7453359\ttotal: 39.2s\tremaining: 2m 38s\n",
            "600:\tlearn: 2631096.3571859\ttotal: 39.2s\tremaining: 2m 38s\n",
            "601:\tlearn: 2628395.2467046\ttotal: 39.2s\tremaining: 2m 38s\n",
            "602:\tlearn: 2624631.0876494\ttotal: 41.9s\tremaining: 2m 48s\n",
            "603:\tlearn: 2621531.0314098\ttotal: 41.9s\tremaining: 2m 48s\n",
            "604:\tlearn: 2617821.9146515\ttotal: 42s\tremaining: 2m 48s\n",
            "605:\tlearn: 2615280.1532329\ttotal: 42s\tremaining: 2m 48s\n",
            "606:\tlearn: 2612920.7157832\ttotal: 42s\tremaining: 2m 47s\n",
            "607:\tlearn: 2610449.8470247\ttotal: 44.7s\tremaining: 2m 58s\n",
            "608:\tlearn: 2607902.8076931\ttotal: 44.7s\tremaining: 2m 58s\n",
            "609:\tlearn: 2604606.0583728\ttotal: 47.3s\tremaining: 3m 8s\n",
            "610:\tlearn: 2602354.5785556\ttotal: 47.4s\tremaining: 3m 7s\n",
            "611:\tlearn: 2598888.4007149\ttotal: 47.4s\tremaining: 3m 7s\n",
            "612:\tlearn: 2597523.9227672\ttotal: 47.4s\tremaining: 3m 7s\n",
            "613:\tlearn: 2593828.4894236\ttotal: 47.5s\tremaining: 3m 7s\n",
            "614:\tlearn: 2591474.7866799\ttotal: 47.5s\tremaining: 3m 7s\n",
            "615:\tlearn: 2587938.3716598\ttotal: 47.6s\tremaining: 3m 6s\n",
            "616:\tlearn: 2584259.9775622\ttotal: 47.6s\tremaining: 3m 6s\n",
            "617:\tlearn: 2582678.1305297\ttotal: 50.2s\tremaining: 3m 16s\n",
            "618:\tlearn: 2580195.0076975\ttotal: 50.3s\tremaining: 3m 16s\n",
            "619:\tlearn: 2576844.0676465\ttotal: 50.3s\tremaining: 3m 16s\n",
            "620:\tlearn: 2574775.6136009\ttotal: 50.3s\tremaining: 3m 15s\n",
            "621:\tlearn: 2573118.8800487\ttotal: 50.4s\tremaining: 3m 15s\n",
            "622:\tlearn: 2570462.4849336\ttotal: 50.4s\tremaining: 3m 15s\n",
            "623:\tlearn: 2568340.4030768\ttotal: 50.4s\tremaining: 3m 15s\n",
            "624:\tlearn: 2565740.7278796\ttotal: 50.5s\tremaining: 3m 14s\n",
            "625:\tlearn: 2563156.5322134\ttotal: 50.5s\tremaining: 3m 14s\n",
            "626:\tlearn: 2561140.0869393\ttotal: 50.6s\tremaining: 3m 14s\n",
            "627:\tlearn: 2557809.1796513\ttotal: 50.6s\tremaining: 3m 14s\n",
            "628:\tlearn: 2556453.9808908\ttotal: 50.6s\tremaining: 3m 13s\n",
            "629:\tlearn: 2553778.3518718\ttotal: 50.7s\tremaining: 3m 13s\n",
            "630:\tlearn: 2551243.4783867\ttotal: 50.7s\tremaining: 3m 13s\n",
            "631:\tlearn: 2549524.5125194\ttotal: 50.7s\tremaining: 3m 13s\n",
            "632:\tlearn: 2546717.1815083\ttotal: 50.8s\tremaining: 3m 12s\n",
            "633:\tlearn: 2544289.6862187\ttotal: 50.8s\tremaining: 3m 12s\n",
            "634:\tlearn: 2541602.7206170\ttotal: 50.9s\tremaining: 3m 12s\n",
            "635:\tlearn: 2539622.0679844\ttotal: 50.9s\tremaining: 3m 12s\n",
            "636:\tlearn: 2537758.7582800\ttotal: 50.9s\tremaining: 3m 12s\n",
            "637:\tlearn: 2534762.4920136\ttotal: 53.6s\tremaining: 3m 21s\n",
            "638:\tlearn: 2531981.3159989\ttotal: 53.6s\tremaining: 3m 21s\n",
            "639:\tlearn: 2527647.2773390\ttotal: 53.6s\tremaining: 3m 21s\n",
            "640:\tlearn: 2524651.5979322\ttotal: 53.7s\tremaining: 3m 21s\n",
            "641:\tlearn: 2522857.7010747\ttotal: 53.7s\tremaining: 3m 20s\n",
            "642:\tlearn: 2520478.2087516\ttotal: 53.8s\tremaining: 3m 20s\n",
            "643:\tlearn: 2518422.7144274\ttotal: 53.8s\tremaining: 3m 20s\n",
            "644:\tlearn: 2516987.4294151\ttotal: 53.8s\tremaining: 3m 19s\n",
            "645:\tlearn: 2515584.7185179\ttotal: 53.9s\tremaining: 3m 19s\n",
            "646:\tlearn: 2514826.5108483\ttotal: 53.9s\tremaining: 3m 19s\n",
            "647:\tlearn: 2511160.2643677\ttotal: 53.9s\tremaining: 3m 19s\n",
            "648:\tlearn: 2508653.6205563\ttotal: 54s\tremaining: 3m 18s\n",
            "649:\tlearn: 2505593.4471920\ttotal: 54s\tremaining: 3m 18s\n",
            "650:\tlearn: 2503759.1977598\ttotal: 54s\tremaining: 3m 18s\n",
            "651:\tlearn: 2500727.8377460\ttotal: 54.1s\tremaining: 3m 18s\n",
            "652:\tlearn: 2499654.2127228\ttotal: 54.1s\tremaining: 3m 17s\n",
            "653:\tlearn: 2497261.7524655\ttotal: 54.2s\tremaining: 3m 17s\n",
            "654:\tlearn: 2495281.5558796\ttotal: 54.2s\tremaining: 3m 17s\n",
            "655:\tlearn: 2492941.5097767\ttotal: 54.2s\tremaining: 3m 17s\n",
            "656:\tlearn: 2492019.4898662\ttotal: 54.3s\tremaining: 3m 16s\n",
            "657:\tlearn: 2488631.6056597\ttotal: 54.3s\tremaining: 3m 16s\n",
            "658:\tlearn: 2484801.2371375\ttotal: 54.3s\tremaining: 3m 16s\n",
            "659:\tlearn: 2482364.4933197\ttotal: 54.4s\tremaining: 3m 16s\n",
            "660:\tlearn: 2480821.7798522\ttotal: 54.4s\tremaining: 3m 15s\n",
            "661:\tlearn: 2478998.5677325\ttotal: 54.5s\tremaining: 3m 15s\n",
            "662:\tlearn: 2476195.8124323\ttotal: 54.5s\tremaining: 3m 15s\n",
            "663:\tlearn: 2474814.4217150\ttotal: 54.5s\tremaining: 3m 15s\n",
            "664:\tlearn: 2472166.7775942\ttotal: 54.6s\tremaining: 3m 14s\n",
            "665:\tlearn: 2468959.4364252\ttotal: 54.6s\tremaining: 3m 14s\n",
            "666:\tlearn: 2466648.8894521\ttotal: 54.7s\tremaining: 3m 14s\n",
            "667:\tlearn: 2464393.1053359\ttotal: 54.7s\tremaining: 3m 14s\n",
            "668:\tlearn: 2461065.2872053\ttotal: 54.7s\tremaining: 3m 13s\n",
            "669:\tlearn: 2459067.4206455\ttotal: 54.8s\tremaining: 3m 13s\n",
            "670:\tlearn: 2455014.8683140\ttotal: 54.8s\tremaining: 3m 13s\n",
            "671:\tlearn: 2451646.2157321\ttotal: 54.8s\tremaining: 3m 13s\n",
            "672:\tlearn: 2449896.8109247\ttotal: 54.9s\tremaining: 3m 12s\n",
            "673:\tlearn: 2448495.1486229\ttotal: 54.9s\tremaining: 3m 12s\n",
            "674:\tlearn: 2446354.5804526\ttotal: 55s\tremaining: 3m 12s\n",
            "675:\tlearn: 2444998.7262634\ttotal: 55s\tremaining: 3m 12s\n",
            "676:\tlearn: 2442300.3529262\ttotal: 55s\tremaining: 3m 11s\n",
            "677:\tlearn: 2440776.6685523\ttotal: 55.1s\tremaining: 3m 11s\n",
            "678:\tlearn: 2438752.8864242\ttotal: 55.1s\tremaining: 3m 11s\n",
            "679:\tlearn: 2437690.9617241\ttotal: 55.1s\tremaining: 3m 11s\n",
            "680:\tlearn: 2435914.5075210\ttotal: 55.2s\tremaining: 3m 11s\n",
            "681:\tlearn: 2434142.1850310\ttotal: 55.2s\tremaining: 3m 10s\n",
            "682:\tlearn: 2431538.6954484\ttotal: 55.3s\tremaining: 3m 10s\n",
            "683:\tlearn: 2429718.3849999\ttotal: 55.3s\tremaining: 3m 10s\n",
            "684:\tlearn: 2426604.7606521\ttotal: 55.3s\tremaining: 3m 10s\n",
            "685:\tlearn: 2425972.7565333\ttotal: 55.3s\tremaining: 3m 10s\n",
            "686:\tlearn: 2424297.9267089\ttotal: 58s\tremaining: 3m 18s\n",
            "687:\tlearn: 2421130.3776846\ttotal: 58s\tremaining: 3m 18s\n",
            "688:\tlearn: 2419773.1048673\ttotal: 58.1s\tremaining: 3m 18s\n",
            "689:\tlearn: 2418775.6428984\ttotal: 58.1s\tremaining: 3m 18s\n",
            "690:\tlearn: 2417735.6037177\ttotal: 58.2s\tremaining: 3m 17s\n",
            "691:\tlearn: 2415098.0455303\ttotal: 58.2s\tremaining: 3m 17s\n",
            "692:\tlearn: 2412561.8771361\ttotal: 58.2s\tremaining: 3m 17s\n",
            "693:\tlearn: 2411769.5992432\ttotal: 58.3s\tremaining: 3m 17s\n",
            "694:\tlearn: 2409152.0758928\ttotal: 58.3s\tremaining: 3m 16s\n",
            "695:\tlearn: 2407580.5304823\ttotal: 58.4s\tremaining: 3m 16s\n",
            "696:\tlearn: 2406069.1341650\ttotal: 58.4s\tremaining: 3m 16s\n",
            "697:\tlearn: 2404919.2201515\ttotal: 58.4s\tremaining: 3m 16s\n",
            "698:\tlearn: 2403655.7281583\ttotal: 58.5s\tremaining: 3m 15s\n",
            "699:\tlearn: 2399809.4120544\ttotal: 58.5s\tremaining: 3m 15s\n",
            "700:\tlearn: 2397542.2395063\ttotal: 58.5s\tremaining: 3m 15s\n",
            "701:\tlearn: 2395562.6844124\ttotal: 58.6s\tremaining: 3m 15s\n",
            "702:\tlearn: 2392562.2831856\ttotal: 58.6s\tremaining: 3m 14s\n",
            "703:\tlearn: 2390073.3625017\ttotal: 58.7s\tremaining: 3m 14s\n",
            "704:\tlearn: 2389223.0563645\ttotal: 58.7s\tremaining: 3m 14s\n",
            "705:\tlearn: 2388242.4799620\ttotal: 58.7s\tremaining: 3m 14s\n",
            "706:\tlearn: 2385157.6542160\ttotal: 58.8s\tremaining: 3m 13s\n",
            "707:\tlearn: 2384465.4936046\ttotal: 58.8s\tremaining: 3m 13s\n",
            "708:\tlearn: 2383195.0942785\ttotal: 58.8s\tremaining: 3m 13s\n",
            "709:\tlearn: 2380652.5341294\ttotal: 58.9s\tremaining: 3m 13s\n",
            "710:\tlearn: 2378835.1559863\ttotal: 58.9s\tremaining: 3m 12s\n",
            "711:\tlearn: 2375729.3795588\ttotal: 58.9s\tremaining: 3m 12s\n",
            "712:\tlearn: 2372754.3927837\ttotal: 1m 1s\tremaining: 3m 21s\n",
            "713:\tlearn: 2369445.9028698\ttotal: 1m 1s\tremaining: 3m 20s\n",
            "714:\tlearn: 2368388.8865463\ttotal: 1m 1s\tremaining: 3m 20s\n",
            "715:\tlearn: 2367684.1092797\ttotal: 1m 1s\tremaining: 3m 20s\n",
            "716:\tlearn: 2365972.7767157\ttotal: 1m 1s\tremaining: 3m 20s\n",
            "717:\tlearn: 2364699.7917545\ttotal: 1m 1s\tremaining: 3m 19s\n",
            "718:\tlearn: 2363407.8885102\ttotal: 1m 1s\tremaining: 3m 19s\n",
            "719:\tlearn: 2360926.0401634\ttotal: 1m 1s\tremaining: 3m 19s\n",
            "720:\tlearn: 2358634.3702691\ttotal: 1m 1s\tremaining: 3m 19s\n",
            "721:\tlearn: 2355114.0998989\ttotal: 1m 1s\tremaining: 3m 19s\n",
            "722:\tlearn: 2354329.1576736\ttotal: 1m 1s\tremaining: 3m 18s\n",
            "723:\tlearn: 2351948.5396342\ttotal: 1m 2s\tremaining: 3m 18s\n",
            "724:\tlearn: 2350418.1497190\ttotal: 1m 2s\tremaining: 3m 18s\n",
            "725:\tlearn: 2348131.2180224\ttotal: 1m 2s\tremaining: 3m 18s\n",
            "726:\tlearn: 2346751.7784093\ttotal: 1m 2s\tremaining: 3m 17s\n",
            "727:\tlearn: 2346054.3039997\ttotal: 1m 2s\tremaining: 3m 17s\n",
            "728:\tlearn: 2345458.2649912\ttotal: 1m 2s\tremaining: 3m 17s\n",
            "729:\tlearn: 2343870.1379031\ttotal: 1m 2s\tremaining: 3m 17s\n",
            "730:\tlearn: 2342734.5770197\ttotal: 1m 2s\tremaining: 3m 16s\n",
            "731:\tlearn: 2339792.7961381\ttotal: 1m 2s\tremaining: 3m 16s\n",
            "732:\tlearn: 2339008.5735474\ttotal: 1m 2s\tremaining: 3m 16s\n",
            "733:\tlearn: 2337253.4460033\ttotal: 1m 2s\tremaining: 3m 16s\n",
            "734:\tlearn: 2336041.1451596\ttotal: 1m 2s\tremaining: 3m 15s\n",
            "735:\tlearn: 2334690.6178733\ttotal: 1m 2s\tremaining: 3m 15s\n",
            "736:\tlearn: 2331574.1412759\ttotal: 1m 2s\tremaining: 3m 15s\n",
            "737:\tlearn: 2329046.6720385\ttotal: 1m 2s\tremaining: 3m 15s\n",
            "738:\tlearn: 2325909.3216811\ttotal: 1m 2s\tremaining: 3m 14s\n",
            "739:\tlearn: 2325139.9904405\ttotal: 1m 2s\tremaining: 3m 14s\n",
            "740:\tlearn: 2324236.2910018\ttotal: 1m 2s\tremaining: 3m 14s\n",
            "741:\tlearn: 2321773.2511709\ttotal: 1m 2s\tremaining: 3m 14s\n",
            "742:\tlearn: 2321103.6618229\ttotal: 1m 2s\tremaining: 3m 14s\n",
            "743:\tlearn: 2319652.4570016\ttotal: 1m 2s\tremaining: 3m 13s\n",
            "744:\tlearn: 2317976.0527698\ttotal: 1m 2s\tremaining: 3m 13s\n",
            "745:\tlearn: 2315814.6270495\ttotal: 1m 2s\tremaining: 3m 13s\n",
            "746:\tlearn: 2315244.7630361\ttotal: 1m 2s\tremaining: 3m 13s\n",
            "747:\tlearn: 2311634.0333294\ttotal: 1m 2s\tremaining: 3m 12s\n",
            "748:\tlearn: 2309390.3513788\ttotal: 1m 2s\tremaining: 3m 12s\n",
            "749:\tlearn: 2306956.9465019\ttotal: 1m 2s\tremaining: 3m 12s\n",
            "750:\tlearn: 2304730.1677126\ttotal: 1m 3s\tremaining: 3m 12s\n",
            "751:\tlearn: 2302588.0344164\ttotal: 1m 5s\tremaining: 3m 20s\n",
            "752:\tlearn: 2298639.7487338\ttotal: 1m 5s\tremaining: 3m 19s\n",
            "753:\tlearn: 2297971.4594012\ttotal: 1m 5s\tremaining: 3m 19s\n",
            "754:\tlearn: 2296526.8258646\ttotal: 1m 5s\tremaining: 3m 19s\n",
            "755:\tlearn: 2294369.2932632\ttotal: 1m 5s\tremaining: 3m 19s\n",
            "756:\tlearn: 2292970.7644425\ttotal: 1m 5s\tremaining: 3m 19s\n",
            "757:\tlearn: 2291778.0217504\ttotal: 1m 8s\tremaining: 3m 26s\n",
            "758:\tlearn: 2289726.3003046\ttotal: 1m 8s\tremaining: 3m 26s\n",
            "759:\tlearn: 2288872.5172010\ttotal: 1m 8s\tremaining: 3m 26s\n",
            "760:\tlearn: 2287304.2533818\ttotal: 1m 8s\tremaining: 3m 25s\n",
            "761:\tlearn: 2285323.2945898\ttotal: 1m 8s\tremaining: 3m 25s\n",
            "762:\tlearn: 2283383.9137066\ttotal: 1m 8s\tremaining: 3m 25s\n",
            "763:\tlearn: 2282585.1779415\ttotal: 1m 8s\tremaining: 3m 25s\n",
            "764:\tlearn: 2281927.0074151\ttotal: 1m 8s\tremaining: 3m 24s\n",
            "765:\tlearn: 2280563.1792358\ttotal: 1m 8s\tremaining: 3m 24s\n",
            "766:\tlearn: 2280034.8948317\ttotal: 1m 8s\tremaining: 3m 24s\n",
            "767:\tlearn: 2276237.0155497\ttotal: 1m 8s\tremaining: 3m 24s\n",
            "768:\tlearn: 2273853.2147423\ttotal: 1m 8s\tremaining: 3m 24s\n",
            "769:\tlearn: 2271333.1440043\ttotal: 1m 8s\tremaining: 3m 23s\n",
            "770:\tlearn: 2269694.6157952\ttotal: 1m 9s\tremaining: 3m 23s\n",
            "771:\tlearn: 2266212.1418610\ttotal: 1m 9s\tremaining: 3m 23s\n",
            "772:\tlearn: 2264597.3546006\ttotal: 1m 9s\tremaining: 3m 23s\n",
            "773:\tlearn: 2262509.7287928\ttotal: 1m 9s\tremaining: 3m 23s\n",
            "774:\tlearn: 2260381.3385937\ttotal: 1m 11s\tremaining: 3m 30s\n",
            "775:\tlearn: 2259592.4231195\ttotal: 1m 11s\tremaining: 3m 30s\n",
            "776:\tlearn: 2257464.1815110\ttotal: 1m 11s\tremaining: 3m 30s\n",
            "777:\tlearn: 2255535.0818291\ttotal: 1m 11s\tremaining: 3m 29s\n",
            "778:\tlearn: 2254697.1759814\ttotal: 1m 11s\tremaining: 3m 29s\n",
            "779:\tlearn: 2253241.7261953\ttotal: 1m 11s\tremaining: 3m 29s\n",
            "780:\tlearn: 2252424.8703381\ttotal: 1m 14s\tremaining: 3m 36s\n",
            "781:\tlearn: 2251085.2001481\ttotal: 1m 14s\tremaining: 3m 36s\n",
            "782:\tlearn: 2248948.1950505\ttotal: 1m 14s\tremaining: 3m 36s\n",
            "783:\tlearn: 2247463.5490457\ttotal: 1m 14s\tremaining: 3m 35s\n",
            "784:\tlearn: 2245799.9431824\ttotal: 1m 14s\tremaining: 3m 35s\n",
            "785:\tlearn: 2243312.9831308\ttotal: 1m 14s\tremaining: 3m 35s\n",
            "786:\tlearn: 2242143.7568127\ttotal: 1m 14s\tremaining: 3m 35s\n",
            "787:\tlearn: 2240584.9061167\ttotal: 1m 14s\tremaining: 3m 34s\n",
            "788:\tlearn: 2238286.8099336\ttotal: 1m 14s\tremaining: 3m 34s\n",
            "789:\tlearn: 2237279.3817248\ttotal: 1m 17s\tremaining: 3m 41s\n",
            "790:\tlearn: 2236658.7076150\ttotal: 1m 17s\tremaining: 3m 41s\n",
            "791:\tlearn: 2233085.6630625\ttotal: 1m 17s\tremaining: 3m 41s\n",
            "792:\tlearn: 2231994.8750191\ttotal: 1m 17s\tremaining: 3m 41s\n",
            "793:\tlearn: 2230371.3481674\ttotal: 1m 17s\tremaining: 3m 40s\n",
            "794:\tlearn: 2228381.8516132\ttotal: 1m 17s\tremaining: 3m 40s\n",
            "795:\tlearn: 2226880.5659993\ttotal: 1m 17s\tremaining: 3m 40s\n",
            "796:\tlearn: 2224455.8694172\ttotal: 1m 17s\tremaining: 3m 39s\n",
            "797:\tlearn: 2221134.2309499\ttotal: 1m 17s\tremaining: 3m 39s\n",
            "798:\tlearn: 2219745.3426101\ttotal: 1m 17s\tremaining: 3m 39s\n",
            "799:\tlearn: 2217075.5976816\ttotal: 1m 17s\tremaining: 3m 39s\n",
            "800:\tlearn: 2216286.1091784\ttotal: 1m 17s\tremaining: 3m 38s\n",
            "801:\tlearn: 2215337.9831445\ttotal: 1m 17s\tremaining: 3m 38s\n",
            "802:\tlearn: 2214242.7464193\ttotal: 1m 18s\tremaining: 3m 38s\n",
            "803:\tlearn: 2211731.8826940\ttotal: 1m 18s\tremaining: 3m 38s\n",
            "804:\tlearn: 2209310.1773906\ttotal: 1m 18s\tremaining: 3m 37s\n",
            "805:\tlearn: 2208355.2400181\ttotal: 1m 18s\tremaining: 3m 37s\n",
            "806:\tlearn: 2207280.1821063\ttotal: 1m 18s\tremaining: 3m 37s\n",
            "807:\tlearn: 2206435.7184110\ttotal: 1m 18s\tremaining: 3m 36s\n",
            "808:\tlearn: 2204474.3205549\ttotal: 1m 18s\tremaining: 3m 36s\n",
            "809:\tlearn: 2203947.7261800\ttotal: 1m 18s\tremaining: 3m 36s\n",
            "810:\tlearn: 2202281.5198807\ttotal: 1m 18s\tremaining: 3m 36s\n",
            "811:\tlearn: 2201715.1224452\ttotal: 1m 18s\tremaining: 3m 35s\n",
            "812:\tlearn: 2200500.2379936\ttotal: 1m 18s\tremaining: 3m 35s\n",
            "813:\tlearn: 2199601.8979347\ttotal: 1m 18s\tremaining: 3m 35s\n",
            "814:\tlearn: 2197351.8545639\ttotal: 1m 18s\tremaining: 3m 35s\n",
            "815:\tlearn: 2195650.4786358\ttotal: 1m 18s\tremaining: 3m 34s\n",
            "816:\tlearn: 2194095.7286181\ttotal: 1m 18s\tremaining: 3m 34s\n",
            "817:\tlearn: 2192580.5334184\ttotal: 1m 18s\tremaining: 3m 34s\n",
            "818:\tlearn: 2190217.5968006\ttotal: 1m 18s\tremaining: 3m 34s\n",
            "819:\tlearn: 2188657.9342513\ttotal: 1m 18s\tremaining: 3m 33s\n",
            "820:\tlearn: 2187481.3892081\ttotal: 1m 18s\tremaining: 3m 33s\n",
            "821:\tlearn: 2185722.7615704\ttotal: 1m 18s\tremaining: 3m 33s\n",
            "822:\tlearn: 2183501.3057295\ttotal: 1m 18s\tremaining: 3m 33s\n",
            "823:\tlearn: 2181894.7982473\ttotal: 1m 18s\tremaining: 3m 32s\n",
            "824:\tlearn: 2181383.5327508\ttotal: 1m 18s\tremaining: 3m 32s\n",
            "825:\tlearn: 2180704.0985857\ttotal: 1m 18s\tremaining: 3m 32s\n",
            "826:\tlearn: 2178355.0224906\ttotal: 1m 18s\tremaining: 3m 32s\n",
            "827:\tlearn: 2177230.8675240\ttotal: 1m 19s\tremaining: 3m 31s\n",
            "828:\tlearn: 2176308.1798563\ttotal: 1m 19s\tremaining: 3m 31s\n",
            "829:\tlearn: 2173833.4929674\ttotal: 1m 19s\tremaining: 3m 31s\n",
            "830:\tlearn: 2172347.0456897\ttotal: 1m 19s\tremaining: 3m 31s\n",
            "831:\tlearn: 2170927.2604485\ttotal: 1m 19s\tremaining: 3m 30s\n",
            "832:\tlearn: 2170070.6003185\ttotal: 1m 19s\tremaining: 3m 30s\n",
            "833:\tlearn: 2168343.1513679\ttotal: 1m 19s\tremaining: 3m 30s\n",
            "834:\tlearn: 2166126.4225073\ttotal: 1m 19s\tremaining: 3m 30s\n",
            "835:\tlearn: 2165136.2077249\ttotal: 1m 19s\tremaining: 3m 29s\n",
            "836:\tlearn: 2163895.4968386\ttotal: 1m 19s\tremaining: 3m 29s\n",
            "837:\tlearn: 2162273.6685676\ttotal: 1m 19s\tremaining: 3m 29s\n",
            "838:\tlearn: 2160616.4647777\ttotal: 1m 19s\tremaining: 3m 29s\n",
            "839:\tlearn: 2160149.3425388\ttotal: 1m 19s\tremaining: 3m 28s\n",
            "840:\tlearn: 2158609.8337221\ttotal: 1m 19s\tremaining: 3m 28s\n",
            "841:\tlearn: 2155115.1085753\ttotal: 1m 19s\tremaining: 3m 28s\n",
            "842:\tlearn: 2152867.9154199\ttotal: 1m 19s\tremaining: 3m 28s\n",
            "843:\tlearn: 2151886.1128102\ttotal: 1m 19s\tremaining: 3m 27s\n",
            "844:\tlearn: 2151255.7473237\ttotal: 1m 19s\tremaining: 3m 27s\n",
            "845:\tlearn: 2149755.3606241\ttotal: 1m 19s\tremaining: 3m 27s\n",
            "846:\tlearn: 2148670.4667811\ttotal: 1m 19s\tremaining: 3m 27s\n",
            "847:\tlearn: 2147115.3932691\ttotal: 1m 19s\tremaining: 3m 26s\n",
            "848:\tlearn: 2145706.2018151\ttotal: 1m 19s\tremaining: 3m 26s\n",
            "849:\tlearn: 2144825.7238463\ttotal: 1m 19s\tremaining: 3m 26s\n",
            "850:\tlearn: 2143909.6712558\ttotal: 1m 19s\tremaining: 3m 26s\n",
            "851:\tlearn: 2142874.8110856\ttotal: 1m 19s\tremaining: 3m 25s\n",
            "852:\tlearn: 2141537.9512370\ttotal: 1m 19s\tremaining: 3m 25s\n",
            "853:\tlearn: 2139681.1744610\ttotal: 1m 19s\tremaining: 3m 25s\n",
            "854:\tlearn: 2138046.3219826\ttotal: 1m 20s\tremaining: 3m 25s\n",
            "855:\tlearn: 2135923.1520434\ttotal: 1m 20s\tremaining: 3m 24s\n",
            "856:\tlearn: 2134320.1299662\ttotal: 1m 20s\tremaining: 3m 24s\n",
            "857:\tlearn: 2132376.3727153\ttotal: 1m 20s\tremaining: 3m 24s\n",
            "858:\tlearn: 2131448.4438287\ttotal: 1m 20s\tremaining: 3m 24s\n",
            "859:\tlearn: 2130579.0998279\ttotal: 1m 20s\tremaining: 3m 23s\n",
            "860:\tlearn: 2129615.6315985\ttotal: 1m 20s\tremaining: 3m 23s\n",
            "861:\tlearn: 2128824.6248595\ttotal: 1m 20s\tremaining: 3m 23s\n",
            "862:\tlearn: 2127641.0834157\ttotal: 1m 20s\tremaining: 3m 23s\n",
            "863:\tlearn: 2126059.1504584\ttotal: 1m 20s\tremaining: 3m 22s\n",
            "864:\tlearn: 2124599.7041149\ttotal: 1m 20s\tremaining: 3m 22s\n",
            "865:\tlearn: 2124001.4685559\ttotal: 1m 23s\tremaining: 3m 29s\n",
            "866:\tlearn: 2121405.6800142\ttotal: 1m 23s\tremaining: 3m 29s\n",
            "867:\tlearn: 2119389.3230180\ttotal: 1m 25s\tremaining: 3m 35s\n",
            "868:\tlearn: 2117921.3574526\ttotal: 1m 25s\tremaining: 3m 35s\n",
            "869:\tlearn: 2116922.3936765\ttotal: 1m 25s\tremaining: 3m 34s\n",
            "870:\tlearn: 2116310.8415729\ttotal: 1m 25s\tremaining: 3m 34s\n",
            "871:\tlearn: 2113236.0576535\ttotal: 1m 25s\tremaining: 3m 34s\n",
            "872:\tlearn: 2110933.6475433\ttotal: 1m 25s\tremaining: 3m 34s\n",
            "873:\tlearn: 2110335.6351399\ttotal: 1m 25s\tremaining: 3m 33s\n",
            "874:\tlearn: 2108714.8556440\ttotal: 1m 25s\tremaining: 3m 33s\n",
            "875:\tlearn: 2107887.0872809\ttotal: 1m 26s\tremaining: 3m 33s\n",
            "876:\tlearn: 2105684.2093672\ttotal: 1m 26s\tremaining: 3m 33s\n",
            "877:\tlearn: 2104101.3673919\ttotal: 1m 26s\tremaining: 3m 32s\n",
            "878:\tlearn: 2103231.4999271\ttotal: 1m 26s\tremaining: 3m 32s\n",
            "879:\tlearn: 2101871.0016620\ttotal: 1m 26s\tremaining: 3m 32s\n",
            "880:\tlearn: 2100845.6405082\ttotal: 1m 26s\tremaining: 3m 32s\n",
            "881:\tlearn: 2099560.3980812\ttotal: 1m 26s\tremaining: 3m 31s\n",
            "882:\tlearn: 2098999.7198225\ttotal: 1m 26s\tremaining: 3m 31s\n",
            "883:\tlearn: 2097295.1156896\ttotal: 1m 26s\tremaining: 3m 31s\n",
            "884:\tlearn: 2096527.7104461\ttotal: 1m 26s\tremaining: 3m 31s\n",
            "885:\tlearn: 2095619.9684839\ttotal: 1m 26s\tremaining: 3m 30s\n",
            "886:\tlearn: 2095191.2716394\ttotal: 1m 26s\tremaining: 3m 30s\n",
            "887:\tlearn: 2092506.5475373\ttotal: 1m 26s\tremaining: 3m 30s\n",
            "888:\tlearn: 2091334.5316871\ttotal: 1m 26s\tremaining: 3m 30s\n",
            "889:\tlearn: 2090576.3361877\ttotal: 1m 26s\tremaining: 3m 29s\n",
            "890:\tlearn: 2089842.7186581\ttotal: 1m 26s\tremaining: 3m 29s\n",
            "891:\tlearn: 2088154.1841227\ttotal: 1m 26s\tremaining: 3m 29s\n",
            "892:\tlearn: 2086973.2125832\ttotal: 1m 26s\tremaining: 3m 29s\n",
            "893:\tlearn: 2086288.3638275\ttotal: 1m 26s\tremaining: 3m 28s\n",
            "894:\tlearn: 2085278.9507817\ttotal: 1m 26s\tremaining: 3m 28s\n",
            "895:\tlearn: 2083190.5011368\ttotal: 1m 26s\tremaining: 3m 28s\n",
            "896:\tlearn: 2081373.0563246\ttotal: 1m 26s\tremaining: 3m 28s\n",
            "897:\tlearn: 2079477.0494661\ttotal: 1m 26s\tremaining: 3m 28s\n",
            "898:\tlearn: 2078117.0346515\ttotal: 1m 26s\tremaining: 3m 27s\n",
            "899:\tlearn: 2076686.3098267\ttotal: 1m 26s\tremaining: 3m 27s\n",
            "900:\tlearn: 2074790.2006677\ttotal: 1m 27s\tremaining: 3m 27s\n",
            "901:\tlearn: 2073662.7474207\ttotal: 1m 27s\tremaining: 3m 27s\n",
            "902:\tlearn: 2072376.2662311\ttotal: 1m 27s\tremaining: 3m 26s\n",
            "903:\tlearn: 2071949.6748038\ttotal: 1m 27s\tremaining: 3m 26s\n",
            "904:\tlearn: 2069869.7098246\ttotal: 1m 27s\tremaining: 3m 26s\n",
            "905:\tlearn: 2068857.8891354\ttotal: 1m 27s\tremaining: 3m 26s\n",
            "906:\tlearn: 2067122.0489766\ttotal: 1m 27s\tremaining: 3m 25s\n",
            "907:\tlearn: 2066328.6539184\ttotal: 1m 27s\tremaining: 3m 25s\n",
            "908:\tlearn: 2065614.7540118\ttotal: 1m 27s\tremaining: 3m 25s\n",
            "909:\tlearn: 2063849.1030296\ttotal: 1m 27s\tremaining: 3m 25s\n",
            "910:\tlearn: 2063060.4755452\ttotal: 1m 27s\tremaining: 3m 25s\n",
            "911:\tlearn: 2061694.3831025\ttotal: 1m 27s\tremaining: 3m 24s\n",
            "912:\tlearn: 2060537.8586695\ttotal: 1m 27s\tremaining: 3m 24s\n",
            "913:\tlearn: 2059346.9466920\ttotal: 1m 27s\tremaining: 3m 24s\n",
            "914:\tlearn: 2058157.0711241\ttotal: 1m 27s\tremaining: 3m 24s\n",
            "915:\tlearn: 2056791.6761289\ttotal: 1m 27s\tremaining: 3m 23s\n",
            "916:\tlearn: 2055574.0842280\ttotal: 1m 27s\tremaining: 3m 23s\n",
            "917:\tlearn: 2054621.2019024\ttotal: 1m 27s\tremaining: 3m 23s\n",
            "918:\tlearn: 2053269.7043419\ttotal: 1m 27s\tremaining: 3m 23s\n",
            "919:\tlearn: 2052533.8055197\ttotal: 1m 27s\tremaining: 3m 22s\n",
            "920:\tlearn: 2051388.0668915\ttotal: 1m 27s\tremaining: 3m 22s\n",
            "921:\tlearn: 2050506.3651129\ttotal: 1m 27s\tremaining: 3m 22s\n",
            "922:\tlearn: 2048604.0267672\ttotal: 1m 27s\tremaining: 3m 22s\n",
            "923:\tlearn: 2047679.8364259\ttotal: 1m 27s\tremaining: 3m 22s\n",
            "924:\tlearn: 2046507.5387894\ttotal: 1m 28s\tremaining: 3m 21s\n",
            "925:\tlearn: 2045106.6258595\ttotal: 1m 28s\tremaining: 3m 21s\n",
            "926:\tlearn: 2044369.5733473\ttotal: 1m 28s\tremaining: 3m 21s\n",
            "927:\tlearn: 2043743.2047675\ttotal: 1m 28s\tremaining: 3m 21s\n",
            "928:\tlearn: 2042032.4241571\ttotal: 1m 28s\tremaining: 3m 20s\n",
            "929:\tlearn: 2040305.4493849\ttotal: 1m 28s\tremaining: 3m 20s\n",
            "930:\tlearn: 2038991.4787885\ttotal: 1m 28s\tremaining: 3m 20s\n",
            "931:\tlearn: 2037664.4936569\ttotal: 1m 28s\tremaining: 3m 20s\n",
            "932:\tlearn: 2035905.7110027\ttotal: 1m 28s\tremaining: 3m 19s\n",
            "933:\tlearn: 2034597.6334456\ttotal: 1m 28s\tremaining: 3m 19s\n",
            "934:\tlearn: 2032894.5787359\ttotal: 1m 28s\tremaining: 3m 19s\n",
            "935:\tlearn: 2031299.4782633\ttotal: 1m 28s\tremaining: 3m 19s\n",
            "936:\tlearn: 2030100.8894550\ttotal: 1m 28s\tremaining: 3m 19s\n",
            "937:\tlearn: 2029211.2741765\ttotal: 1m 28s\tremaining: 3m 18s\n",
            "938:\tlearn: 2025998.7425446\ttotal: 1m 28s\tremaining: 3m 18s\n",
            "939:\tlearn: 2024943.2845448\ttotal: 1m 28s\tremaining: 3m 18s\n",
            "940:\tlearn: 2023337.1203105\ttotal: 1m 28s\tremaining: 3m 18s\n",
            "941:\tlearn: 2022191.0761274\ttotal: 1m 28s\tremaining: 3m 17s\n",
            "942:\tlearn: 2020864.6230120\ttotal: 1m 28s\tremaining: 3m 17s\n",
            "943:\tlearn: 2019581.9177953\ttotal: 1m 28s\tremaining: 3m 17s\n",
            "944:\tlearn: 2018308.4687868\ttotal: 1m 28s\tremaining: 3m 17s\n",
            "945:\tlearn: 2017354.8005894\ttotal: 1m 28s\tremaining: 3m 17s\n",
            "946:\tlearn: 2016444.7837004\ttotal: 1m 28s\tremaining: 3m 16s\n",
            "947:\tlearn: 2014407.4521568\ttotal: 1m 28s\tremaining: 3m 16s\n",
            "948:\tlearn: 2012368.5904404\ttotal: 1m 28s\tremaining: 3m 16s\n",
            "949:\tlearn: 2011414.0009302\ttotal: 1m 28s\tremaining: 3m 16s\n",
            "950:\tlearn: 2009674.2236708\ttotal: 1m 29s\tremaining: 3m 15s\n",
            "951:\tlearn: 2008111.7570732\ttotal: 1m 29s\tremaining: 3m 15s\n",
            "952:\tlearn: 2007063.8848600\ttotal: 1m 29s\tremaining: 3m 15s\n",
            "953:\tlearn: 2005939.7084178\ttotal: 1m 29s\tremaining: 3m 15s\n",
            "954:\tlearn: 2005116.3296136\ttotal: 1m 29s\tremaining: 3m 15s\n",
            "955:\tlearn: 2004459.8979823\ttotal: 1m 29s\tremaining: 3m 14s\n",
            "956:\tlearn: 2002691.0643632\ttotal: 1m 29s\tremaining: 3m 14s\n",
            "957:\tlearn: 2000774.2687506\ttotal: 1m 29s\tremaining: 3m 14s\n",
            "958:\tlearn: 1999984.0583957\ttotal: 1m 29s\tremaining: 3m 14s\n",
            "959:\tlearn: 1998706.0603399\ttotal: 1m 29s\tremaining: 3m 13s\n",
            "960:\tlearn: 1997314.5549701\ttotal: 1m 29s\tremaining: 3m 13s\n",
            "961:\tlearn: 1996758.7668329\ttotal: 1m 29s\tremaining: 3m 13s\n",
            "962:\tlearn: 1994561.5073993\ttotal: 1m 29s\tremaining: 3m 13s\n",
            "963:\tlearn: 1993542.1448858\ttotal: 1m 29s\tremaining: 3m 13s\n",
            "964:\tlearn: 1992620.1782114\ttotal: 1m 29s\tremaining: 3m 12s\n",
            "965:\tlearn: 1992025.5787757\ttotal: 1m 29s\tremaining: 3m 12s\n",
            "966:\tlearn: 1989842.2843820\ttotal: 1m 29s\tremaining: 3m 12s\n",
            "967:\tlearn: 1988557.0437878\ttotal: 1m 29s\tremaining: 3m 12s\n",
            "968:\tlearn: 1987312.0555386\ttotal: 1m 29s\tremaining: 3m 12s\n",
            "969:\tlearn: 1985222.1902241\ttotal: 1m 29s\tremaining: 3m 11s\n",
            "970:\tlearn: 1983906.2595071\ttotal: 1m 29s\tremaining: 3m 11s\n",
            "971:\tlearn: 1981657.3409543\ttotal: 1m 29s\tremaining: 3m 11s\n",
            "972:\tlearn: 1980347.7664019\ttotal: 1m 29s\tremaining: 3m 11s\n",
            "973:\tlearn: 1978755.8066400\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "974:\tlearn: 1977725.8861219\ttotal: 1m 29s\tremaining: 3m 10s\n",
            "975:\tlearn: 1976198.3494489\ttotal: 1m 30s\tremaining: 3m 10s\n",
            "976:\tlearn: 1974996.9173907\ttotal: 1m 30s\tremaining: 3m 10s\n",
            "977:\tlearn: 1974425.3566798\ttotal: 1m 30s\tremaining: 3m 10s\n",
            "978:\tlearn: 1972821.3599654\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "979:\tlearn: 1971431.9737137\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "980:\tlearn: 1969983.9087731\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "981:\tlearn: 1969004.7173177\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "982:\tlearn: 1968343.2763444\ttotal: 1m 30s\tremaining: 3m 9s\n",
            "983:\tlearn: 1966993.0050607\ttotal: 1m 30s\tremaining: 3m 8s\n",
            "984:\tlearn: 1965551.4792625\ttotal: 1m 30s\tremaining: 3m 8s\n",
            "985:\tlearn: 1964426.8550618\ttotal: 1m 30s\tremaining: 3m 8s\n",
            "986:\tlearn: 1963625.7596492\ttotal: 1m 30s\tremaining: 3m 8s\n",
            "987:\tlearn: 1962685.4557250\ttotal: 1m 30s\tremaining: 3m 8s\n",
            "988:\tlearn: 1961754.3366928\ttotal: 1m 30s\tremaining: 3m 7s\n",
            "989:\tlearn: 1961117.4016751\ttotal: 1m 30s\tremaining: 3m 7s\n",
            "990:\tlearn: 1959637.1873714\ttotal: 1m 30s\tremaining: 3m 7s\n",
            "991:\tlearn: 1959041.4238988\ttotal: 1m 30s\tremaining: 3m 7s\n",
            "992:\tlearn: 1956759.2654451\ttotal: 1m 30s\tremaining: 3m 6s\n",
            "993:\tlearn: 1955449.9303529\ttotal: 1m 30s\tremaining: 3m 6s\n",
            "994:\tlearn: 1954118.9194988\ttotal: 1m 30s\tremaining: 3m 6s\n",
            "995:\tlearn: 1952232.0822857\ttotal: 1m 30s\tremaining: 3m 6s\n",
            "996:\tlearn: 1951649.6484938\ttotal: 1m 30s\tremaining: 3m 6s\n",
            "997:\tlearn: 1950846.1658171\ttotal: 1m 30s\tremaining: 3m 5s\n",
            "998:\tlearn: 1948702.1127476\ttotal: 1m 30s\tremaining: 3m 5s\n",
            "999:\tlearn: 1947992.6808828\ttotal: 1m 30s\tremaining: 3m 5s\n",
            "1000:\tlearn: 1947378.4036675\ttotal: 1m 30s\tremaining: 3m 5s\n",
            "1001:\tlearn: 1947003.2755722\ttotal: 1m 30s\tremaining: 3m 5s\n",
            "1002:\tlearn: 1946029.3253429\ttotal: 1m 31s\tremaining: 3m 4s\n",
            "1003:\tlearn: 1944931.8752552\ttotal: 1m 31s\tremaining: 3m 4s\n",
            "1004:\tlearn: 1943737.7899286\ttotal: 1m 31s\tremaining: 3m 4s\n",
            "1005:\tlearn: 1942502.8411306\ttotal: 1m 31s\tremaining: 3m 4s\n",
            "1006:\tlearn: 1940581.3725507\ttotal: 1m 31s\tremaining: 3m 4s\n",
            "1007:\tlearn: 1938800.1884916\ttotal: 1m 31s\tremaining: 3m 3s\n",
            "1008:\tlearn: 1937408.3812365\ttotal: 1m 31s\tremaining: 3m 3s\n",
            "1009:\tlearn: 1936902.3542394\ttotal: 1m 31s\tremaining: 3m 3s\n",
            "1010:\tlearn: 1936113.9731840\ttotal: 1m 31s\tremaining: 3m 3s\n",
            "1011:\tlearn: 1934770.6930564\ttotal: 1m 31s\tremaining: 3m 3s\n",
            "1012:\tlearn: 1934047.3451867\ttotal: 1m 31s\tremaining: 3m 2s\n",
            "1013:\tlearn: 1933458.7156611\ttotal: 1m 31s\tremaining: 3m 2s\n",
            "1014:\tlearn: 1932655.9959836\ttotal: 1m 31s\tremaining: 3m 2s\n",
            "1015:\tlearn: 1930572.0764309\ttotal: 1m 31s\tremaining: 3m 2s\n",
            "1016:\tlearn: 1928998.7989080\ttotal: 1m 31s\tremaining: 3m 2s\n",
            "1017:\tlearn: 1927770.1110000\ttotal: 1m 31s\tremaining: 3m 1s\n",
            "1018:\tlearn: 1926626.2598812\ttotal: 1m 31s\tremaining: 3m 1s\n",
            "1019:\tlearn: 1925447.8041392\ttotal: 1m 31s\tremaining: 3m 1s\n",
            "1020:\tlearn: 1923085.9560105\ttotal: 1m 31s\tremaining: 3m 1s\n",
            "1021:\tlearn: 1921705.6889976\ttotal: 1m 31s\tremaining: 3m 1s\n",
            "1022:\tlearn: 1919624.7226104\ttotal: 1m 31s\tremaining: 3m\n",
            "1023:\tlearn: 1918829.7040818\ttotal: 1m 31s\tremaining: 3m\n",
            "1024:\tlearn: 1917935.5130688\ttotal: 1m 31s\tremaining: 3m\n",
            "1025:\tlearn: 1917134.6711385\ttotal: 1m 31s\tremaining: 3m\n",
            "1026:\tlearn: 1916090.7892676\ttotal: 1m 31s\tremaining: 3m\n",
            "1027:\tlearn: 1915537.8117580\ttotal: 1m 32s\tremaining: 3m\n",
            "1028:\tlearn: 1914895.0311221\ttotal: 1m 32s\tremaining: 2m 59s\n",
            "1029:\tlearn: 1913525.1585274\ttotal: 1m 32s\tremaining: 2m 59s\n",
            "1030:\tlearn: 1912409.9781040\ttotal: 1m 32s\tremaining: 2m 59s\n",
            "1031:\tlearn: 1911449.0392493\ttotal: 1m 32s\tremaining: 2m 59s\n",
            "1032:\tlearn: 1909504.9299097\ttotal: 1m 32s\tremaining: 2m 59s\n",
            "1033:\tlearn: 1909074.6095249\ttotal: 1m 32s\tremaining: 2m 58s\n",
            "1034:\tlearn: 1907076.9480415\ttotal: 1m 32s\tremaining: 2m 58s\n",
            "1035:\tlearn: 1905281.4419207\ttotal: 1m 32s\tremaining: 2m 58s\n",
            "1036:\tlearn: 1904020.4410726\ttotal: 1m 32s\tremaining: 2m 58s\n",
            "1037:\tlearn: 1903229.0694865\ttotal: 1m 32s\tremaining: 2m 58s\n",
            "1038:\tlearn: 1902103.0315911\ttotal: 1m 32s\tremaining: 2m 57s\n",
            "1039:\tlearn: 1900725.1606319\ttotal: 1m 32s\tremaining: 2m 57s\n",
            "1040:\tlearn: 1899666.4503949\ttotal: 1m 32s\tremaining: 2m 57s\n",
            "1041:\tlearn: 1898532.6033773\ttotal: 1m 32s\tremaining: 2m 57s\n",
            "1042:\tlearn: 1897795.3601022\ttotal: 1m 32s\tremaining: 2m 57s\n",
            "1043:\tlearn: 1897010.3422675\ttotal: 1m 32s\tremaining: 2m 56s\n",
            "1044:\tlearn: 1896010.5148647\ttotal: 1m 32s\tremaining: 2m 56s\n",
            "1045:\tlearn: 1894163.6065034\ttotal: 1m 32s\tremaining: 2m 56s\n",
            "1046:\tlearn: 1892376.5893855\ttotal: 1m 32s\tremaining: 2m 56s\n",
            "1047:\tlearn: 1890906.6197778\ttotal: 1m 32s\tremaining: 2m 56s\n",
            "1048:\tlearn: 1890022.8206711\ttotal: 1m 32s\tremaining: 2m 55s\n",
            "1049:\tlearn: 1888472.7874648\ttotal: 1m 32s\tremaining: 2m 55s\n",
            "1050:\tlearn: 1887426.0771746\ttotal: 1m 32s\tremaining: 2m 55s\n",
            "1051:\tlearn: 1886500.8741176\ttotal: 1m 32s\tremaining: 2m 55s\n",
            "1052:\tlearn: 1885745.0917517\ttotal: 1m 32s\tremaining: 2m 55s\n",
            "1053:\tlearn: 1885122.7928481\ttotal: 1m 33s\tremaining: 2m 55s\n",
            "1054:\tlearn: 1884529.6220197\ttotal: 1m 33s\tremaining: 2m 54s\n",
            "1055:\tlearn: 1883911.0523993\ttotal: 1m 33s\tremaining: 2m 54s\n",
            "1056:\tlearn: 1882596.8681237\ttotal: 1m 33s\tremaining: 2m 54s\n",
            "1057:\tlearn: 1880502.2420973\ttotal: 1m 33s\tremaining: 2m 54s\n",
            "1058:\tlearn: 1879273.4259465\ttotal: 1m 33s\tremaining: 2m 54s\n",
            "1059:\tlearn: 1878289.8120753\ttotal: 1m 33s\tremaining: 2m 53s\n",
            "1060:\tlearn: 1876817.9946125\ttotal: 1m 33s\tremaining: 2m 53s\n",
            "1061:\tlearn: 1875461.3021659\ttotal: 1m 33s\tremaining: 2m 53s\n",
            "1062:\tlearn: 1874198.6059260\ttotal: 1m 33s\tremaining: 2m 53s\n",
            "1063:\tlearn: 1872447.8971077\ttotal: 1m 33s\tremaining: 2m 53s\n",
            "1064:\tlearn: 1871282.1428717\ttotal: 1m 33s\tremaining: 2m 53s\n",
            "1065:\tlearn: 1869915.7475425\ttotal: 1m 33s\tremaining: 2m 52s\n",
            "1066:\tlearn: 1868765.9546494\ttotal: 1m 33s\tremaining: 2m 52s\n",
            "1067:\tlearn: 1867018.1039415\ttotal: 1m 33s\tremaining: 2m 52s\n",
            "1068:\tlearn: 1864902.2151670\ttotal: 1m 33s\tremaining: 2m 52s\n",
            "1069:\tlearn: 1863195.0797225\ttotal: 1m 33s\tremaining: 2m 52s\n",
            "1070:\tlearn: 1862414.1155477\ttotal: 1m 33s\tremaining: 2m 51s\n",
            "1071:\tlearn: 1861315.2221983\ttotal: 1m 33s\tremaining: 2m 51s\n",
            "1072:\tlearn: 1860457.7455662\ttotal: 1m 33s\tremaining: 2m 51s\n",
            "1073:\tlearn: 1859507.8699913\ttotal: 1m 33s\tremaining: 2m 51s\n",
            "1074:\tlearn: 1858169.1817645\ttotal: 1m 33s\tremaining: 2m 51s\n",
            "1075:\tlearn: 1856109.1034596\ttotal: 1m 33s\tremaining: 2m 51s\n",
            "1076:\tlearn: 1854776.5500356\ttotal: 1m 33s\tremaining: 2m 50s\n",
            "1077:\tlearn: 1853520.6710017\ttotal: 1m 33s\tremaining: 2m 50s\n",
            "1078:\tlearn: 1852186.4407136\ttotal: 1m 33s\tremaining: 2m 50s\n",
            "1079:\tlearn: 1851460.6586348\ttotal: 1m 34s\tremaining: 2m 50s\n",
            "1080:\tlearn: 1850835.9711818\ttotal: 1m 34s\tremaining: 2m 50s\n",
            "1081:\tlearn: 1849546.6656439\ttotal: 1m 34s\tremaining: 2m 49s\n",
            "1082:\tlearn: 1848476.9427547\ttotal: 1m 34s\tremaining: 2m 49s\n",
            "1083:\tlearn: 1847107.9290653\ttotal: 1m 34s\tremaining: 2m 49s\n",
            "1084:\tlearn: 1846493.3714315\ttotal: 1m 34s\tremaining: 2m 49s\n",
            "1085:\tlearn: 1845357.9900872\ttotal: 1m 34s\tremaining: 2m 49s\n",
            "1086:\tlearn: 1844672.0288730\ttotal: 1m 34s\tremaining: 2m 49s\n",
            "1087:\tlearn: 1843126.3943526\ttotal: 1m 34s\tremaining: 2m 48s\n",
            "1088:\tlearn: 1841969.2196654\ttotal: 1m 34s\tremaining: 2m 48s\n",
            "1089:\tlearn: 1839208.0858782\ttotal: 1m 34s\tremaining: 2m 48s\n",
            "1090:\tlearn: 1837490.4844421\ttotal: 1m 37s\tremaining: 2m 52s\n",
            "1091:\tlearn: 1837004.8983235\ttotal: 1m 37s\tremaining: 2m 52s\n",
            "1092:\tlearn: 1835853.6270478\ttotal: 1m 37s\tremaining: 2m 52s\n",
            "1093:\tlearn: 1834119.6294557\ttotal: 1m 39s\tremaining: 2m 57s\n",
            "1094:\tlearn: 1833018.4082653\ttotal: 1m 39s\tremaining: 2m 56s\n",
            "1095:\tlearn: 1832422.4853049\ttotal: 1m 39s\tremaining: 2m 56s\n",
            "1096:\tlearn: 1831163.8126660\ttotal: 1m 39s\tremaining: 2m 56s\n",
            "1097:\tlearn: 1830605.8736649\ttotal: 1m 39s\tremaining: 2m 56s\n",
            "1098:\tlearn: 1829936.4971224\ttotal: 1m 39s\tremaining: 2m 56s\n",
            "1099:\tlearn: 1829395.7292211\ttotal: 1m 39s\tremaining: 2m 56s\n",
            "1100:\tlearn: 1827686.3347596\ttotal: 1m 40s\tremaining: 2m 55s\n",
            "1101:\tlearn: 1826946.6566855\ttotal: 1m 40s\tremaining: 2m 55s\n",
            "1102:\tlearn: 1826225.9830240\ttotal: 1m 40s\tremaining: 2m 55s\n",
            "1103:\tlearn: 1825078.4527783\ttotal: 1m 40s\tremaining: 2m 55s\n",
            "1104:\tlearn: 1824222.6848002\ttotal: 1m 40s\tremaining: 2m 55s\n",
            "1105:\tlearn: 1822289.7261610\ttotal: 1m 40s\tremaining: 2m 54s\n",
            "1106:\tlearn: 1820244.2502817\ttotal: 1m 40s\tremaining: 2m 54s\n",
            "1107:\tlearn: 1819096.4456659\ttotal: 1m 40s\tremaining: 2m 54s\n",
            "1108:\tlearn: 1817732.2466790\ttotal: 1m 40s\tremaining: 2m 54s\n",
            "1109:\tlearn: 1817310.9637652\ttotal: 1m 40s\tremaining: 2m 54s\n",
            "1110:\tlearn: 1815883.0404649\ttotal: 1m 40s\tremaining: 2m 54s\n",
            "1111:\tlearn: 1814629.7075218\ttotal: 1m 40s\tremaining: 2m 53s\n",
            "1112:\tlearn: 1814100.1612095\ttotal: 1m 40s\tremaining: 2m 53s\n",
            "1113:\tlearn: 1813235.0350520\ttotal: 1m 40s\tremaining: 2m 53s\n",
            "1114:\tlearn: 1812159.3799405\ttotal: 1m 40s\tremaining: 2m 53s\n",
            "1115:\tlearn: 1811189.7360353\ttotal: 1m 40s\tremaining: 2m 53s\n",
            "1116:\tlearn: 1810609.4219053\ttotal: 1m 40s\tremaining: 2m 52s\n",
            "1117:\tlearn: 1809960.5908341\ttotal: 1m 40s\tremaining: 2m 52s\n",
            "1118:\tlearn: 1809370.8000672\ttotal: 1m 40s\tremaining: 2m 52s\n",
            "1119:\tlearn: 1808068.7616118\ttotal: 1m 40s\tremaining: 2m 52s\n",
            "1120:\tlearn: 1807538.9400761\ttotal: 1m 40s\tremaining: 2m 52s\n",
            "1121:\tlearn: 1806644.5486150\ttotal: 1m 40s\tremaining: 2m 52s\n",
            "1122:\tlearn: 1805638.3359505\ttotal: 1m 40s\tremaining: 2m 51s\n",
            "1123:\tlearn: 1804870.6856706\ttotal: 1m 40s\tremaining: 2m 51s\n",
            "1124:\tlearn: 1804097.7753619\ttotal: 1m 40s\tremaining: 2m 51s\n",
            "1125:\tlearn: 1802641.4231329\ttotal: 1m 41s\tremaining: 2m 51s\n",
            "1126:\tlearn: 1801661.8197138\ttotal: 1m 41s\tremaining: 2m 51s\n",
            "1127:\tlearn: 1800959.6079016\ttotal: 1m 41s\tremaining: 2m 50s\n",
            "1128:\tlearn: 1800035.0888434\ttotal: 1m 41s\tremaining: 2m 50s\n",
            "1129:\tlearn: 1799276.1412200\ttotal: 1m 41s\tremaining: 2m 50s\n",
            "1130:\tlearn: 1797724.3544613\ttotal: 1m 41s\tremaining: 2m 50s\n",
            "1131:\tlearn: 1797071.5953057\ttotal: 1m 41s\tremaining: 2m 50s\n",
            "1132:\tlearn: 1796371.1556004\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1133:\tlearn: 1795308.2653904\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1134:\tlearn: 1793401.7481869\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1135:\tlearn: 1792374.4783084\ttotal: 1m 43s\tremaining: 2m 54s\n",
            "1136:\tlearn: 1790464.4303178\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1137:\tlearn: 1788811.3981319\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1138:\tlearn: 1786763.2186983\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1139:\tlearn: 1785522.9088677\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1140:\tlearn: 1784008.4526627\ttotal: 1m 44s\tremaining: 2m 53s\n",
            "1141:\tlearn: 1783257.6533769\ttotal: 1m 44s\tremaining: 2m 52s\n",
            "1142:\tlearn: 1781939.9148492\ttotal: 1m 44s\tremaining: 2m 52s\n",
            "1143:\tlearn: 1779997.9867309\ttotal: 1m 44s\tremaining: 2m 52s\n",
            "1144:\tlearn: 1777490.7601950\ttotal: 1m 44s\tremaining: 2m 52s\n",
            "1145:\tlearn: 1776636.7522375\ttotal: 1m 44s\tremaining: 2m 52s\n",
            "1146:\tlearn: 1775602.2754450\ttotal: 1m 44s\tremaining: 2m 52s\n",
            "1147:\tlearn: 1774485.9543656\ttotal: 1m 44s\tremaining: 2m 51s\n",
            "1148:\tlearn: 1774033.2251627\ttotal: 1m 44s\tremaining: 2m 51s\n",
            "1149:\tlearn: 1772619.0444871\ttotal: 1m 44s\tremaining: 2m 51s\n",
            "1150:\tlearn: 1771652.1575072\ttotal: 1m 44s\tremaining: 2m 51s\n",
            "1151:\tlearn: 1771015.2868235\ttotal: 1m 44s\tremaining: 2m 51s\n",
            "1152:\tlearn: 1769116.6560432\ttotal: 1m 44s\tremaining: 2m 50s\n",
            "1153:\tlearn: 1767039.4924277\ttotal: 1m 44s\tremaining: 2m 50s\n",
            "1154:\tlearn: 1765582.0305651\ttotal: 1m 44s\tremaining: 2m 50s\n",
            "1155:\tlearn: 1764697.9398984\ttotal: 1m 44s\tremaining: 2m 50s\n",
            "1156:\tlearn: 1763465.5561899\ttotal: 1m 44s\tremaining: 2m 50s\n",
            "1157:\tlearn: 1761831.0260084\ttotal: 1m 44s\tremaining: 2m 50s\n",
            "1158:\tlearn: 1760473.3484041\ttotal: 1m 44s\tremaining: 2m 49s\n",
            "1159:\tlearn: 1759703.9614966\ttotal: 1m 44s\tremaining: 2m 49s\n",
            "1160:\tlearn: 1758431.2928098\ttotal: 1m 44s\tremaining: 2m 49s\n",
            "1161:\tlearn: 1757555.5263669\ttotal: 1m 45s\tremaining: 2m 49s\n",
            "1162:\tlearn: 1756688.7684569\ttotal: 1m 45s\tremaining: 2m 49s\n",
            "1163:\tlearn: 1756133.0055909\ttotal: 1m 45s\tremaining: 2m 48s\n",
            "1164:\tlearn: 1754019.1335988\ttotal: 1m 45s\tremaining: 2m 48s\n",
            "1165:\tlearn: 1752749.3936759\ttotal: 1m 45s\tremaining: 2m 48s\n",
            "1166:\tlearn: 1751238.9107012\ttotal: 1m 45s\tremaining: 2m 48s\n",
            "1167:\tlearn: 1749923.9961709\ttotal: 1m 45s\tremaining: 2m 48s\n",
            "1168:\tlearn: 1748555.7897725\ttotal: 1m 45s\tremaining: 2m 48s\n",
            "1169:\tlearn: 1747582.9169763\ttotal: 1m 45s\tremaining: 2m 47s\n",
            "1170:\tlearn: 1746996.1663859\ttotal: 1m 45s\tremaining: 2m 47s\n",
            "1171:\tlearn: 1745589.1539799\ttotal: 1m 45s\tremaining: 2m 47s\n",
            "1172:\tlearn: 1744800.5149460\ttotal: 1m 45s\tremaining: 2m 47s\n",
            "1173:\tlearn: 1744095.5713918\ttotal: 1m 45s\tremaining: 2m 47s\n",
            "1174:\tlearn: 1743145.7714011\ttotal: 1m 45s\tremaining: 2m 47s\n",
            "1175:\tlearn: 1742654.2234624\ttotal: 1m 45s\tremaining: 2m 46s\n",
            "1176:\tlearn: 1742171.9288712\ttotal: 1m 45s\tremaining: 2m 46s\n",
            "1177:\tlearn: 1741150.7331850\ttotal: 1m 45s\tremaining: 2m 46s\n",
            "1178:\tlearn: 1740436.1503543\ttotal: 1m 45s\tremaining: 2m 46s\n",
            "1179:\tlearn: 1739319.6636242\ttotal: 1m 45s\tremaining: 2m 46s\n",
            "1180:\tlearn: 1738121.9037241\ttotal: 1m 45s\tremaining: 2m 45s\n",
            "1181:\tlearn: 1737360.8777548\ttotal: 1m 45s\tremaining: 2m 45s\n",
            "1182:\tlearn: 1735992.0888258\ttotal: 1m 45s\tremaining: 2m 45s\n",
            "1183:\tlearn: 1735444.5999350\ttotal: 1m 45s\tremaining: 2m 45s\n",
            "1184:\tlearn: 1734214.1084577\ttotal: 1m 45s\tremaining: 2m 45s\n",
            "1185:\tlearn: 1733306.9553625\ttotal: 1m 45s\tremaining: 2m 45s\n",
            "1186:\tlearn: 1732526.7421050\ttotal: 1m 45s\tremaining: 2m 44s\n",
            "1187:\tlearn: 1731614.8194181\ttotal: 1m 46s\tremaining: 2m 44s\n",
            "1188:\tlearn: 1730888.0137331\ttotal: 1m 46s\tremaining: 2m 44s\n",
            "1189:\tlearn: 1730151.2091915\ttotal: 1m 46s\tremaining: 2m 44s\n",
            "1190:\tlearn: 1729528.4262679\ttotal: 1m 46s\tremaining: 2m 44s\n",
            "1191:\tlearn: 1728675.1260811\ttotal: 1m 46s\tremaining: 2m 44s\n",
            "1192:\tlearn: 1727736.6038280\ttotal: 1m 46s\tremaining: 2m 43s\n",
            "1193:\tlearn: 1726965.4115642\ttotal: 1m 46s\tremaining: 2m 43s\n",
            "1194:\tlearn: 1725235.6098969\ttotal: 1m 46s\tremaining: 2m 43s\n",
            "1195:\tlearn: 1723928.9693951\ttotal: 1m 46s\tremaining: 2m 43s\n",
            "1196:\tlearn: 1723489.3642400\ttotal: 1m 46s\tremaining: 2m 43s\n",
            "1197:\tlearn: 1722742.3435075\ttotal: 1m 46s\tremaining: 2m 43s\n",
            "1198:\tlearn: 1721764.8943711\ttotal: 1m 46s\tremaining: 2m 42s\n",
            "1199:\tlearn: 1720188.3812715\ttotal: 1m 46s\tremaining: 2m 42s\n",
            "1200:\tlearn: 1718093.1064494\ttotal: 1m 46s\tremaining: 2m 42s\n",
            "1201:\tlearn: 1716663.2064302\ttotal: 1m 46s\tremaining: 2m 42s\n",
            "1202:\tlearn: 1716110.8005069\ttotal: 1m 46s\tremaining: 2m 42s\n",
            "1203:\tlearn: 1715370.9767250\ttotal: 1m 46s\tremaining: 2m 42s\n",
            "1204:\tlearn: 1714748.2960516\ttotal: 1m 46s\tremaining: 2m 41s\n",
            "1205:\tlearn: 1713992.1956626\ttotal: 1m 46s\tremaining: 2m 41s\n",
            "1206:\tlearn: 1713375.2402844\ttotal: 1m 46s\tremaining: 2m 41s\n",
            "1207:\tlearn: 1712164.7418146\ttotal: 1m 46s\tremaining: 2m 41s\n",
            "1208:\tlearn: 1709898.5284284\ttotal: 1m 46s\tremaining: 2m 41s\n",
            "1209:\tlearn: 1709269.4074714\ttotal: 1m 46s\tremaining: 2m 41s\n",
            "1210:\tlearn: 1708397.4104615\ttotal: 1m 46s\tremaining: 2m 40s\n",
            "1211:\tlearn: 1707385.0184418\ttotal: 1m 46s\tremaining: 2m 40s\n",
            "1212:\tlearn: 1705599.9385923\ttotal: 1m 47s\tremaining: 2m 40s\n",
            "1213:\tlearn: 1705057.4227667\ttotal: 1m 47s\tremaining: 2m 40s\n",
            "1214:\tlearn: 1703870.8761228\ttotal: 1m 47s\tremaining: 2m 40s\n",
            "1215:\tlearn: 1703319.0727646\ttotal: 1m 47s\tremaining: 2m 40s\n",
            "1216:\tlearn: 1702325.6605754\ttotal: 1m 47s\tremaining: 2m 39s\n",
            "1217:\tlearn: 1701428.1514268\ttotal: 1m 47s\tremaining: 2m 39s\n",
            "1218:\tlearn: 1700887.7895475\ttotal: 1m 47s\tremaining: 2m 39s\n",
            "1219:\tlearn: 1700327.0014069\ttotal: 1m 47s\tremaining: 2m 39s\n",
            "1220:\tlearn: 1699595.8521097\ttotal: 1m 47s\tremaining: 2m 39s\n",
            "1221:\tlearn: 1698616.4293168\ttotal: 1m 47s\tremaining: 2m 39s\n",
            "1222:\tlearn: 1697460.3719839\ttotal: 1m 47s\tremaining: 2m 38s\n",
            "1223:\tlearn: 1697103.6821150\ttotal: 1m 47s\tremaining: 2m 38s\n",
            "1224:\tlearn: 1696546.2447187\ttotal: 1m 47s\tremaining: 2m 38s\n",
            "1225:\tlearn: 1694975.4963405\ttotal: 1m 47s\tremaining: 2m 38s\n",
            "1226:\tlearn: 1694394.6478895\ttotal: 1m 47s\tremaining: 2m 38s\n",
            "1227:\tlearn: 1693657.0713878\ttotal: 1m 47s\tremaining: 2m 38s\n",
            "1228:\tlearn: 1693108.8339968\ttotal: 1m 47s\tremaining: 2m 37s\n",
            "1229:\tlearn: 1692185.0533662\ttotal: 1m 47s\tremaining: 2m 37s\n",
            "1230:\tlearn: 1691611.4311326\ttotal: 1m 50s\tremaining: 2m 41s\n",
            "1231:\tlearn: 1690950.3783148\ttotal: 1m 50s\tremaining: 2m 41s\n",
            "1232:\tlearn: 1689411.2630740\ttotal: 1m 50s\tremaining: 2m 41s\n",
            "1233:\tlearn: 1688816.7615912\ttotal: 1m 50s\tremaining: 2m 41s\n",
            "1234:\tlearn: 1687662.8669876\ttotal: 1m 50s\tremaining: 2m 40s\n",
            "1235:\tlearn: 1687007.2948414\ttotal: 1m 50s\tremaining: 2m 40s\n",
            "1236:\tlearn: 1686551.6410887\ttotal: 1m 50s\tremaining: 2m 40s\n",
            "1237:\tlearn: 1685751.0995170\ttotal: 1m 50s\tremaining: 2m 40s\n",
            "1238:\tlearn: 1684283.6875734\ttotal: 1m 50s\tremaining: 2m 40s\n",
            "1239:\tlearn: 1683632.9005372\ttotal: 1m 50s\tremaining: 2m 40s\n",
            "1240:\tlearn: 1682808.9269498\ttotal: 1m 50s\tremaining: 2m 39s\n",
            "1241:\tlearn: 1682409.8546779\ttotal: 1m 50s\tremaining: 2m 39s\n",
            "1242:\tlearn: 1681540.4442379\ttotal: 1m 50s\tremaining: 2m 39s\n",
            "1243:\tlearn: 1680526.0980875\ttotal: 1m 50s\tremaining: 2m 39s\n",
            "1244:\tlearn: 1680152.3955311\ttotal: 1m 50s\tremaining: 2m 39s\n",
            "1245:\tlearn: 1678824.9414785\ttotal: 1m 50s\tremaining: 2m 39s\n",
            "1246:\tlearn: 1677371.7427535\ttotal: 1m 50s\tremaining: 2m 38s\n",
            "1247:\tlearn: 1676148.3864397\ttotal: 1m 50s\tremaining: 2m 38s\n",
            "1248:\tlearn: 1675357.5341259\ttotal: 1m 51s\tremaining: 2m 38s\n",
            "1249:\tlearn: 1674314.2536449\ttotal: 1m 51s\tremaining: 2m 38s\n",
            "1250:\tlearn: 1672972.6686144\ttotal: 1m 51s\tremaining: 2m 38s\n",
            "1251:\tlearn: 1672200.4020057\ttotal: 1m 51s\tremaining: 2m 38s\n",
            "1252:\tlearn: 1670830.1098962\ttotal: 1m 51s\tremaining: 2m 37s\n",
            "1253:\tlearn: 1670137.1768414\ttotal: 1m 51s\tremaining: 2m 37s\n",
            "1254:\tlearn: 1668100.5190844\ttotal: 1m 51s\tremaining: 2m 37s\n",
            "1255:\tlearn: 1667152.4439104\ttotal: 1m 51s\tremaining: 2m 37s\n",
            "1256:\tlearn: 1666709.1700846\ttotal: 1m 51s\tremaining: 2m 37s\n",
            "1257:\tlearn: 1666068.6482861\ttotal: 1m 51s\tremaining: 2m 37s\n",
            "1258:\tlearn: 1665000.0823636\ttotal: 1m 51s\tremaining: 2m 36s\n",
            "1259:\tlearn: 1664336.8894697\ttotal: 1m 51s\tremaining: 2m 36s\n",
            "1260:\tlearn: 1662878.4212314\ttotal: 1m 51s\tremaining: 2m 36s\n",
            "1261:\tlearn: 1662463.6017066\ttotal: 1m 51s\tremaining: 2m 36s\n",
            "1262:\tlearn: 1661125.1456846\ttotal: 1m 51s\tremaining: 2m 36s\n",
            "1263:\tlearn: 1660169.3245767\ttotal: 1m 51s\tremaining: 2m 36s\n",
            "1264:\tlearn: 1659710.7961359\ttotal: 1m 51s\tremaining: 2m 35s\n",
            "1265:\tlearn: 1658822.8698758\ttotal: 1m 51s\tremaining: 2m 35s\n",
            "1266:\tlearn: 1657862.5583233\ttotal: 1m 51s\tremaining: 2m 35s\n",
            "1267:\tlearn: 1657334.4903754\ttotal: 1m 51s\tremaining: 2m 35s\n",
            "1268:\tlearn: 1656354.9163268\ttotal: 1m 51s\tremaining: 2m 35s\n",
            "1269:\tlearn: 1655624.8569756\ttotal: 1m 51s\tremaining: 2m 35s\n",
            "1270:\tlearn: 1654887.0393325\ttotal: 1m 51s\tremaining: 2m 34s\n",
            "1271:\tlearn: 1653846.9268990\ttotal: 1m 51s\tremaining: 2m 34s\n",
            "1272:\tlearn: 1652634.7973966\ttotal: 1m 51s\tremaining: 2m 34s\n",
            "1273:\tlearn: 1651453.1403597\ttotal: 1m 51s\tremaining: 2m 34s\n",
            "1274:\tlearn: 1650408.6802446\ttotal: 1m 52s\tremaining: 2m 34s\n",
            "1275:\tlearn: 1649705.2319419\ttotal: 1m 52s\tremaining: 2m 34s\n",
            "1276:\tlearn: 1649135.1263035\ttotal: 1m 52s\tremaining: 2m 34s\n",
            "1277:\tlearn: 1647756.6282595\ttotal: 1m 52s\tremaining: 2m 33s\n",
            "1278:\tlearn: 1647266.4499243\ttotal: 1m 52s\tremaining: 2m 33s\n",
            "1279:\tlearn: 1646546.8328291\ttotal: 1m 52s\tremaining: 2m 33s\n",
            "1280:\tlearn: 1645859.3582609\ttotal: 1m 52s\tremaining: 2m 33s\n",
            "1281:\tlearn: 1644486.2899358\ttotal: 1m 52s\tremaining: 2m 33s\n",
            "1282:\tlearn: 1643079.1521539\ttotal: 1m 52s\tremaining: 2m 33s\n",
            "1283:\tlearn: 1641744.3856067\ttotal: 1m 52s\tremaining: 2m 32s\n",
            "1284:\tlearn: 1641111.3928462\ttotal: 1m 52s\tremaining: 2m 32s\n",
            "1285:\tlearn: 1640560.4182055\ttotal: 1m 52s\tremaining: 2m 32s\n",
            "1286:\tlearn: 1639660.4078855\ttotal: 1m 52s\tremaining: 2m 32s\n",
            "1287:\tlearn: 1638706.5699615\ttotal: 1m 52s\tremaining: 2m 32s\n",
            "1288:\tlearn: 1638099.1575417\ttotal: 1m 52s\tremaining: 2m 32s\n",
            "1289:\tlearn: 1637735.2023843\ttotal: 1m 52s\tremaining: 2m 31s\n",
            "1290:\tlearn: 1635665.3159027\ttotal: 1m 52s\tremaining: 2m 31s\n",
            "1291:\tlearn: 1634595.4444349\ttotal: 1m 52s\tremaining: 2m 31s\n",
            "1292:\tlearn: 1634198.3465953\ttotal: 1m 52s\tremaining: 2m 31s\n",
            "1293:\tlearn: 1633377.2646127\ttotal: 1m 52s\tremaining: 2m 31s\n",
            "1294:\tlearn: 1632927.8139789\ttotal: 1m 52s\tremaining: 2m 31s\n",
            "1295:\tlearn: 1632085.3171918\ttotal: 1m 52s\tremaining: 2m 31s\n",
            "1296:\tlearn: 1631375.6213778\ttotal: 1m 52s\tremaining: 2m 30s\n",
            "1297:\tlearn: 1630819.4982011\ttotal: 1m 52s\tremaining: 2m 30s\n",
            "1298:\tlearn: 1630284.3816215\ttotal: 1m 52s\tremaining: 2m 30s\n",
            "1299:\tlearn: 1629520.3103855\ttotal: 1m 52s\tremaining: 2m 30s\n",
            "1300:\tlearn: 1627839.4240168\ttotal: 1m 53s\tremaining: 2m 30s\n",
            "1301:\tlearn: 1626516.8152410\ttotal: 1m 53s\tremaining: 2m 30s\n",
            "1302:\tlearn: 1625804.3944801\ttotal: 1m 53s\tremaining: 2m 29s\n",
            "1303:\tlearn: 1624226.9084576\ttotal: 1m 53s\tremaining: 2m 29s\n",
            "1304:\tlearn: 1622622.0128440\ttotal: 1m 53s\tremaining: 2m 29s\n",
            "1305:\tlearn: 1621959.1436541\ttotal: 1m 53s\tremaining: 2m 29s\n",
            "1306:\tlearn: 1620967.5855904\ttotal: 1m 53s\tremaining: 2m 29s\n",
            "1307:\tlearn: 1620026.6067844\ttotal: 1m 53s\tremaining: 2m 29s\n",
            "1308:\tlearn: 1618941.2635003\ttotal: 1m 53s\tremaining: 2m 29s\n",
            "1309:\tlearn: 1617209.1659987\ttotal: 1m 53s\tremaining: 2m 28s\n",
            "1310:\tlearn: 1616480.5050337\ttotal: 1m 53s\tremaining: 2m 28s\n",
            "1311:\tlearn: 1615746.8838655\ttotal: 1m 53s\tremaining: 2m 28s\n",
            "1312:\tlearn: 1614895.3886776\ttotal: 1m 53s\tremaining: 2m 28s\n",
            "1313:\tlearn: 1613605.5066529\ttotal: 1m 53s\tremaining: 2m 28s\n",
            "1314:\tlearn: 1612951.4637944\ttotal: 1m 53s\tremaining: 2m 28s\n",
            "1315:\tlearn: 1612201.9843933\ttotal: 1m 53s\tremaining: 2m 27s\n",
            "1316:\tlearn: 1611399.9593296\ttotal: 1m 53s\tremaining: 2m 27s\n",
            "1317:\tlearn: 1610816.2763214\ttotal: 1m 53s\tremaining: 2m 27s\n",
            "1318:\tlearn: 1610052.8757487\ttotal: 1m 53s\tremaining: 2m 27s\n",
            "1319:\tlearn: 1609479.7990652\ttotal: 1m 53s\tremaining: 2m 27s\n",
            "1320:\tlearn: 1608780.4017929\ttotal: 1m 53s\tremaining: 2m 27s\n",
            "1321:\tlearn: 1607305.5162305\ttotal: 1m 53s\tremaining: 2m 27s\n",
            "1322:\tlearn: 1606249.3948078\ttotal: 1m 53s\tremaining: 2m 26s\n",
            "1323:\tlearn: 1604954.7162492\ttotal: 1m 53s\tremaining: 2m 26s\n",
            "1324:\tlearn: 1604431.5714992\ttotal: 1m 53s\tremaining: 2m 26s\n",
            "1325:\tlearn: 1603451.2387286\ttotal: 1m 54s\tremaining: 2m 26s\n",
            "1326:\tlearn: 1602828.5336319\ttotal: 1m 54s\tremaining: 2m 26s\n",
            "1327:\tlearn: 1602088.2951849\ttotal: 1m 54s\tremaining: 2m 26s\n",
            "1328:\tlearn: 1601585.7653341\ttotal: 1m 54s\tremaining: 2m 26s\n",
            "1329:\tlearn: 1600619.3627623\ttotal: 1m 54s\tremaining: 2m 25s\n",
            "1330:\tlearn: 1600031.2842439\ttotal: 1m 54s\tremaining: 2m 25s\n",
            "1331:\tlearn: 1598880.3843130\ttotal: 1m 54s\tremaining: 2m 25s\n",
            "1332:\tlearn: 1597859.9476489\ttotal: 1m 54s\tremaining: 2m 25s\n",
            "1333:\tlearn: 1597308.7215508\ttotal: 1m 54s\tremaining: 2m 25s\n",
            "1334:\tlearn: 1596226.7759950\ttotal: 1m 54s\tremaining: 2m 25s\n",
            "1335:\tlearn: 1594736.1650484\ttotal: 1m 54s\tremaining: 2m 25s\n",
            "1336:\tlearn: 1592945.4707317\ttotal: 1m 54s\tremaining: 2m 24s\n",
            "1337:\tlearn: 1592439.6793720\ttotal: 1m 54s\tremaining: 2m 24s\n",
            "1338:\tlearn: 1591870.4629061\ttotal: 1m 54s\tremaining: 2m 24s\n",
            "1339:\tlearn: 1590664.6198070\ttotal: 1m 54s\tremaining: 2m 24s\n",
            "1340:\tlearn: 1589437.7126202\ttotal: 1m 54s\tremaining: 2m 24s\n",
            "1341:\tlearn: 1588864.9621690\ttotal: 1m 54s\tremaining: 2m 24s\n",
            "1342:\tlearn: 1587255.0900793\ttotal: 1m 54s\tremaining: 2m 23s\n",
            "1343:\tlearn: 1586524.8311285\ttotal: 1m 54s\tremaining: 2m 23s\n",
            "1344:\tlearn: 1585504.7096265\ttotal: 1m 54s\tremaining: 2m 23s\n",
            "1345:\tlearn: 1584334.0218809\ttotal: 1m 54s\tremaining: 2m 23s\n",
            "1346:\tlearn: 1582678.7361586\ttotal: 1m 54s\tremaining: 2m 23s\n",
            "1347:\tlearn: 1581909.9150631\ttotal: 1m 54s\tremaining: 2m 23s\n",
            "1348:\tlearn: 1581440.2657984\ttotal: 1m 54s\tremaining: 2m 23s\n",
            "1349:\tlearn: 1580726.3777115\ttotal: 1m 54s\tremaining: 2m 22s\n",
            "1350:\tlearn: 1579292.8311962\ttotal: 1m 55s\tremaining: 2m 22s\n",
            "1351:\tlearn: 1577984.4734186\ttotal: 1m 55s\tremaining: 2m 22s\n",
            "1352:\tlearn: 1577515.7002502\ttotal: 1m 55s\tremaining: 2m 22s\n",
            "1353:\tlearn: 1576376.1937541\ttotal: 1m 55s\tremaining: 2m 22s\n",
            "1354:\tlearn: 1575759.5221348\ttotal: 1m 55s\tremaining: 2m 22s\n",
            "1355:\tlearn: 1574054.1492164\ttotal: 1m 55s\tremaining: 2m 22s\n",
            "1356:\tlearn: 1573432.5162168\ttotal: 1m 55s\tremaining: 2m 21s\n",
            "1357:\tlearn: 1572185.6752014\ttotal: 1m 55s\tremaining: 2m 21s\n",
            "1358:\tlearn: 1571460.8831600\ttotal: 1m 55s\tremaining: 2m 21s\n",
            "1359:\tlearn: 1570754.7804104\ttotal: 1m 55s\tremaining: 2m 21s\n",
            "1360:\tlearn: 1569752.3859537\ttotal: 1m 55s\tremaining: 2m 21s\n",
            "1361:\tlearn: 1568560.1306438\ttotal: 1m 55s\tremaining: 2m 21s\n",
            "1362:\tlearn: 1567883.2493575\ttotal: 1m 55s\tremaining: 2m 21s\n",
            "1363:\tlearn: 1567441.3366435\ttotal: 1m 55s\tremaining: 2m 20s\n",
            "1364:\tlearn: 1566446.9965199\ttotal: 1m 55s\tremaining: 2m 20s\n",
            "1365:\tlearn: 1564727.8128419\ttotal: 1m 55s\tremaining: 2m 20s\n",
            "1366:\tlearn: 1564202.6842592\ttotal: 1m 55s\tremaining: 2m 20s\n",
            "1367:\tlearn: 1563701.5638428\ttotal: 1m 55s\tremaining: 2m 20s\n",
            "1368:\tlearn: 1562999.4580366\ttotal: 1m 55s\tremaining: 2m 20s\n",
            "1369:\tlearn: 1562347.5625760\ttotal: 1m 55s\tremaining: 2m 20s\n",
            "1370:\tlearn: 1561499.7971760\ttotal: 1m 55s\tremaining: 2m 19s\n",
            "1371:\tlearn: 1560612.0903123\ttotal: 1m 55s\tremaining: 2m 19s\n",
            "1372:\tlearn: 1560151.6436090\ttotal: 1m 55s\tremaining: 2m 19s\n",
            "1373:\tlearn: 1559386.1724720\ttotal: 1m 55s\tremaining: 2m 19s\n",
            "1374:\tlearn: 1558530.7809450\ttotal: 1m 58s\tremaining: 2m 22s\n",
            "1375:\tlearn: 1557915.6571100\ttotal: 1m 58s\tremaining: 2m 22s\n",
            "1376:\tlearn: 1556980.8745229\ttotal: 1m 58s\tremaining: 2m 22s\n",
            "1377:\tlearn: 1556419.5880174\ttotal: 1m 58s\tremaining: 2m 22s\n",
            "1378:\tlearn: 1555785.6956951\ttotal: 1m 58s\tremaining: 2m 21s\n",
            "1379:\tlearn: 1554874.6170442\ttotal: 1m 58s\tremaining: 2m 21s\n",
            "1380:\tlearn: 1553941.6790629\ttotal: 1m 58s\tremaining: 2m 21s\n",
            "1381:\tlearn: 1553561.4327098\ttotal: 1m 58s\tremaining: 2m 21s\n",
            "1382:\tlearn: 1551892.7712408\ttotal: 1m 58s\tremaining: 2m 21s\n",
            "1383:\tlearn: 1551026.4353172\ttotal: 1m 58s\tremaining: 2m 21s\n",
            "1384:\tlearn: 1550233.4569997\ttotal: 1m 58s\tremaining: 2m 21s\n",
            "1385:\tlearn: 1549510.6618975\ttotal: 1m 58s\tremaining: 2m 20s\n",
            "1386:\tlearn: 1548611.0522811\ttotal: 1m 58s\tremaining: 2m 20s\n",
            "1387:\tlearn: 1547935.6260476\ttotal: 1m 58s\tremaining: 2m 20s\n",
            "1388:\tlearn: 1547438.4550007\ttotal: 1m 59s\tremaining: 2m 20s\n",
            "1389:\tlearn: 1546475.5744737\ttotal: 1m 59s\tremaining: 2m 20s\n",
            "1390:\tlearn: 1545684.2053682\ttotal: 1m 59s\tremaining: 2m 20s\n",
            "1391:\tlearn: 1544646.9771946\ttotal: 1m 59s\tremaining: 2m 20s\n",
            "1392:\tlearn: 1543766.9559510\ttotal: 1m 59s\tremaining: 2m 20s\n",
            "1393:\tlearn: 1542686.0285154\ttotal: 1m 59s\tremaining: 2m 19s\n",
            "1394:\tlearn: 1541467.1371759\ttotal: 2m 1s\tremaining: 2m 22s\n",
            "1395:\tlearn: 1540343.4088209\ttotal: 2m 1s\tremaining: 2m 22s\n",
            "1396:\tlearn: 1539246.3007663\ttotal: 2m 1s\tremaining: 2m 22s\n",
            "1397:\tlearn: 1538636.6906481\ttotal: 2m 1s\tremaining: 2m 22s\n",
            "1398:\tlearn: 1537606.2815521\ttotal: 2m 2s\tremaining: 2m 22s\n",
            "1399:\tlearn: 1537222.3808266\ttotal: 2m 2s\tremaining: 2m 22s\n",
            "1400:\tlearn: 1536307.9300234\ttotal: 2m 2s\tremaining: 2m 21s\n",
            "1401:\tlearn: 1535553.7591038\ttotal: 2m 2s\tremaining: 2m 21s\n",
            "1402:\tlearn: 1534869.3908386\ttotal: 2m 2s\tremaining: 2m 21s\n",
            "1403:\tlearn: 1533933.4885198\ttotal: 2m 2s\tremaining: 2m 21s\n",
            "1404:\tlearn: 1532465.8892055\ttotal: 2m 2s\tremaining: 2m 21s\n",
            "1405:\tlearn: 1531497.4535243\ttotal: 2m 2s\tremaining: 2m 21s\n",
            "1406:\tlearn: 1530712.1439213\ttotal: 2m 4s\tremaining: 2m 24s\n",
            "1407:\tlearn: 1529983.7543544\ttotal: 2m 4s\tremaining: 2m 23s\n",
            "1408:\tlearn: 1528923.6773104\ttotal: 2m 4s\tremaining: 2m 23s\n",
            "1409:\tlearn: 1528244.6482876\ttotal: 2m 5s\tremaining: 2m 23s\n",
            "1410:\tlearn: 1527356.2677584\ttotal: 2m 5s\tremaining: 2m 23s\n",
            "1411:\tlearn: 1526502.7217351\ttotal: 2m 5s\tremaining: 2m 23s\n",
            "1412:\tlearn: 1525332.5359324\ttotal: 2m 5s\tremaining: 2m 23s\n",
            "1413:\tlearn: 1524443.1511199\ttotal: 2m 5s\tremaining: 2m 23s\n",
            "1414:\tlearn: 1523929.5859664\ttotal: 2m 5s\tremaining: 2m 22s\n",
            "1415:\tlearn: 1523174.9222167\ttotal: 2m 5s\tremaining: 2m 22s\n",
            "1416:\tlearn: 1522347.9185909\ttotal: 2m 5s\tremaining: 2m 22s\n",
            "1417:\tlearn: 1521635.8778814\ttotal: 2m 5s\tremaining: 2m 22s\n",
            "1418:\tlearn: 1520798.2426171\ttotal: 2m 5s\tremaining: 2m 22s\n",
            "1419:\tlearn: 1519765.2001606\ttotal: 2m 5s\tremaining: 2m 22s\n",
            "1420:\tlearn: 1519220.4358318\ttotal: 2m 5s\tremaining: 2m 21s\n",
            "1421:\tlearn: 1518448.7501652\ttotal: 2m 5s\tremaining: 2m 21s\n",
            "1422:\tlearn: 1517847.1848094\ttotal: 2m 5s\tremaining: 2m 21s\n",
            "1423:\tlearn: 1516983.7676816\ttotal: 2m 5s\tremaining: 2m 21s\n",
            "1424:\tlearn: 1516638.9390787\ttotal: 2m 5s\tremaining: 2m 21s\n",
            "1425:\tlearn: 1515697.1626529\ttotal: 2m 5s\tremaining: 2m 21s\n",
            "1426:\tlearn: 1514701.4983509\ttotal: 2m 8s\tremaining: 2m 24s\n",
            "1427:\tlearn: 1514083.2251230\ttotal: 2m 8s\tremaining: 2m 23s\n",
            "1428:\tlearn: 1512638.7200101\ttotal: 2m 8s\tremaining: 2m 23s\n",
            "1429:\tlearn: 1512080.4683720\ttotal: 2m 8s\tremaining: 2m 23s\n",
            "1430:\tlearn: 1511158.6092872\ttotal: 2m 8s\tremaining: 2m 23s\n",
            "1431:\tlearn: 1510723.5550972\ttotal: 2m 8s\tremaining: 2m 23s\n",
            "1432:\tlearn: 1510079.5196177\ttotal: 2m 8s\tremaining: 2m 23s\n",
            "1433:\tlearn: 1509454.5297182\ttotal: 2m 8s\tremaining: 2m 23s\n",
            "1434:\tlearn: 1508548.2878275\ttotal: 2m 8s\tremaining: 2m 22s\n",
            "1435:\tlearn: 1507815.8501724\ttotal: 2m 8s\tremaining: 2m 22s\n",
            "1436:\tlearn: 1507413.9963945\ttotal: 2m 8s\tremaining: 2m 22s\n",
            "1437:\tlearn: 1506711.9753487\ttotal: 2m 8s\tremaining: 2m 22s\n",
            "1438:\tlearn: 1505818.8063697\ttotal: 2m 8s\tremaining: 2m 22s\n",
            "1439:\tlearn: 1505168.2159231\ttotal: 2m 8s\tremaining: 2m 22s\n",
            "1440:\tlearn: 1503586.1708061\ttotal: 2m 8s\tremaining: 2m 22s\n",
            "1441:\tlearn: 1502932.5232525\ttotal: 2m 8s\tremaining: 2m 21s\n",
            "1442:\tlearn: 1501496.5036382\ttotal: 2m 8s\tremaining: 2m 21s\n",
            "1443:\tlearn: 1500840.9134515\ttotal: 2m 8s\tremaining: 2m 21s\n",
            "1444:\tlearn: 1499874.9444360\ttotal: 2m 8s\tremaining: 2m 21s\n",
            "1445:\tlearn: 1499010.6898057\ttotal: 2m 8s\tremaining: 2m 21s\n",
            "1446:\tlearn: 1498446.1138241\ttotal: 2m 9s\tremaining: 2m 21s\n",
            "1447:\tlearn: 1497621.0273748\ttotal: 2m 9s\tremaining: 2m 20s\n",
            "1448:\tlearn: 1496962.2601220\ttotal: 2m 9s\tremaining: 2m 20s\n",
            "1449:\tlearn: 1495955.8811185\ttotal: 2m 9s\tremaining: 2m 20s\n",
            "1450:\tlearn: 1495184.9512889\ttotal: 2m 9s\tremaining: 2m 20s\n",
            "1451:\tlearn: 1494442.6474588\ttotal: 2m 9s\tremaining: 2m 20s\n",
            "1452:\tlearn: 1493207.8412101\ttotal: 2m 9s\tremaining: 2m 20s\n",
            "1453:\tlearn: 1492418.9530714\ttotal: 2m 9s\tremaining: 2m 20s\n",
            "1454:\tlearn: 1491776.0082821\ttotal: 2m 9s\tremaining: 2m 19s\n",
            "1455:\tlearn: 1491121.0458454\ttotal: 2m 9s\tremaining: 2m 19s\n",
            "1456:\tlearn: 1490386.2469090\ttotal: 2m 9s\tremaining: 2m 19s\n",
            "1457:\tlearn: 1489805.4915765\ttotal: 2m 9s\tremaining: 2m 19s\n",
            "1458:\tlearn: 1489298.1071521\ttotal: 2m 9s\tremaining: 2m 19s\n",
            "1459:\tlearn: 1488618.2983754\ttotal: 2m 9s\tremaining: 2m 19s\n",
            "1460:\tlearn: 1487901.4893980\ttotal: 2m 9s\tremaining: 2m 19s\n",
            "1461:\tlearn: 1486359.7384753\ttotal: 2m 9s\tremaining: 2m 18s\n",
            "1462:\tlearn: 1485736.6710436\ttotal: 2m 9s\tremaining: 2m 18s\n",
            "1463:\tlearn: 1485221.2508514\ttotal: 2m 9s\tremaining: 2m 18s\n",
            "1464:\tlearn: 1483901.8173226\ttotal: 2m 9s\tremaining: 2m 18s\n",
            "1465:\tlearn: 1482997.7226687\ttotal: 2m 9s\tremaining: 2m 18s\n",
            "1466:\tlearn: 1482302.8344478\ttotal: 2m 9s\tremaining: 2m 18s\n",
            "1467:\tlearn: 1481411.4710372\ttotal: 2m 9s\tremaining: 2m 18s\n",
            "1468:\tlearn: 1480904.8463585\ttotal: 2m 9s\tremaining: 2m 17s\n",
            "1469:\tlearn: 1479997.7562861\ttotal: 2m 9s\tremaining: 2m 17s\n",
            "1470:\tlearn: 1479479.3298544\ttotal: 2m 9s\tremaining: 2m 17s\n",
            "1471:\tlearn: 1478902.8449954\ttotal: 2m 10s\tremaining: 2m 17s\n",
            "1472:\tlearn: 1478357.3013258\ttotal: 2m 10s\tremaining: 2m 17s\n",
            "1473:\tlearn: 1477689.3872704\ttotal: 2m 10s\tremaining: 2m 17s\n",
            "1474:\tlearn: 1476693.1090721\ttotal: 2m 10s\tremaining: 2m 17s\n",
            "1475:\tlearn: 1476041.2065418\ttotal: 2m 10s\tremaining: 2m 16s\n",
            "1476:\tlearn: 1475126.8005217\ttotal: 2m 10s\tremaining: 2m 16s\n",
            "1477:\tlearn: 1474279.2240805\ttotal: 2m 10s\tremaining: 2m 16s\n",
            "1478:\tlearn: 1473520.8096231\ttotal: 2m 10s\tremaining: 2m 16s\n",
            "1479:\tlearn: 1472854.3226860\ttotal: 2m 10s\tremaining: 2m 16s\n",
            "1480:\tlearn: 1472334.1552541\ttotal: 2m 10s\tremaining: 2m 16s\n",
            "1481:\tlearn: 1471948.0631420\ttotal: 2m 10s\tremaining: 2m 16s\n",
            "1482:\tlearn: 1471616.3901621\ttotal: 2m 10s\tremaining: 2m 15s\n",
            "1483:\tlearn: 1470753.3106819\ttotal: 2m 10s\tremaining: 2m 15s\n",
            "1484:\tlearn: 1469837.0105530\ttotal: 2m 10s\tremaining: 2m 15s\n",
            "1485:\tlearn: 1469367.9650215\ttotal: 2m 10s\tremaining: 2m 15s\n",
            "1486:\tlearn: 1468089.3870043\ttotal: 2m 10s\tremaining: 2m 15s\n",
            "1487:\tlearn: 1467368.6396624\ttotal: 2m 10s\tremaining: 2m 15s\n",
            "1488:\tlearn: 1466362.9065811\ttotal: 2m 13s\tremaining: 2m 17s\n",
            "1489:\tlearn: 1465956.3807730\ttotal: 2m 13s\tremaining: 2m 17s\n",
            "1490:\tlearn: 1465079.4285252\ttotal: 2m 13s\tremaining: 2m 17s\n",
            "1491:\tlearn: 1464592.7514793\ttotal: 2m 13s\tremaining: 2m 17s\n",
            "1492:\tlearn: 1463459.5257035\ttotal: 2m 13s\tremaining: 2m 17s\n",
            "1493:\tlearn: 1462490.2335127\ttotal: 2m 13s\tremaining: 2m 17s\n",
            "1494:\tlearn: 1461539.6077094\ttotal: 2m 13s\tremaining: 2m 16s\n",
            "1495:\tlearn: 1460849.6144571\ttotal: 2m 13s\tremaining: 2m 16s\n",
            "1496:\tlearn: 1460493.8099270\ttotal: 2m 13s\tremaining: 2m 16s\n",
            "1497:\tlearn: 1459891.3788927\ttotal: 2m 13s\tremaining: 2m 16s\n",
            "1498:\tlearn: 1459086.1291159\ttotal: 2m 13s\tremaining: 2m 16s\n",
            "1499:\tlearn: 1458289.6503963\ttotal: 2m 13s\tremaining: 2m 16s\n",
            "1500:\tlearn: 1458004.5031409\ttotal: 2m 13s\tremaining: 2m 16s\n",
            "1501:\tlearn: 1457506.8778593\ttotal: 2m 13s\tremaining: 2m 15s\n",
            "1502:\tlearn: 1456931.3057767\ttotal: 2m 13s\tremaining: 2m 15s\n",
            "1503:\tlearn: 1455622.1958365\ttotal: 2m 13s\tremaining: 2m 15s\n",
            "1504:\tlearn: 1455059.6308793\ttotal: 2m 13s\tremaining: 2m 15s\n",
            "1505:\tlearn: 1453997.5148930\ttotal: 2m 13s\tremaining: 2m 15s\n",
            "1506:\tlearn: 1453231.8332650\ttotal: 2m 13s\tremaining: 2m 15s\n",
            "1507:\tlearn: 1452628.0372377\ttotal: 2m 14s\tremaining: 2m 15s\n",
            "1508:\tlearn: 1451709.1530746\ttotal: 2m 14s\tremaining: 2m 14s\n",
            "1509:\tlearn: 1450855.5158619\ttotal: 2m 14s\tremaining: 2m 14s\n",
            "1510:\tlearn: 1450104.9894899\ttotal: 2m 14s\tremaining: 2m 14s\n",
            "1511:\tlearn: 1449301.7096306\ttotal: 2m 14s\tremaining: 2m 14s\n",
            "1512:\tlearn: 1448546.1017233\ttotal: 2m 14s\tremaining: 2m 14s\n",
            "1513:\tlearn: 1447646.1451451\ttotal: 2m 14s\tremaining: 2m 14s\n",
            "1514:\tlearn: 1447198.2664639\ttotal: 2m 14s\tremaining: 2m 14s\n",
            "1515:\tlearn: 1446871.3022255\ttotal: 2m 14s\tremaining: 2m 14s\n",
            "1516:\tlearn: 1445865.5231635\ttotal: 2m 16s\tremaining: 2m 16s\n",
            "1517:\tlearn: 1445054.8537040\ttotal: 2m 16s\tremaining: 2m 16s\n",
            "1518:\tlearn: 1444378.1708935\ttotal: 2m 16s\tremaining: 2m 16s\n",
            "1519:\tlearn: 1443767.0842896\ttotal: 2m 17s\tremaining: 2m 16s\n",
            "1520:\tlearn: 1443241.5925992\ttotal: 2m 17s\tremaining: 2m 15s\n",
            "1521:\tlearn: 1442630.7450104\ttotal: 2m 17s\tremaining: 2m 15s\n",
            "1522:\tlearn: 1441320.9489293\ttotal: 2m 17s\tremaining: 2m 15s\n",
            "1523:\tlearn: 1440669.3904548\ttotal: 2m 19s\tremaining: 2m 18s\n",
            "1524:\tlearn: 1440045.3376714\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "1525:\tlearn: 1439107.4579264\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "1526:\tlearn: 1438454.0743385\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "1527:\tlearn: 1437425.0379395\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "1528:\tlearn: 1437195.9096754\ttotal: 2m 19s\tremaining: 2m 17s\n",
            "1529:\tlearn: 1436289.1712853\ttotal: 2m 20s\tremaining: 2m 17s\n",
            "1530:\tlearn: 1435044.9276117\ttotal: 2m 20s\tremaining: 2m 17s\n",
            "1531:\tlearn: 1434193.3201473\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "1532:\tlearn: 1433928.0632599\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "1533:\tlearn: 1433466.5870400\ttotal: 2m 20s\tremaining: 2m 16s\n",
            "1534:\tlearn: 1432867.0394410\ttotal: 2m 22s\tremaining: 2m 19s\n",
            "1535:\tlearn: 1432064.3128699\ttotal: 2m 22s\tremaining: 2m 18s\n",
            "1536:\tlearn: 1431830.5962815\ttotal: 2m 22s\tremaining: 2m 18s\n",
            "1537:\tlearn: 1431364.2769981\ttotal: 2m 22s\tremaining: 2m 18s\n",
            "1538:\tlearn: 1430489.6860232\ttotal: 2m 22s\tremaining: 2m 18s\n",
            "1539:\tlearn: 1429754.7438747\ttotal: 2m 22s\tremaining: 2m 18s\n",
            "1540:\tlearn: 1429197.9089265\ttotal: 2m 22s\tremaining: 2m 18s\n",
            "1541:\tlearn: 1427823.6602091\ttotal: 2m 23s\tremaining: 2m 18s\n",
            "1542:\tlearn: 1427345.3196228\ttotal: 2m 25s\tremaining: 2m 20s\n",
            "1543:\tlearn: 1426362.0961369\ttotal: 2m 25s\tremaining: 2m 20s\n",
            "1544:\tlearn: 1425613.8088384\ttotal: 2m 25s\tremaining: 2m 20s\n",
            "1545:\tlearn: 1424976.3588510\ttotal: 2m 25s\tremaining: 2m 20s\n",
            "1546:\tlearn: 1423468.9774664\ttotal: 2m 25s\tremaining: 2m 19s\n",
            "1547:\tlearn: 1423065.6811059\ttotal: 2m 25s\tremaining: 2m 19s\n",
            "1548:\tlearn: 1422317.6220049\ttotal: 2m 25s\tremaining: 2m 19s\n",
            "1549:\tlearn: 1421148.5808836\ttotal: 2m 25s\tremaining: 2m 19s\n",
            "1550:\tlearn: 1420467.2820051\ttotal: 2m 25s\tremaining: 2m 19s\n",
            "1551:\tlearn: 1419917.4017592\ttotal: 2m 26s\tremaining: 2m 19s\n",
            "1552:\tlearn: 1419139.1939373\ttotal: 2m 26s\tremaining: 2m 18s\n",
            "1553:\tlearn: 1418926.2724057\ttotal: 2m 26s\tremaining: 2m 18s\n",
            "1554:\tlearn: 1418277.6426964\ttotal: 2m 26s\tremaining: 2m 18s\n",
            "1555:\tlearn: 1417538.3339711\ttotal: 2m 26s\tremaining: 2m 18s\n",
            "1556:\tlearn: 1416808.5079112\ttotal: 2m 26s\tremaining: 2m 18s\n",
            "1557:\tlearn: 1416297.9574332\ttotal: 2m 26s\tremaining: 2m 18s\n",
            "1558:\tlearn: 1415594.0652041\ttotal: 2m 26s\tremaining: 2m 18s\n",
            "1559:\tlearn: 1414913.6154544\ttotal: 2m 26s\tremaining: 2m 17s\n",
            "1560:\tlearn: 1414284.6819583\ttotal: 2m 26s\tremaining: 2m 17s\n",
            "1561:\tlearn: 1413988.3188563\ttotal: 2m 26s\tremaining: 2m 17s\n",
            "1562:\tlearn: 1413110.8500373\ttotal: 2m 26s\tremaining: 2m 17s\n",
            "1563:\tlearn: 1412710.2995294\ttotal: 2m 26s\tremaining: 2m 17s\n",
            "1564:\tlearn: 1411906.9920660\ttotal: 2m 26s\tremaining: 2m 17s\n",
            "1565:\tlearn: 1411099.1877546\ttotal: 2m 26s\tremaining: 2m 17s\n",
            "1566:\tlearn: 1410613.9380490\ttotal: 2m 26s\tremaining: 2m 16s\n",
            "1567:\tlearn: 1410204.6165772\ttotal: 2m 26s\tremaining: 2m 16s\n",
            "1568:\tlearn: 1409958.2991624\ttotal: 2m 26s\tremaining: 2m 16s\n",
            "1569:\tlearn: 1409025.6238563\ttotal: 2m 26s\tremaining: 2m 16s\n",
            "1570:\tlearn: 1408626.9567339\ttotal: 2m 26s\tremaining: 2m 16s\n",
            "1571:\tlearn: 1407817.0709928\ttotal: 2m 26s\tremaining: 2m 16s\n",
            "1572:\tlearn: 1406534.7329894\ttotal: 2m 26s\tremaining: 2m 15s\n",
            "1573:\tlearn: 1405527.5206932\ttotal: 2m 26s\tremaining: 2m 15s\n",
            "1574:\tlearn: 1405017.5541975\ttotal: 2m 26s\tremaining: 2m 15s\n",
            "1575:\tlearn: 1404098.0696114\ttotal: 2m 26s\tremaining: 2m 15s\n",
            "1576:\tlearn: 1403703.7509913\ttotal: 2m 26s\tremaining: 2m 15s\n",
            "1577:\tlearn: 1402762.7540766\ttotal: 2m 27s\tremaining: 2m 15s\n",
            "1578:\tlearn: 1401684.4148618\ttotal: 2m 27s\tremaining: 2m 15s\n",
            "1579:\tlearn: 1401008.6092721\ttotal: 2m 27s\tremaining: 2m 14s\n",
            "1580:\tlearn: 1400481.0892169\ttotal: 2m 27s\tremaining: 2m 14s\n",
            "1581:\tlearn: 1400082.4144305\ttotal: 2m 27s\tremaining: 2m 14s\n",
            "1582:\tlearn: 1399801.3653988\ttotal: 2m 27s\tremaining: 2m 14s\n",
            "1583:\tlearn: 1399046.8623203\ttotal: 2m 27s\tremaining: 2m 14s\n",
            "1584:\tlearn: 1398270.2922982\ttotal: 2m 27s\tremaining: 2m 14s\n",
            "1585:\tlearn: 1397913.5787550\ttotal: 2m 27s\tremaining: 2m 14s\n",
            "1586:\tlearn: 1397496.6071263\ttotal: 2m 27s\tremaining: 2m 13s\n",
            "1587:\tlearn: 1396224.5147605\ttotal: 2m 27s\tremaining: 2m 13s\n",
            "1588:\tlearn: 1395548.2121641\ttotal: 2m 27s\tremaining: 2m 13s\n",
            "1589:\tlearn: 1394818.2977697\ttotal: 2m 27s\tremaining: 2m 13s\n",
            "1590:\tlearn: 1394210.8996181\ttotal: 2m 27s\tremaining: 2m 13s\n",
            "1591:\tlearn: 1393276.3648929\ttotal: 2m 27s\tremaining: 2m 13s\n",
            "1592:\tlearn: 1392564.4103687\ttotal: 2m 27s\tremaining: 2m 13s\n",
            "1593:\tlearn: 1391853.9438162\ttotal: 2m 27s\tremaining: 2m 12s\n",
            "1594:\tlearn: 1390950.5221471\ttotal: 2m 27s\tremaining: 2m 12s\n",
            "1595:\tlearn: 1390395.9063988\ttotal: 2m 27s\tremaining: 2m 12s\n",
            "1596:\tlearn: 1389893.2912752\ttotal: 2m 27s\tremaining: 2m 12s\n",
            "1597:\tlearn: 1389302.4607381\ttotal: 2m 27s\tremaining: 2m 12s\n",
            "1598:\tlearn: 1388691.2740116\ttotal: 2m 27s\tremaining: 2m 12s\n",
            "1599:\tlearn: 1388284.4716548\ttotal: 2m 27s\tremaining: 2m 12s\n",
            "1600:\tlearn: 1388042.4341734\ttotal: 2m 27s\tremaining: 2m 11s\n",
            "1601:\tlearn: 1387448.8484296\ttotal: 2m 27s\tremaining: 2m 11s\n",
            "1602:\tlearn: 1386618.1263741\ttotal: 2m 27s\tremaining: 2m 11s\n",
            "1603:\tlearn: 1386117.2426304\ttotal: 2m 28s\tremaining: 2m 11s\n",
            "1604:\tlearn: 1385796.7224856\ttotal: 2m 28s\tremaining: 2m 11s\n",
            "1605:\tlearn: 1385535.2517099\ttotal: 2m 28s\tremaining: 2m 11s\n",
            "1606:\tlearn: 1384265.7541983\ttotal: 2m 28s\tremaining: 2m 11s\n",
            "1607:\tlearn: 1383599.5836777\ttotal: 2m 28s\tremaining: 2m 10s\n",
            "1608:\tlearn: 1382813.0500280\ttotal: 2m 28s\tremaining: 2m 10s\n",
            "1609:\tlearn: 1381706.7828589\ttotal: 2m 28s\tremaining: 2m 10s\n",
            "1610:\tlearn: 1380829.8508646\ttotal: 2m 28s\tremaining: 2m 10s\n",
            "1611:\tlearn: 1379429.1169028\ttotal: 2m 28s\tremaining: 2m 10s\n",
            "1612:\tlearn: 1378688.4311445\ttotal: 2m 28s\tremaining: 2m 10s\n",
            "1613:\tlearn: 1378277.5364249\ttotal: 2m 28s\tremaining: 2m 10s\n",
            "1614:\tlearn: 1377479.5831953\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "1615:\tlearn: 1376923.3017979\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "1616:\tlearn: 1376330.3141182\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "1617:\tlearn: 1375430.2026451\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "1618:\tlearn: 1374822.8246639\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "1619:\tlearn: 1374033.2500567\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "1620:\tlearn: 1373778.7642233\ttotal: 2m 28s\tremaining: 2m 9s\n",
            "1621:\tlearn: 1372878.3892507\ttotal: 2m 28s\tremaining: 2m 8s\n",
            "1622:\tlearn: 1371512.7693672\ttotal: 2m 28s\tremaining: 2m 8s\n",
            "1623:\tlearn: 1370818.8983622\ttotal: 2m 28s\tremaining: 2m 8s\n",
            "1624:\tlearn: 1370575.0914026\ttotal: 2m 28s\tremaining: 2m 8s\n",
            "1625:\tlearn: 1369930.0749354\ttotal: 2m 28s\tremaining: 2m 8s\n",
            "1626:\tlearn: 1369231.8342988\ttotal: 2m 28s\tremaining: 2m 8s\n",
            "1627:\tlearn: 1368416.3431850\ttotal: 2m 28s\tremaining: 2m 8s\n",
            "1628:\tlearn: 1367570.0499630\ttotal: 2m 28s\tremaining: 2m 7s\n",
            "1629:\tlearn: 1367024.3992213\ttotal: 2m 29s\tremaining: 2m 7s\n",
            "1630:\tlearn: 1366710.9456708\ttotal: 2m 29s\tremaining: 2m 7s\n",
            "1631:\tlearn: 1366332.0507148\ttotal: 2m 29s\tremaining: 2m 7s\n",
            "1632:\tlearn: 1365575.8233466\ttotal: 2m 29s\tremaining: 2m 7s\n",
            "1633:\tlearn: 1365047.1325713\ttotal: 2m 29s\tremaining: 2m 7s\n",
            "1634:\tlearn: 1364416.4283355\ttotal: 2m 29s\tremaining: 2m 7s\n",
            "1635:\tlearn: 1363460.2162980\ttotal: 2m 29s\tremaining: 2m 6s\n",
            "1636:\tlearn: 1362936.3211278\ttotal: 2m 29s\tremaining: 2m 6s\n",
            "1637:\tlearn: 1361980.1180430\ttotal: 2m 29s\tremaining: 2m 6s\n",
            "1638:\tlearn: 1361212.8062079\ttotal: 2m 29s\tremaining: 2m 6s\n",
            "1639:\tlearn: 1360676.6473133\ttotal: 2m 29s\tremaining: 2m 6s\n",
            "1640:\tlearn: 1360075.9460013\ttotal: 2m 29s\tremaining: 2m 6s\n",
            "1641:\tlearn: 1359685.3855921\ttotal: 2m 29s\tremaining: 2m 6s\n",
            "1642:\tlearn: 1358960.7620387\ttotal: 2m 29s\tremaining: 2m 5s\n",
            "1643:\tlearn: 1358235.8036511\ttotal: 2m 29s\tremaining: 2m 5s\n",
            "1644:\tlearn: 1357236.4937239\ttotal: 2m 29s\tremaining: 2m 5s\n",
            "1645:\tlearn: 1356437.7036634\ttotal: 2m 29s\tremaining: 2m 5s\n",
            "1646:\tlearn: 1355759.9004263\ttotal: 2m 29s\tremaining: 2m 5s\n",
            "1647:\tlearn: 1354880.0420820\ttotal: 2m 29s\tremaining: 2m 5s\n",
            "1648:\tlearn: 1354130.1774976\ttotal: 2m 29s\tremaining: 2m 5s\n",
            "1649:\tlearn: 1353731.7322976\ttotal: 2m 29s\tremaining: 2m 4s\n",
            "1650:\tlearn: 1353155.5237220\ttotal: 2m 29s\tremaining: 2m 4s\n",
            "1651:\tlearn: 1352210.2816106\ttotal: 2m 29s\tremaining: 2m 4s\n",
            "1652:\tlearn: 1351183.1706423\ttotal: 2m 29s\tremaining: 2m 4s\n",
            "1653:\tlearn: 1350731.9476618\ttotal: 2m 29s\tremaining: 2m 4s\n",
            "1654:\tlearn: 1349908.7357127\ttotal: 2m 29s\tremaining: 2m 4s\n",
            "1655:\tlearn: 1349375.3880862\ttotal: 2m 30s\tremaining: 2m 4s\n",
            "1656:\tlearn: 1348685.2769071\ttotal: 2m 30s\tremaining: 2m 4s\n",
            "1657:\tlearn: 1347790.1026128\ttotal: 2m 30s\tremaining: 2m 3s\n",
            "1658:\tlearn: 1347331.2340630\ttotal: 2m 30s\tremaining: 2m 3s\n",
            "1659:\tlearn: 1346639.4660098\ttotal: 2m 30s\tremaining: 2m 3s\n",
            "1660:\tlearn: 1345858.2239330\ttotal: 2m 30s\tremaining: 2m 3s\n",
            "1661:\tlearn: 1345176.3096813\ttotal: 2m 30s\tremaining: 2m 3s\n",
            "1662:\tlearn: 1344474.0558928\ttotal: 2m 30s\tremaining: 2m 3s\n",
            "1663:\tlearn: 1343967.6959728\ttotal: 2m 30s\tremaining: 2m 3s\n",
            "1664:\tlearn: 1343313.8978593\ttotal: 2m 30s\tremaining: 2m 2s\n",
            "1665:\tlearn: 1342349.9942114\ttotal: 2m 30s\tremaining: 2m 2s\n",
            "1666:\tlearn: 1341400.2202095\ttotal: 2m 30s\tremaining: 2m 2s\n",
            "1667:\tlearn: 1341066.9944086\ttotal: 2m 30s\tremaining: 2m 2s\n",
            "1668:\tlearn: 1340470.5132673\ttotal: 2m 30s\tremaining: 2m 2s\n",
            "1669:\tlearn: 1339613.3196306\ttotal: 2m 30s\tremaining: 2m 2s\n",
            "1670:\tlearn: 1339177.4874992\ttotal: 2m 30s\tremaining: 2m 2s\n",
            "1671:\tlearn: 1338678.0862445\ttotal: 2m 30s\tremaining: 2m 1s\n",
            "1672:\tlearn: 1337862.1268136\ttotal: 2m 30s\tremaining: 2m 1s\n",
            "1673:\tlearn: 1337106.4251028\ttotal: 2m 30s\tremaining: 2m 1s\n",
            "1674:\tlearn: 1336446.9583162\ttotal: 2m 30s\tremaining: 2m 1s\n",
            "1675:\tlearn: 1335836.9027429\ttotal: 2m 30s\tremaining: 2m 1s\n",
            "1676:\tlearn: 1335231.2878856\ttotal: 2m 30s\tremaining: 2m 1s\n",
            "1677:\tlearn: 1334489.3828642\ttotal: 2m 30s\tremaining: 2m 1s\n",
            "1678:\tlearn: 1333861.6708432\ttotal: 2m 30s\tremaining: 2m 1s\n",
            "1679:\tlearn: 1333329.8574409\ttotal: 2m 30s\tremaining: 2m\n",
            "1680:\tlearn: 1332815.5981916\ttotal: 2m 30s\tremaining: 2m\n",
            "1681:\tlearn: 1332108.6074976\ttotal: 2m 31s\tremaining: 2m\n",
            "1682:\tlearn: 1331214.3217389\ttotal: 2m 31s\tremaining: 2m\n",
            "1683:\tlearn: 1330595.4741375\ttotal: 2m 31s\tremaining: 2m\n",
            "1684:\tlearn: 1329788.7062077\ttotal: 2m 31s\tremaining: 2m\n",
            "1685:\tlearn: 1329423.4163239\ttotal: 2m 31s\tremaining: 2m\n",
            "1686:\tlearn: 1329046.1845383\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "1687:\tlearn: 1328623.4354896\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "1688:\tlearn: 1328424.2243690\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "1689:\tlearn: 1328212.0994398\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "1690:\tlearn: 1327334.8896877\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "1691:\tlearn: 1326746.6233329\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "1692:\tlearn: 1325956.7463508\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "1693:\tlearn: 1325141.9749188\ttotal: 2m 31s\tremaining: 1m 59s\n",
            "1694:\tlearn: 1324638.8042045\ttotal: 2m 31s\tremaining: 1m 58s\n",
            "1695:\tlearn: 1324041.2588047\ttotal: 2m 31s\tremaining: 1m 58s\n",
            "1696:\tlearn: 1323163.0328832\ttotal: 2m 31s\tremaining: 1m 58s\n",
            "1697:\tlearn: 1322315.5184825\ttotal: 2m 31s\tremaining: 1m 58s\n",
            "1698:\tlearn: 1321427.7582052\ttotal: 2m 31s\tremaining: 1m 58s\n",
            "1699:\tlearn: 1320810.6191234\ttotal: 2m 31s\tremaining: 1m 58s\n",
            "1700:\tlearn: 1320354.2991191\ttotal: 2m 31s\tremaining: 1m 58s\n",
            "1701:\tlearn: 1319882.2030352\ttotal: 2m 31s\tremaining: 1m 57s\n",
            "1702:\tlearn: 1319330.5023586\ttotal: 2m 31s\tremaining: 1m 57s\n",
            "1703:\tlearn: 1318832.8736095\ttotal: 2m 31s\tremaining: 1m 57s\n",
            "1704:\tlearn: 1318358.1419445\ttotal: 2m 31s\tremaining: 1m 57s\n",
            "1705:\tlearn: 1317359.3814454\ttotal: 2m 31s\tremaining: 1m 57s\n",
            "1706:\tlearn: 1316709.9485503\ttotal: 2m 31s\tremaining: 1m 57s\n",
            "1707:\tlearn: 1316219.2739596\ttotal: 2m 31s\tremaining: 1m 57s\n",
            "1708:\tlearn: 1315446.3308636\ttotal: 2m 32s\tremaining: 1m 57s\n",
            "1709:\tlearn: 1314760.0060847\ttotal: 2m 32s\tremaining: 1m 56s\n",
            "1710:\tlearn: 1314051.7393245\ttotal: 2m 32s\tremaining: 1m 56s\n",
            "1711:\tlearn: 1313250.1249132\ttotal: 2m 32s\tremaining: 1m 56s\n",
            "1712:\tlearn: 1311952.5425856\ttotal: 2m 32s\tremaining: 1m 56s\n",
            "1713:\tlearn: 1311278.7705666\ttotal: 2m 32s\tremaining: 1m 56s\n",
            "1714:\tlearn: 1310836.5248164\ttotal: 2m 32s\tremaining: 1m 56s\n",
            "1715:\tlearn: 1309946.8400686\ttotal: 2m 32s\tremaining: 1m 56s\n",
            "1716:\tlearn: 1309525.0579038\ttotal: 2m 32s\tremaining: 1m 55s\n",
            "1717:\tlearn: 1308366.4559684\ttotal: 2m 32s\tremaining: 1m 55s\n",
            "1718:\tlearn: 1307537.4458426\ttotal: 2m 32s\tremaining: 1m 55s\n",
            "1719:\tlearn: 1307385.0038219\ttotal: 2m 32s\tremaining: 1m 55s\n",
            "1720:\tlearn: 1306812.3443874\ttotal: 2m 32s\tremaining: 1m 55s\n",
            "1721:\tlearn: 1306186.6802443\ttotal: 2m 32s\tremaining: 1m 55s\n",
            "1722:\tlearn: 1305952.1139706\ttotal: 2m 32s\tremaining: 1m 55s\n",
            "1723:\tlearn: 1305335.9135592\ttotal: 2m 32s\tremaining: 1m 55s\n",
            "1724:\tlearn: 1304628.0208395\ttotal: 2m 32s\tremaining: 1m 54s\n",
            "1725:\tlearn: 1304182.1907204\ttotal: 2m 32s\tremaining: 1m 54s\n",
            "1726:\tlearn: 1303299.4995380\ttotal: 2m 32s\tremaining: 1m 54s\n",
            "1727:\tlearn: 1302842.8054558\ttotal: 2m 32s\tremaining: 1m 54s\n",
            "1728:\tlearn: 1302307.5153947\ttotal: 2m 32s\tremaining: 1m 54s\n",
            "1729:\tlearn: 1301937.2382527\ttotal: 2m 32s\tremaining: 1m 54s\n",
            "1730:\tlearn: 1301429.9528302\ttotal: 2m 32s\tremaining: 1m 54s\n",
            "1731:\tlearn: 1300204.2506131\ttotal: 2m 32s\tremaining: 1m 54s\n",
            "1732:\tlearn: 1300011.4765885\ttotal: 2m 32s\tremaining: 1m 53s\n",
            "1733:\tlearn: 1299652.8181975\ttotal: 2m 32s\tremaining: 1m 53s\n",
            "1734:\tlearn: 1299056.2510787\ttotal: 2m 32s\tremaining: 1m 53s\n",
            "1735:\tlearn: 1298641.2481062\ttotal: 2m 33s\tremaining: 1m 53s\n",
            "1736:\tlearn: 1298096.3109312\ttotal: 2m 33s\tremaining: 1m 53s\n",
            "1737:\tlearn: 1297793.5747994\ttotal: 2m 33s\tremaining: 1m 53s\n",
            "1738:\tlearn: 1297279.2417622\ttotal: 2m 33s\tremaining: 1m 53s\n",
            "1739:\tlearn: 1296604.2737222\ttotal: 2m 33s\tremaining: 1m 53s\n",
            "1740:\tlearn: 1295926.9963857\ttotal: 2m 33s\tremaining: 1m 52s\n",
            "1741:\tlearn: 1295405.4452815\ttotal: 2m 33s\tremaining: 1m 52s\n",
            "1742:\tlearn: 1295028.6310687\ttotal: 2m 33s\tremaining: 1m 52s\n",
            "1743:\tlearn: 1294671.8821484\ttotal: 2m 33s\tremaining: 1m 52s\n",
            "1744:\tlearn: 1293830.9006901\ttotal: 2m 33s\tremaining: 1m 52s\n",
            "1745:\tlearn: 1293260.6874519\ttotal: 2m 35s\tremaining: 1m 54s\n",
            "1746:\tlearn: 1292607.4386519\ttotal: 2m 36s\tremaining: 1m 54s\n",
            "1747:\tlearn: 1291973.0542542\ttotal: 2m 36s\tremaining: 1m 53s\n",
            "1748:\tlearn: 1291651.6134316\ttotal: 2m 36s\tremaining: 1m 53s\n",
            "1749:\tlearn: 1291095.5583981\ttotal: 2m 36s\tremaining: 1m 53s\n",
            "1750:\tlearn: 1290491.4396043\ttotal: 2m 36s\tremaining: 1m 53s\n",
            "1751:\tlearn: 1290156.9610225\ttotal: 2m 36s\tremaining: 1m 53s\n",
            "1752:\tlearn: 1289350.0412914\ttotal: 2m 36s\tremaining: 1m 53s\n",
            "1753:\tlearn: 1288889.2432022\ttotal: 2m 36s\tremaining: 1m 53s\n",
            "1754:\tlearn: 1288189.9317521\ttotal: 2m 36s\tremaining: 1m 53s\n",
            "1755:\tlearn: 1287614.0843573\ttotal: 2m 36s\tremaining: 1m 52s\n",
            "1756:\tlearn: 1287222.8055449\ttotal: 2m 36s\tremaining: 1m 52s\n",
            "1757:\tlearn: 1286531.9691383\ttotal: 2m 36s\tremaining: 1m 52s\n",
            "1758:\tlearn: 1285733.5659263\ttotal: 2m 36s\tremaining: 1m 52s\n",
            "1759:\tlearn: 1284817.4665546\ttotal: 2m 36s\tremaining: 1m 52s\n",
            "1760:\tlearn: 1284182.1235402\ttotal: 2m 36s\tremaining: 1m 52s\n",
            "1761:\tlearn: 1283194.4147597\ttotal: 2m 36s\tremaining: 1m 52s\n",
            "1762:\tlearn: 1282676.7671108\ttotal: 2m 36s\tremaining: 1m 51s\n",
            "1763:\tlearn: 1282123.1278105\ttotal: 2m 36s\tremaining: 1m 51s\n",
            "1764:\tlearn: 1281687.6611190\ttotal: 2m 36s\tremaining: 1m 51s\n",
            "1765:\tlearn: 1281106.0353867\ttotal: 2m 36s\tremaining: 1m 51s\n",
            "1766:\tlearn: 1280208.9181888\ttotal: 2m 36s\tremaining: 1m 51s\n",
            "1767:\tlearn: 1279986.5220805\ttotal: 2m 36s\tremaining: 1m 51s\n",
            "1768:\tlearn: 1279753.2882297\ttotal: 2m 36s\tremaining: 1m 51s\n",
            "1769:\tlearn: 1279061.7520014\ttotal: 2m 36s\tremaining: 1m 51s\n",
            "1770:\tlearn: 1278575.8655132\ttotal: 2m 36s\tremaining: 1m 50s\n",
            "1771:\tlearn: 1277896.1237175\ttotal: 2m 36s\tremaining: 1m 50s\n",
            "1772:\tlearn: 1277514.8524903\ttotal: 2m 37s\tremaining: 1m 50s\n",
            "1773:\tlearn: 1276799.5544900\ttotal: 2m 37s\tremaining: 1m 50s\n",
            "1774:\tlearn: 1276347.5448080\ttotal: 2m 37s\tremaining: 1m 50s\n",
            "1775:\tlearn: 1276071.8993831\ttotal: 2m 37s\tremaining: 1m 50s\n",
            "1776:\tlearn: 1274845.3679870\ttotal: 2m 37s\tremaining: 1m 50s\n",
            "1777:\tlearn: 1274127.1566749\ttotal: 2m 37s\tremaining: 1m 50s\n",
            "1778:\tlearn: 1273601.7534673\ttotal: 2m 37s\tremaining: 1m 49s\n",
            "1779:\tlearn: 1273050.3352210\ttotal: 2m 37s\tremaining: 1m 49s\n",
            "1780:\tlearn: 1272670.9260214\ttotal: 2m 37s\tremaining: 1m 49s\n",
            "1781:\tlearn: 1271622.3377404\ttotal: 2m 37s\tremaining: 1m 49s\n",
            "1782:\tlearn: 1271125.8242221\ttotal: 2m 37s\tremaining: 1m 49s\n",
            "1783:\tlearn: 1270343.0907131\ttotal: 2m 37s\tremaining: 1m 49s\n",
            "1784:\tlearn: 1269830.5327526\ttotal: 2m 37s\tremaining: 1m 49s\n",
            "1785:\tlearn: 1269137.9026412\ttotal: 2m 37s\tremaining: 1m 49s\n",
            "1786:\tlearn: 1268522.2010047\ttotal: 2m 37s\tremaining: 1m 48s\n",
            "1787:\tlearn: 1268358.9083362\ttotal: 2m 37s\tremaining: 1m 48s\n",
            "1788:\tlearn: 1267664.6089457\ttotal: 2m 37s\tremaining: 1m 48s\n",
            "1789:\tlearn: 1267213.4647519\ttotal: 2m 37s\tremaining: 1m 48s\n",
            "1790:\tlearn: 1266434.5251106\ttotal: 2m 37s\tremaining: 1m 48s\n",
            "1791:\tlearn: 1266319.4732701\ttotal: 2m 37s\tremaining: 1m 48s\n",
            "1792:\tlearn: 1265556.9394002\ttotal: 2m 37s\tremaining: 1m 48s\n",
            "1793:\tlearn: 1265137.9740950\ttotal: 2m 37s\tremaining: 1m 48s\n",
            "1794:\tlearn: 1264458.6246678\ttotal: 2m 37s\tremaining: 1m 47s\n",
            "1795:\tlearn: 1263700.5948667\ttotal: 2m 37s\tremaining: 1m 47s\n",
            "1796:\tlearn: 1263078.3469076\ttotal: 2m 37s\tremaining: 1m 47s\n",
            "1797:\tlearn: 1262604.5921530\ttotal: 2m 37s\tremaining: 1m 47s\n",
            "1798:\tlearn: 1262195.3437397\ttotal: 2m 37s\tremaining: 1m 47s\n",
            "1799:\tlearn: 1261518.8174373\ttotal: 2m 38s\tremaining: 1m 47s\n",
            "1800:\tlearn: 1260856.9267199\ttotal: 2m 38s\tremaining: 1m 47s\n",
            "1801:\tlearn: 1260540.6415823\ttotal: 2m 38s\tremaining: 1m 47s\n",
            "1802:\tlearn: 1259833.3714154\ttotal: 2m 38s\tremaining: 1m 46s\n",
            "1803:\tlearn: 1259557.9744423\ttotal: 2m 38s\tremaining: 1m 46s\n",
            "1804:\tlearn: 1258686.2247705\ttotal: 2m 38s\tremaining: 1m 46s\n",
            "1805:\tlearn: 1258110.4827384\ttotal: 2m 38s\tremaining: 1m 46s\n",
            "1806:\tlearn: 1257381.5709879\ttotal: 2m 38s\tremaining: 1m 46s\n",
            "1807:\tlearn: 1256825.0342643\ttotal: 2m 38s\tremaining: 1m 46s\n",
            "1808:\tlearn: 1255737.1165112\ttotal: 2m 38s\tremaining: 1m 46s\n",
            "1809:\tlearn: 1255080.3582985\ttotal: 2m 38s\tremaining: 1m 46s\n",
            "1810:\tlearn: 1254532.1361955\ttotal: 2m 38s\tremaining: 1m 45s\n",
            "1811:\tlearn: 1253695.7594305\ttotal: 2m 38s\tremaining: 1m 45s\n",
            "1812:\tlearn: 1253274.7237031\ttotal: 2m 38s\tremaining: 1m 45s\n",
            "1813:\tlearn: 1252928.9695926\ttotal: 2m 38s\tremaining: 1m 45s\n",
            "1814:\tlearn: 1252625.9292769\ttotal: 2m 38s\tremaining: 1m 45s\n",
            "1815:\tlearn: 1252218.4494833\ttotal: 2m 38s\tremaining: 1m 45s\n",
            "1816:\tlearn: 1250975.9708221\ttotal: 2m 38s\tremaining: 1m 45s\n",
            "1817:\tlearn: 1250530.9249895\ttotal: 2m 38s\tremaining: 1m 45s\n",
            "1818:\tlearn: 1250079.7358967\ttotal: 2m 38s\tremaining: 1m 44s\n",
            "1819:\tlearn: 1249687.2697737\ttotal: 2m 38s\tremaining: 1m 44s\n",
            "1820:\tlearn: 1249031.1193752\ttotal: 2m 38s\tremaining: 1m 44s\n",
            "1821:\tlearn: 1248714.8468232\ttotal: 2m 38s\tremaining: 1m 44s\n",
            "1822:\tlearn: 1248105.5194520\ttotal: 2m 38s\tremaining: 1m 44s\n",
            "1823:\tlearn: 1247602.4452271\ttotal: 2m 38s\tremaining: 1m 44s\n",
            "1824:\tlearn: 1246841.6491268\ttotal: 2m 38s\tremaining: 1m 44s\n",
            "1825:\tlearn: 1246419.2953968\ttotal: 2m 39s\tremaining: 1m 44s\n",
            "1826:\tlearn: 1245981.0352496\ttotal: 2m 39s\tremaining: 1m 44s\n",
            "1827:\tlearn: 1245360.4522934\ttotal: 2m 39s\tremaining: 1m 43s\n",
            "1828:\tlearn: 1244095.3328267\ttotal: 2m 39s\tremaining: 1m 43s\n",
            "1829:\tlearn: 1243787.5899681\ttotal: 2m 39s\tremaining: 1m 43s\n",
            "1830:\tlearn: 1243312.0013755\ttotal: 2m 39s\tremaining: 1m 43s\n",
            "1831:\tlearn: 1242499.4334523\ttotal: 2m 39s\tremaining: 1m 43s\n",
            "1832:\tlearn: 1241413.0318535\ttotal: 2m 39s\tremaining: 1m 43s\n",
            "1833:\tlearn: 1241083.3401706\ttotal: 2m 39s\tremaining: 1m 43s\n",
            "1834:\tlearn: 1240485.7395060\ttotal: 2m 39s\tremaining: 1m 43s\n",
            "1835:\tlearn: 1239686.5697192\ttotal: 2m 39s\tremaining: 1m 42s\n",
            "1836:\tlearn: 1239188.7471163\ttotal: 2m 39s\tremaining: 1m 42s\n",
            "1837:\tlearn: 1238758.8876227\ttotal: 2m 39s\tremaining: 1m 42s\n",
            "1838:\tlearn: 1238086.5308878\ttotal: 2m 39s\tremaining: 1m 42s\n",
            "1839:\tlearn: 1237800.0283169\ttotal: 2m 39s\tremaining: 1m 42s\n",
            "1840:\tlearn: 1237133.0301320\ttotal: 2m 39s\tremaining: 1m 42s\n",
            "1841:\tlearn: 1236404.1379474\ttotal: 2m 42s\tremaining: 1m 43s\n",
            "1842:\tlearn: 1236025.7498701\ttotal: 2m 42s\tremaining: 1m 43s\n",
            "1843:\tlearn: 1235540.6618604\ttotal: 2m 42s\tremaining: 1m 43s\n",
            "1844:\tlearn: 1234771.3725012\ttotal: 2m 42s\tremaining: 1m 43s\n",
            "1845:\tlearn: 1233933.4073871\ttotal: 2m 42s\tremaining: 1m 43s\n",
            "1846:\tlearn: 1233072.9326947\ttotal: 2m 42s\tremaining: 1m 43s\n",
            "1847:\tlearn: 1232590.7170957\ttotal: 2m 42s\tremaining: 1m 43s\n",
            "1848:\tlearn: 1232381.7065918\ttotal: 2m 42s\tremaining: 1m 43s\n",
            "1849:\tlearn: 1231778.4296341\ttotal: 2m 42s\tremaining: 1m 42s\n",
            "1850:\tlearn: 1231314.7414846\ttotal: 2m 42s\tremaining: 1m 42s\n",
            "1851:\tlearn: 1230744.8990968\ttotal: 2m 42s\tremaining: 1m 42s\n",
            "1852:\tlearn: 1230080.9631496\ttotal: 2m 42s\tremaining: 1m 42s\n",
            "1853:\tlearn: 1229763.2863747\ttotal: 2m 42s\tremaining: 1m 42s\n",
            "1854:\tlearn: 1229361.3288305\ttotal: 2m 42s\tremaining: 1m 42s\n",
            "1855:\tlearn: 1228461.3376893\ttotal: 2m 42s\tremaining: 1m 42s\n",
            "1856:\tlearn: 1227644.3208195\ttotal: 2m 42s\tremaining: 1m 42s\n",
            "1857:\tlearn: 1227112.4771067\ttotal: 2m 42s\tremaining: 1m 41s\n",
            "1858:\tlearn: 1226620.7646456\ttotal: 2m 42s\tremaining: 1m 41s\n",
            "1859:\tlearn: 1226101.9858201\ttotal: 2m 42s\tremaining: 1m 41s\n",
            "1860:\tlearn: 1225884.1639234\ttotal: 2m 42s\tremaining: 1m 41s\n",
            "1861:\tlearn: 1225278.3402862\ttotal: 2m 42s\tremaining: 1m 41s\n",
            "1862:\tlearn: 1224123.9334271\ttotal: 2m 43s\tremaining: 1m 41s\n",
            "1863:\tlearn: 1223364.8012645\ttotal: 2m 43s\tremaining: 1m 41s\n",
            "1864:\tlearn: 1222887.6166789\ttotal: 2m 43s\tremaining: 1m 41s\n",
            "1865:\tlearn: 1222175.1306351\ttotal: 2m 43s\tremaining: 1m 40s\n",
            "1866:\tlearn: 1221728.2354442\ttotal: 2m 43s\tremaining: 1m 40s\n",
            "1867:\tlearn: 1220997.3884338\ttotal: 2m 43s\tremaining: 1m 40s\n",
            "1868:\tlearn: 1220667.2896489\ttotal: 2m 43s\tremaining: 1m 40s\n",
            "1869:\tlearn: 1220175.0604328\ttotal: 2m 43s\tremaining: 1m 40s\n",
            "1870:\tlearn: 1219665.3273203\ttotal: 2m 43s\tremaining: 1m 40s\n",
            "1871:\tlearn: 1219179.3439384\ttotal: 2m 43s\tremaining: 1m 40s\n",
            "1872:\tlearn: 1218644.7653318\ttotal: 2m 43s\tremaining: 1m 40s\n",
            "1873:\tlearn: 1218259.7958963\ttotal: 2m 43s\tremaining: 1m 39s\n",
            "1874:\tlearn: 1217371.7425535\ttotal: 2m 43s\tremaining: 1m 39s\n",
            "1875:\tlearn: 1216892.7311948\ttotal: 2m 43s\tremaining: 1m 39s\n",
            "1876:\tlearn: 1216316.7643396\ttotal: 2m 43s\tremaining: 1m 39s\n",
            "1877:\tlearn: 1215633.0598981\ttotal: 2m 43s\tremaining: 1m 39s\n",
            "1878:\tlearn: 1215202.0704891\ttotal: 2m 43s\tremaining: 1m 39s\n",
            "1879:\tlearn: 1214963.0323586\ttotal: 2m 43s\tremaining: 1m 39s\n",
            "1880:\tlearn: 1214435.7375121\ttotal: 2m 43s\tremaining: 1m 39s\n",
            "1881:\tlearn: 1213766.2281590\ttotal: 2m 43s\tremaining: 1m 39s\n",
            "1882:\tlearn: 1213279.0337209\ttotal: 2m 43s\tremaining: 1m 38s\n",
            "1883:\tlearn: 1212411.0090257\ttotal: 2m 43s\tremaining: 1m 38s\n",
            "1884:\tlearn: 1212005.3633738\ttotal: 2m 43s\tremaining: 1m 38s\n",
            "1885:\tlearn: 1211211.4327630\ttotal: 2m 43s\tremaining: 1m 38s\n",
            "1886:\tlearn: 1210578.0553601\ttotal: 2m 43s\tremaining: 1m 38s\n",
            "1887:\tlearn: 1210135.2957294\ttotal: 2m 43s\tremaining: 1m 38s\n",
            "1888:\tlearn: 1209797.9654810\ttotal: 2m 43s\tremaining: 1m 38s\n",
            "1889:\tlearn: 1209010.7203974\ttotal: 2m 43s\tremaining: 1m 38s\n",
            "1890:\tlearn: 1208585.6596546\ttotal: 2m 44s\tremaining: 1m 37s\n",
            "1891:\tlearn: 1208217.4040835\ttotal: 2m 44s\tremaining: 1m 37s\n",
            "1892:\tlearn: 1207545.7776650\ttotal: 2m 44s\tremaining: 1m 37s\n",
            "1893:\tlearn: 1207294.2734067\ttotal: 2m 44s\tremaining: 1m 37s\n",
            "1894:\tlearn: 1206959.9185358\ttotal: 2m 44s\tremaining: 1m 37s\n",
            "1895:\tlearn: 1206582.8039616\ttotal: 2m 44s\tremaining: 1m 37s\n",
            "1896:\tlearn: 1205829.4663192\ttotal: 2m 44s\tremaining: 1m 37s\n",
            "1897:\tlearn: 1205373.3036863\ttotal: 2m 44s\tremaining: 1m 37s\n",
            "1898:\tlearn: 1204991.8737339\ttotal: 2m 44s\tremaining: 1m 37s\n",
            "1899:\tlearn: 1204695.0271384\ttotal: 2m 44s\tremaining: 1m 36s\n",
            "1900:\tlearn: 1204396.9986465\ttotal: 2m 44s\tremaining: 1m 36s\n",
            "1901:\tlearn: 1203928.9861900\ttotal: 2m 44s\tremaining: 1m 36s\n",
            "1902:\tlearn: 1203444.1386558\ttotal: 2m 44s\tremaining: 1m 36s\n",
            "1903:\tlearn: 1203124.6114924\ttotal: 2m 44s\tremaining: 1m 36s\n",
            "1904:\tlearn: 1202637.4201323\ttotal: 2m 44s\tremaining: 1m 36s\n",
            "1905:\tlearn: 1201916.0064673\ttotal: 2m 44s\tremaining: 1m 36s\n",
            "1906:\tlearn: 1201087.2484583\ttotal: 2m 44s\tremaining: 1m 36s\n",
            "1907:\tlearn: 1200698.3174157\ttotal: 2m 44s\tremaining: 1m 35s\n",
            "1908:\tlearn: 1200165.9397304\ttotal: 2m 44s\tremaining: 1m 35s\n",
            "1909:\tlearn: 1199764.3209928\ttotal: 2m 44s\tremaining: 1m 35s\n",
            "1910:\tlearn: 1199320.3909444\ttotal: 2m 44s\tremaining: 1m 35s\n",
            "1911:\tlearn: 1198585.6694660\ttotal: 2m 44s\tremaining: 1m 35s\n",
            "1912:\tlearn: 1198171.9898858\ttotal: 2m 44s\tremaining: 1m 35s\n",
            "1913:\tlearn: 1197745.1472568\ttotal: 2m 44s\tremaining: 1m 35s\n",
            "1914:\tlearn: 1197405.4149924\ttotal: 2m 44s\tremaining: 1m 35s\n",
            "1915:\tlearn: 1196492.8431259\ttotal: 2m 44s\tremaining: 1m 35s\n",
            "1916:\tlearn: 1195821.6427878\ttotal: 2m 44s\tremaining: 1m 34s\n",
            "1917:\tlearn: 1195436.5667132\ttotal: 2m 47s\tremaining: 1m 36s\n",
            "1918:\tlearn: 1194830.0285540\ttotal: 2m 47s\tremaining: 1m 36s\n",
            "1919:\tlearn: 1194222.1852566\ttotal: 2m 47s\tremaining: 1m 36s\n",
            "1920:\tlearn: 1193735.9691677\ttotal: 2m 47s\tremaining: 1m 35s\n",
            "1921:\tlearn: 1193479.8453911\ttotal: 2m 47s\tremaining: 1m 35s\n",
            "1922:\tlearn: 1192961.9308554\ttotal: 2m 47s\tremaining: 1m 35s\n",
            "1923:\tlearn: 1192652.8924174\ttotal: 2m 50s\tremaining: 1m 37s\n",
            "1924:\tlearn: 1192199.8296518\ttotal: 2m 50s\tremaining: 1m 37s\n",
            "1925:\tlearn: 1191645.0486378\ttotal: 2m 50s\tremaining: 1m 36s\n",
            "1926:\tlearn: 1191131.5198192\ttotal: 2m 50s\tremaining: 1m 36s\n",
            "1927:\tlearn: 1190629.4858751\ttotal: 2m 50s\tremaining: 1m 36s\n",
            "1928:\tlearn: 1190358.6613519\ttotal: 2m 50s\tremaining: 1m 36s\n",
            "1929:\tlearn: 1189930.5658273\ttotal: 2m 50s\tremaining: 1m 36s\n",
            "1930:\tlearn: 1189432.0583068\ttotal: 2m 50s\tremaining: 1m 36s\n",
            "1931:\tlearn: 1188862.1951537\ttotal: 2m 50s\tremaining: 1m 36s\n",
            "1932:\tlearn: 1188297.5383740\ttotal: 2m 50s\tremaining: 1m 36s\n",
            "1933:\tlearn: 1187894.2870691\ttotal: 2m 50s\tremaining: 1m 35s\n",
            "1934:\tlearn: 1186946.7039228\ttotal: 2m 50s\tremaining: 1m 35s\n",
            "1935:\tlearn: 1186594.8060426\ttotal: 2m 50s\tremaining: 1m 35s\n",
            "1936:\tlearn: 1185910.3015425\ttotal: 2m 50s\tremaining: 1m 35s\n",
            "1937:\tlearn: 1184930.3018980\ttotal: 2m 50s\tremaining: 1m 35s\n",
            "1938:\tlearn: 1184685.4101874\ttotal: 2m 50s\tremaining: 1m 35s\n",
            "1939:\tlearn: 1184271.4294849\ttotal: 2m 51s\tremaining: 1m 35s\n",
            "1940:\tlearn: 1183855.1130725\ttotal: 2m 51s\tremaining: 1m 35s\n",
            "1941:\tlearn: 1183383.4349229\ttotal: 2m 51s\tremaining: 1m 34s\n",
            "1942:\tlearn: 1182829.2019035\ttotal: 2m 51s\tremaining: 1m 34s\n",
            "1943:\tlearn: 1182499.1790592\ttotal: 2m 51s\tremaining: 1m 34s\n",
            "1944:\tlearn: 1181815.7291025\ttotal: 2m 51s\tremaining: 1m 34s\n",
            "1945:\tlearn: 1181112.0726552\ttotal: 2m 51s\tremaining: 1m 34s\n",
            "1946:\tlearn: 1180620.5592652\ttotal: 2m 51s\tremaining: 1m 34s\n",
            "1947:\tlearn: 1180291.7980175\ttotal: 2m 51s\tremaining: 1m 34s\n",
            "1948:\tlearn: 1179549.9905527\ttotal: 2m 53s\tremaining: 1m 35s\n",
            "1949:\tlearn: 1179092.8362516\ttotal: 2m 53s\tremaining: 1m 35s\n",
            "1950:\tlearn: 1178054.1639323\ttotal: 2m 54s\tremaining: 1m 35s\n",
            "1951:\tlearn: 1177350.1838296\ttotal: 2m 54s\tremaining: 1m 35s\n",
            "1952:\tlearn: 1176689.2512421\ttotal: 2m 54s\tremaining: 1m 35s\n",
            "1953:\tlearn: 1176181.2773152\ttotal: 2m 54s\tremaining: 1m 35s\n",
            "1954:\tlearn: 1175978.7740835\ttotal: 2m 54s\tremaining: 1m 34s\n",
            "1955:\tlearn: 1175563.0334683\ttotal: 2m 54s\tremaining: 1m 34s\n",
            "1956:\tlearn: 1175273.2497068\ttotal: 2m 54s\tremaining: 1m 34s\n",
            "1957:\tlearn: 1174526.4653421\ttotal: 2m 54s\tremaining: 1m 34s\n",
            "1958:\tlearn: 1174034.8861013\ttotal: 2m 54s\tremaining: 1m 34s\n",
            "1959:\tlearn: 1173659.1862740\ttotal: 2m 54s\tremaining: 1m 34s\n",
            "1960:\tlearn: 1173298.9130152\ttotal: 2m 54s\tremaining: 1m 34s\n",
            "1961:\tlearn: 1172780.6348906\ttotal: 2m 54s\tremaining: 1m 34s\n",
            "1962:\tlearn: 1172134.1470126\ttotal: 2m 54s\tremaining: 1m 33s\n",
            "1963:\tlearn: 1171533.0496409\ttotal: 2m 54s\tremaining: 1m 33s\n",
            "1964:\tlearn: 1170965.3286854\ttotal: 2m 54s\tremaining: 1m 33s\n",
            "1965:\tlearn: 1170712.4160244\ttotal: 2m 54s\tremaining: 1m 33s\n",
            "1966:\tlearn: 1170231.1157583\ttotal: 2m 54s\tremaining: 1m 33s\n",
            "1967:\tlearn: 1169122.0598142\ttotal: 2m 54s\tremaining: 1m 33s\n",
            "1968:\tlearn: 1168249.4421217\ttotal: 2m 54s\tremaining: 1m 33s\n",
            "1969:\tlearn: 1167705.5410623\ttotal: 2m 54s\tremaining: 1m 33s\n",
            "1970:\tlearn: 1167329.7359702\ttotal: 2m 54s\tremaining: 1m 32s\n",
            "1971:\tlearn: 1166948.6577672\ttotal: 2m 54s\tremaining: 1m 32s\n",
            "1972:\tlearn: 1166655.0210384\ttotal: 2m 54s\tremaining: 1m 32s\n",
            "1973:\tlearn: 1166081.2218327\ttotal: 2m 54s\tremaining: 1m 32s\n",
            "1974:\tlearn: 1165726.1249024\ttotal: 2m 54s\tremaining: 1m 32s\n",
            "1975:\tlearn: 1165217.7920727\ttotal: 2m 54s\tremaining: 1m 32s\n",
            "1976:\tlearn: 1164726.2090874\ttotal: 2m 55s\tremaining: 1m 32s\n",
            "1977:\tlearn: 1164252.8022937\ttotal: 2m 55s\tremaining: 1m 32s\n",
            "1978:\tlearn: 1163617.1025957\ttotal: 2m 55s\tremaining: 1m 32s\n",
            "1979:\tlearn: 1163125.9035469\ttotal: 2m 55s\tremaining: 1m 31s\n",
            "1980:\tlearn: 1162736.3670182\ttotal: 2m 55s\tremaining: 1m 31s\n",
            "1981:\tlearn: 1162203.7164932\ttotal: 2m 55s\tremaining: 1m 31s\n",
            "1982:\tlearn: 1161748.8972759\ttotal: 2m 55s\tremaining: 1m 31s\n",
            "1983:\tlearn: 1160603.8571645\ttotal: 2m 55s\tremaining: 1m 31s\n",
            "1984:\tlearn: 1159900.1324967\ttotal: 2m 55s\tremaining: 1m 31s\n",
            "1985:\tlearn: 1159026.4529242\ttotal: 2m 55s\tremaining: 1m 31s\n",
            "1986:\tlearn: 1158632.8863401\ttotal: 2m 55s\tremaining: 1m 31s\n",
            "1987:\tlearn: 1158206.1456976\ttotal: 2m 55s\tremaining: 1m 30s\n",
            "1988:\tlearn: 1157697.2788880\ttotal: 2m 55s\tremaining: 1m 30s\n",
            "1989:\tlearn: 1157114.0146212\ttotal: 2m 55s\tremaining: 1m 30s\n",
            "1990:\tlearn: 1156635.0088576\ttotal: 2m 55s\tremaining: 1m 30s\n",
            "1991:\tlearn: 1156227.0038298\ttotal: 2m 55s\tremaining: 1m 30s\n",
            "1992:\tlearn: 1155516.5244180\ttotal: 2m 55s\tremaining: 1m 30s\n",
            "1993:\tlearn: 1155051.3165560\ttotal: 2m 55s\tremaining: 1m 30s\n",
            "1994:\tlearn: 1154893.2979300\ttotal: 2m 55s\tremaining: 1m 30s\n",
            "1995:\tlearn: 1154178.7822105\ttotal: 2m 55s\tremaining: 1m 30s\n",
            "1996:\tlearn: 1153613.9337488\ttotal: 2m 55s\tremaining: 1m 29s\n",
            "1997:\tlearn: 1152753.6315121\ttotal: 2m 55s\tremaining: 1m 29s\n",
            "1998:\tlearn: 1152246.0181659\ttotal: 2m 55s\tremaining: 1m 29s\n",
            "1999:\tlearn: 1151738.9507564\ttotal: 2m 55s\tremaining: 1m 29s\n",
            "2000:\tlearn: 1151014.5836675\ttotal: 2m 55s\tremaining: 1m 29s\n",
            "2001:\tlearn: 1150784.2502388\ttotal: 2m 55s\tremaining: 1m 29s\n",
            "2002:\tlearn: 1150551.1963168\ttotal: 2m 55s\tremaining: 1m 29s\n",
            "2003:\tlearn: 1150211.9250966\ttotal: 2m 56s\tremaining: 1m 29s\n",
            "2004:\tlearn: 1149769.9824725\ttotal: 2m 56s\tremaining: 1m 29s\n",
            "2005:\tlearn: 1148829.2038717\ttotal: 2m 56s\tremaining: 1m 28s\n",
            "2006:\tlearn: 1148109.8779132\ttotal: 2m 56s\tremaining: 1m 28s\n",
            "2007:\tlearn: 1147456.0804927\ttotal: 2m 56s\tremaining: 1m 28s\n",
            "2008:\tlearn: 1146876.1908713\ttotal: 2m 56s\tremaining: 1m 28s\n",
            "2009:\tlearn: 1146219.2173804\ttotal: 2m 56s\tremaining: 1m 28s\n",
            "2010:\tlearn: 1146001.3923191\ttotal: 2m 56s\tremaining: 1m 28s\n",
            "2011:\tlearn: 1145731.0002079\ttotal: 2m 56s\tremaining: 1m 28s\n",
            "2012:\tlearn: 1145040.2200902\ttotal: 2m 56s\tremaining: 1m 28s\n",
            "2013:\tlearn: 1144701.9776154\ttotal: 2m 56s\tremaining: 1m 27s\n",
            "2014:\tlearn: 1144498.3332818\ttotal: 2m 56s\tremaining: 1m 27s\n",
            "2015:\tlearn: 1144106.4003601\ttotal: 2m 56s\tremaining: 1m 27s\n",
            "2016:\tlearn: 1143619.0961523\ttotal: 2m 56s\tremaining: 1m 27s\n",
            "2017:\tlearn: 1143339.2769687\ttotal: 2m 56s\tremaining: 1m 27s\n",
            "2018:\tlearn: 1142420.8057685\ttotal: 2m 56s\tremaining: 1m 27s\n",
            "2019:\tlearn: 1141989.3399442\ttotal: 2m 56s\tremaining: 1m 27s\n",
            "2020:\tlearn: 1141258.3243106\ttotal: 2m 56s\tremaining: 1m 27s\n",
            "2021:\tlearn: 1141040.0446135\ttotal: 2m 56s\tremaining: 1m 27s\n",
            "2022:\tlearn: 1140451.6261971\ttotal: 2m 56s\tremaining: 1m 26s\n",
            "2023:\tlearn: 1139802.7482226\ttotal: 2m 56s\tremaining: 1m 26s\n",
            "2024:\tlearn: 1139626.7392524\ttotal: 2m 56s\tremaining: 1m 26s\n",
            "2025:\tlearn: 1138996.0289528\ttotal: 2m 56s\tremaining: 1m 26s\n",
            "2026:\tlearn: 1138236.9277851\ttotal: 2m 56s\tremaining: 1m 26s\n",
            "2027:\tlearn: 1137638.3867602\ttotal: 2m 56s\tremaining: 1m 26s\n",
            "2028:\tlearn: 1137117.5573279\ttotal: 2m 56s\tremaining: 1m 26s\n",
            "2029:\tlearn: 1136650.0068538\ttotal: 2m 56s\tremaining: 1m 26s\n",
            "2030:\tlearn: 1135962.6325466\ttotal: 2m 57s\tremaining: 1m 26s\n",
            "2031:\tlearn: 1135359.1957872\ttotal: 2m 57s\tremaining: 1m 25s\n",
            "2032:\tlearn: 1134999.9574423\ttotal: 2m 57s\tremaining: 1m 25s\n",
            "2033:\tlearn: 1134608.2840397\ttotal: 2m 57s\tremaining: 1m 25s\n",
            "2034:\tlearn: 1133977.4177179\ttotal: 2m 57s\tremaining: 1m 25s\n",
            "2035:\tlearn: 1133351.1703810\ttotal: 2m 57s\tremaining: 1m 25s\n",
            "2036:\tlearn: 1132888.0641931\ttotal: 2m 57s\tremaining: 1m 25s\n",
            "2037:\tlearn: 1132413.7435466\ttotal: 2m 57s\tremaining: 1m 25s\n",
            "2038:\tlearn: 1131887.9256615\ttotal: 2m 57s\tremaining: 1m 25s\n",
            "2039:\tlearn: 1131313.4339882\ttotal: 2m 57s\tremaining: 1m 25s\n",
            "2040:\tlearn: 1130810.4582914\ttotal: 2m 57s\tremaining: 1m 24s\n",
            "2041:\tlearn: 1130377.8738552\ttotal: 2m 57s\tremaining: 1m 24s\n",
            "2042:\tlearn: 1129830.7122945\ttotal: 2m 57s\tremaining: 1m 24s\n",
            "2043:\tlearn: 1129310.2906555\ttotal: 2m 57s\tremaining: 1m 24s\n",
            "2044:\tlearn: 1129049.4546130\ttotal: 2m 57s\tremaining: 1m 24s\n",
            "2045:\tlearn: 1128504.2335462\ttotal: 2m 57s\tremaining: 1m 24s\n",
            "2046:\tlearn: 1128113.9335387\ttotal: 2m 57s\tremaining: 1m 24s\n",
            "2047:\tlearn: 1127663.1912026\ttotal: 2m 57s\tremaining: 1m 24s\n",
            "2048:\tlearn: 1127301.6952506\ttotal: 2m 57s\tremaining: 1m 24s\n",
            "2049:\tlearn: 1126910.6639259\ttotal: 2m 57s\tremaining: 1m 23s\n",
            "2050:\tlearn: 1126432.5746268\ttotal: 2m 57s\tremaining: 1m 23s\n",
            "2051:\tlearn: 1125950.0279165\ttotal: 2m 57s\tremaining: 1m 23s\n",
            "2052:\tlearn: 1125497.1657295\ttotal: 2m 57s\tremaining: 1m 23s\n",
            "2053:\tlearn: 1124966.8955125\ttotal: 2m 57s\tremaining: 1m 23s\n",
            "2054:\tlearn: 1124466.1929000\ttotal: 2m 57s\tremaining: 1m 23s\n",
            "2055:\tlearn: 1123885.4348107\ttotal: 2m 57s\tremaining: 1m 23s\n",
            "2056:\tlearn: 1123112.8391586\ttotal: 2m 58s\tremaining: 1m 23s\n",
            "2057:\tlearn: 1122580.8812524\ttotal: 2m 58s\tremaining: 1m 22s\n",
            "2058:\tlearn: 1122185.9250805\ttotal: 2m 58s\tremaining: 1m 22s\n",
            "2059:\tlearn: 1121860.4882928\ttotal: 2m 58s\tremaining: 1m 22s\n",
            "2060:\tlearn: 1120641.2634202\ttotal: 2m 58s\tremaining: 1m 22s\n",
            "2061:\tlearn: 1120150.4791549\ttotal: 2m 58s\tremaining: 1m 22s\n",
            "2062:\tlearn: 1119336.1766230\ttotal: 2m 58s\tremaining: 1m 22s\n",
            "2063:\tlearn: 1118767.8105396\ttotal: 2m 58s\tremaining: 1m 22s\n",
            "2064:\tlearn: 1118333.7219992\ttotal: 2m 58s\tremaining: 1m 22s\n",
            "2065:\tlearn: 1117780.6305054\ttotal: 2m 58s\tremaining: 1m 22s\n",
            "2066:\tlearn: 1117382.4260573\ttotal: 2m 58s\tremaining: 1m 21s\n",
            "2067:\tlearn: 1117123.2905759\ttotal: 2m 58s\tremaining: 1m 21s\n",
            "2068:\tlearn: 1116787.5690657\ttotal: 2m 58s\tremaining: 1m 21s\n",
            "2069:\tlearn: 1116575.4408260\ttotal: 2m 58s\tremaining: 1m 21s\n",
            "2070:\tlearn: 1116159.5739171\ttotal: 2m 58s\tremaining: 1m 21s\n",
            "2071:\tlearn: 1115993.7966301\ttotal: 2m 58s\tremaining: 1m 21s\n",
            "2072:\tlearn: 1115709.3500956\ttotal: 2m 58s\tremaining: 1m 21s\n",
            "2073:\tlearn: 1114983.7642969\ttotal: 2m 58s\tremaining: 1m 21s\n",
            "2074:\tlearn: 1114570.8293534\ttotal: 2m 58s\tremaining: 1m 21s\n",
            "2075:\tlearn: 1114169.5653781\ttotal: 2m 58s\tremaining: 1m 20s\n",
            "2076:\tlearn: 1114029.1118279\ttotal: 2m 58s\tremaining: 1m 20s\n",
            "2077:\tlearn: 1113184.9797803\ttotal: 2m 58s\tremaining: 1m 20s\n",
            "2078:\tlearn: 1112704.1092986\ttotal: 2m 58s\tremaining: 1m 20s\n",
            "2079:\tlearn: 1112309.0229816\ttotal: 2m 58s\tremaining: 1m 20s\n",
            "2080:\tlearn: 1111988.9170574\ttotal: 2m 58s\tremaining: 1m 20s\n",
            "2081:\tlearn: 1111659.5073657\ttotal: 2m 58s\tremaining: 1m 20s\n",
            "2082:\tlearn: 1111085.0179742\ttotal: 2m 58s\tremaining: 1m 20s\n",
            "2083:\tlearn: 1110689.4630449\ttotal: 2m 59s\tremaining: 1m 20s\n",
            "2084:\tlearn: 1110179.0542175\ttotal: 2m 59s\tremaining: 1m 19s\n",
            "2085:\tlearn: 1109444.3121837\ttotal: 2m 59s\tremaining: 1m 19s\n",
            "2086:\tlearn: 1108493.7425255\ttotal: 2m 59s\tremaining: 1m 19s\n",
            "2087:\tlearn: 1108067.2907576\ttotal: 2m 59s\tremaining: 1m 19s\n",
            "2088:\tlearn: 1107657.8909209\ttotal: 2m 59s\tremaining: 1m 19s\n",
            "2089:\tlearn: 1106707.9284605\ttotal: 2m 59s\tremaining: 1m 19s\n",
            "2090:\tlearn: 1106230.1173062\ttotal: 2m 59s\tremaining: 1m 19s\n",
            "2091:\tlearn: 1105931.7613255\ttotal: 2m 59s\tremaining: 1m 19s\n",
            "2092:\tlearn: 1105790.6572774\ttotal: 2m 59s\tremaining: 1m 19s\n",
            "2093:\tlearn: 1105253.3938489\ttotal: 2m 59s\tremaining: 1m 19s\n",
            "2094:\tlearn: 1104895.9147601\ttotal: 2m 59s\tremaining: 1m 18s\n",
            "2095:\tlearn: 1104526.0893163\ttotal: 2m 59s\tremaining: 1m 18s\n",
            "2096:\tlearn: 1104021.4257613\ttotal: 2m 59s\tremaining: 1m 18s\n",
            "2097:\tlearn: 1103637.4068507\ttotal: 2m 59s\tremaining: 1m 18s\n",
            "2098:\tlearn: 1102959.4974340\ttotal: 2m 59s\tremaining: 1m 18s\n",
            "2099:\tlearn: 1102195.2050519\ttotal: 2m 59s\tremaining: 1m 18s\n",
            "2100:\tlearn: 1101996.7327393\ttotal: 2m 59s\tremaining: 1m 18s\n",
            "2101:\tlearn: 1101697.9770695\ttotal: 2m 59s\tremaining: 1m 18s\n",
            "2102:\tlearn: 1101288.8056469\ttotal: 2m 59s\tremaining: 1m 18s\n",
            "2103:\tlearn: 1101031.9398533\ttotal: 2m 59s\tremaining: 1m 17s\n",
            "2104:\tlearn: 1100571.1211893\ttotal: 2m 59s\tremaining: 1m 17s\n",
            "2105:\tlearn: 1099993.4493104\ttotal: 2m 59s\tremaining: 1m 17s\n",
            "2106:\tlearn: 1099481.8525681\ttotal: 2m 59s\tremaining: 1m 17s\n",
            "2107:\tlearn: 1099127.2864497\ttotal: 2m 59s\tremaining: 1m 17s\n",
            "2108:\tlearn: 1098665.0386899\ttotal: 2m 59s\tremaining: 1m 17s\n",
            "2109:\tlearn: 1097789.8163946\ttotal: 3m\tremaining: 1m 17s\n",
            "2110:\tlearn: 1097551.9812226\ttotal: 3m\tremaining: 1m 17s\n",
            "2111:\tlearn: 1096921.2985970\ttotal: 3m\tremaining: 1m 17s\n",
            "2112:\tlearn: 1096442.3402787\ttotal: 3m\tremaining: 1m 16s\n",
            "2113:\tlearn: 1096090.4775885\ttotal: 3m\tremaining: 1m 16s\n",
            "2114:\tlearn: 1095589.5397516\ttotal: 3m\tremaining: 1m 16s\n",
            "2115:\tlearn: 1095345.0922175\ttotal: 3m\tremaining: 1m 16s\n",
            "2116:\tlearn: 1094942.6944751\ttotal: 3m\tremaining: 1m 16s\n",
            "2117:\tlearn: 1094329.1849405\ttotal: 3m\tremaining: 1m 16s\n",
            "2118:\tlearn: 1093315.6455680\ttotal: 3m\tremaining: 1m 16s\n",
            "2119:\tlearn: 1092924.1666565\ttotal: 3m\tremaining: 1m 16s\n",
            "2120:\tlearn: 1092237.3197251\ttotal: 3m\tremaining: 1m 16s\n",
            "2121:\tlearn: 1091697.8890507\ttotal: 3m\tremaining: 1m 16s\n",
            "2122:\tlearn: 1091198.0558120\ttotal: 3m\tremaining: 1m 15s\n",
            "2123:\tlearn: 1090602.3132982\ttotal: 3m\tremaining: 1m 15s\n",
            "2124:\tlearn: 1090276.7519997\ttotal: 3m\tremaining: 1m 15s\n",
            "2125:\tlearn: 1089894.5441957\ttotal: 3m 3s\tremaining: 1m 16s\n",
            "2126:\tlearn: 1089392.0577254\ttotal: 3m 3s\tremaining: 1m 16s\n",
            "2127:\tlearn: 1088884.7923720\ttotal: 3m 3s\tremaining: 1m 16s\n",
            "2128:\tlearn: 1088354.8947220\ttotal: 3m 3s\tremaining: 1m 16s\n",
            "2129:\tlearn: 1087842.2818267\ttotal: 3m 3s\tremaining: 1m 16s\n",
            "2130:\tlearn: 1087429.2360864\ttotal: 3m 3s\tremaining: 1m 16s\n",
            "2131:\tlearn: 1086616.5983639\ttotal: 3m 3s\tremaining: 1m 16s\n",
            "2132:\tlearn: 1086176.0585531\ttotal: 3m 3s\tremaining: 1m 15s\n",
            "2133:\tlearn: 1085862.7660255\ttotal: 3m 3s\tremaining: 1m 15s\n",
            "2134:\tlearn: 1085409.3590508\ttotal: 3m 3s\tremaining: 1m 15s\n",
            "2135:\tlearn: 1085068.4177184\ttotal: 3m 3s\tremaining: 1m 15s\n",
            "2136:\tlearn: 1084811.5724719\ttotal: 3m 3s\tremaining: 1m 15s\n",
            "2137:\tlearn: 1084283.0934129\ttotal: 3m 3s\tremaining: 1m 15s\n",
            "2138:\tlearn: 1083396.8625926\ttotal: 3m 3s\tremaining: 1m 15s\n",
            "2139:\tlearn: 1082949.0678901\ttotal: 3m 3s\tremaining: 1m 15s\n",
            "2140:\tlearn: 1082707.3846173\ttotal: 3m 3s\tremaining: 1m 15s\n",
            "2141:\tlearn: 1082451.3307893\ttotal: 3m 3s\tremaining: 1m 14s\n",
            "2142:\tlearn: 1082032.3114140\ttotal: 3m 3s\tremaining: 1m 14s\n",
            "2143:\tlearn: 1081780.5732892\ttotal: 3m 3s\tremaining: 1m 14s\n",
            "2144:\tlearn: 1081493.5655071\ttotal: 3m 3s\tremaining: 1m 14s\n",
            "2145:\tlearn: 1081294.5557492\ttotal: 3m 3s\tremaining: 1m 14s\n",
            "2146:\tlearn: 1081077.1734460\ttotal: 3m 4s\tremaining: 1m 14s\n",
            "2147:\tlearn: 1080609.6486005\ttotal: 3m 4s\tremaining: 1m 14s\n",
            "2148:\tlearn: 1079975.0968841\ttotal: 3m 4s\tremaining: 1m 14s\n",
            "2149:\tlearn: 1079628.4915188\ttotal: 3m 4s\tremaining: 1m 14s\n",
            "2150:\tlearn: 1078906.0904304\ttotal: 3m 4s\tremaining: 1m 14s\n",
            "2151:\tlearn: 1078488.3572803\ttotal: 3m 4s\tremaining: 1m 13s\n",
            "2152:\tlearn: 1077970.6645525\ttotal: 3m 4s\tremaining: 1m 13s\n",
            "2153:\tlearn: 1077739.3306259\ttotal: 3m 4s\tremaining: 1m 13s\n",
            "2154:\tlearn: 1077435.1158136\ttotal: 3m 4s\tremaining: 1m 13s\n",
            "2155:\tlearn: 1077300.5982460\ttotal: 3m 4s\tremaining: 1m 13s\n",
            "2156:\tlearn: 1076899.7169312\ttotal: 3m 4s\tremaining: 1m 13s\n",
            "2157:\tlearn: 1076658.7114040\ttotal: 3m 4s\tremaining: 1m 13s\n",
            "2158:\tlearn: 1076310.4927708\ttotal: 3m 4s\tremaining: 1m 13s\n",
            "2159:\tlearn: 1075617.7930122\ttotal: 3m 4s\tremaining: 1m 13s\n",
            "2160:\tlearn: 1075260.8432302\ttotal: 3m 4s\tremaining: 1m 12s\n",
            "2161:\tlearn: 1074848.9620139\ttotal: 3m 4s\tremaining: 1m 12s\n",
            "2162:\tlearn: 1073871.1655557\ttotal: 3m 4s\tremaining: 1m 12s\n",
            "2163:\tlearn: 1073526.6561867\ttotal: 3m 4s\tremaining: 1m 12s\n",
            "2164:\tlearn: 1073086.0384857\ttotal: 3m 4s\tremaining: 1m 12s\n",
            "2165:\tlearn: 1072774.9130810\ttotal: 3m 4s\tremaining: 1m 12s\n",
            "2166:\tlearn: 1072319.5061785\ttotal: 3m 4s\tremaining: 1m 12s\n",
            "2167:\tlearn: 1071878.4733935\ttotal: 3m 4s\tremaining: 1m 12s\n",
            "2168:\tlearn: 1071453.5162251\ttotal: 3m 4s\tremaining: 1m 12s\n",
            "2169:\tlearn: 1070996.2732645\ttotal: 3m 4s\tremaining: 1m 11s\n",
            "2170:\tlearn: 1070559.4097291\ttotal: 3m 4s\tremaining: 1m 11s\n",
            "2171:\tlearn: 1070113.1335601\ttotal: 3m 4s\tremaining: 1m 11s\n",
            "2172:\tlearn: 1069457.2480290\ttotal: 3m 5s\tremaining: 1m 11s\n",
            "2173:\tlearn: 1069184.1000855\ttotal: 3m 5s\tremaining: 1m 11s\n",
            "2174:\tlearn: 1068797.0369316\ttotal: 3m 5s\tremaining: 1m 11s\n",
            "2175:\tlearn: 1068210.8665679\ttotal: 3m 5s\tremaining: 1m 11s\n",
            "2176:\tlearn: 1067793.2218451\ttotal: 3m 5s\tremaining: 1m 11s\n",
            "2177:\tlearn: 1067237.9687625\ttotal: 3m 5s\tremaining: 1m 11s\n",
            "2178:\tlearn: 1067032.9411210\ttotal: 3m 7s\tremaining: 1m 12s\n",
            "2179:\tlearn: 1066753.1468506\ttotal: 3m 7s\tremaining: 1m 11s\n",
            "2180:\tlearn: 1066309.8744847\ttotal: 3m 7s\tremaining: 1m 11s\n",
            "2181:\tlearn: 1065890.1791550\ttotal: 3m 7s\tremaining: 1m 11s\n",
            "2182:\tlearn: 1065646.1941609\ttotal: 3m 7s\tremaining: 1m 11s\n",
            "2183:\tlearn: 1065069.9182763\ttotal: 3m 7s\tremaining: 1m 11s\n",
            "2184:\tlearn: 1064659.7503566\ttotal: 3m 8s\tremaining: 1m 11s\n",
            "2185:\tlearn: 1064224.9838363\ttotal: 3m 8s\tremaining: 1m 11s\n",
            "2186:\tlearn: 1063718.6172267\ttotal: 3m 8s\tremaining: 1m 11s\n",
            "2187:\tlearn: 1063389.1420308\ttotal: 3m 8s\tremaining: 1m 11s\n",
            "2188:\tlearn: 1062998.3840284\ttotal: 3m 8s\tremaining: 1m 10s\n",
            "2189:\tlearn: 1062224.6713307\ttotal: 3m 8s\tremaining: 1m 10s\n",
            "2190:\tlearn: 1061953.5185381\ttotal: 3m 8s\tremaining: 1m 10s\n",
            "2191:\tlearn: 1061483.8467278\ttotal: 3m 8s\tremaining: 1m 10s\n",
            "2192:\tlearn: 1061134.7724520\ttotal: 3m 8s\tremaining: 1m 10s\n",
            "2193:\tlearn: 1060984.2269013\ttotal: 3m 8s\tremaining: 1m 10s\n",
            "2194:\tlearn: 1060629.9957948\ttotal: 3m 8s\tremaining: 1m 10s\n",
            "2195:\tlearn: 1060050.5734878\ttotal: 3m 8s\tremaining: 1m 10s\n",
            "2196:\tlearn: 1059694.6822105\ttotal: 3m 8s\tremaining: 1m 10s\n",
            "2197:\tlearn: 1059363.1401039\ttotal: 3m 8s\tremaining: 1m 10s\n",
            "2198:\tlearn: 1058877.6520187\ttotal: 3m 8s\tremaining: 1m 9s\n",
            "2199:\tlearn: 1058461.9485949\ttotal: 3m 8s\tremaining: 1m 9s\n",
            "2200:\tlearn: 1057842.2187720\ttotal: 3m 8s\tremaining: 1m 9s\n",
            "2201:\tlearn: 1057539.5368078\ttotal: 3m 8s\tremaining: 1m 9s\n",
            "2202:\tlearn: 1057271.8118428\ttotal: 3m 8s\tremaining: 1m 9s\n",
            "2203:\tlearn: 1056678.6879261\ttotal: 3m 8s\tremaining: 1m 9s\n",
            "2204:\tlearn: 1056337.9652446\ttotal: 3m 8s\tremaining: 1m 9s\n",
            "2205:\tlearn: 1055980.1383969\ttotal: 3m 8s\tremaining: 1m 9s\n",
            "2206:\tlearn: 1055563.1627853\ttotal: 3m 8s\tremaining: 1m 9s\n",
            "2207:\tlearn: 1055309.9370210\ttotal: 3m 8s\tremaining: 1m 8s\n",
            "2208:\tlearn: 1054921.3635279\ttotal: 3m 8s\tremaining: 1m 8s\n",
            "2209:\tlearn: 1054532.7190915\ttotal: 3m 8s\tremaining: 1m 8s\n",
            "2210:\tlearn: 1054150.6121073\ttotal: 3m 9s\tremaining: 1m 8s\n",
            "2211:\tlearn: 1053926.6117210\ttotal: 3m 9s\tremaining: 1m 8s\n",
            "2212:\tlearn: 1053214.4855768\ttotal: 3m 9s\tremaining: 1m 8s\n",
            "2213:\tlearn: 1052834.6541035\ttotal: 3m 9s\tremaining: 1m 8s\n",
            "2214:\tlearn: 1052039.7592352\ttotal: 3m 9s\tremaining: 1m 8s\n",
            "2215:\tlearn: 1051564.9933272\ttotal: 3m 9s\tremaining: 1m 8s\n",
            "2216:\tlearn: 1051467.9117487\ttotal: 3m 9s\tremaining: 1m 8s\n",
            "2217:\tlearn: 1051028.7128801\ttotal: 3m 9s\tremaining: 1m 7s\n",
            "2218:\tlearn: 1050557.7425543\ttotal: 3m 9s\tremaining: 1m 7s\n",
            "2219:\tlearn: 1049998.9940932\ttotal: 3m 9s\tremaining: 1m 7s\n",
            "2220:\tlearn: 1049588.6180015\ttotal: 3m 9s\tremaining: 1m 7s\n",
            "2221:\tlearn: 1049229.4499801\ttotal: 3m 9s\tremaining: 1m 7s\n",
            "2222:\tlearn: 1048589.2897669\ttotal: 3m 9s\tremaining: 1m 7s\n",
            "2223:\tlearn: 1048096.9129709\ttotal: 3m 9s\tremaining: 1m 7s\n",
            "2224:\tlearn: 1047576.1271029\ttotal: 3m 9s\tremaining: 1m 7s\n",
            "2225:\tlearn: 1046880.2842301\ttotal: 3m 9s\tremaining: 1m 7s\n",
            "2226:\tlearn: 1046354.0296671\ttotal: 3m 9s\tremaining: 1m 6s\n",
            "2227:\tlearn: 1045987.4387991\ttotal: 3m 9s\tremaining: 1m 6s\n",
            "2228:\tlearn: 1045433.5375835\ttotal: 3m 9s\tremaining: 1m 6s\n",
            "2229:\tlearn: 1045086.1431855\ttotal: 3m 9s\tremaining: 1m 6s\n",
            "2230:\tlearn: 1044837.5888546\ttotal: 3m 9s\tremaining: 1m 6s\n",
            "2231:\tlearn: 1044290.8128557\ttotal: 3m 9s\tremaining: 1m 6s\n",
            "2232:\tlearn: 1043725.3417138\ttotal: 3m 9s\tremaining: 1m 6s\n",
            "2233:\tlearn: 1043366.0080119\ttotal: 3m 9s\tremaining: 1m 6s\n",
            "2234:\tlearn: 1042870.1588452\ttotal: 3m 9s\tremaining: 1m 6s\n",
            "2235:\tlearn: 1042586.5915829\ttotal: 3m 9s\tremaining: 1m 6s\n",
            "2236:\tlearn: 1042071.2303780\ttotal: 3m 10s\tremaining: 1m 5s\n",
            "2237:\tlearn: 1041668.4200519\ttotal: 3m 10s\tremaining: 1m 5s\n",
            "2238:\tlearn: 1041186.8961801\ttotal: 3m 10s\tremaining: 1m 5s\n",
            "2239:\tlearn: 1040455.7483928\ttotal: 3m 10s\tremaining: 1m 5s\n",
            "2240:\tlearn: 1039970.6344238\ttotal: 3m 10s\tremaining: 1m 5s\n",
            "2241:\tlearn: 1039786.4317061\ttotal: 3m 10s\tremaining: 1m 5s\n",
            "2242:\tlearn: 1039530.1723808\ttotal: 3m 10s\tremaining: 1m 5s\n",
            "2243:\tlearn: 1039164.1173443\ttotal: 3m 10s\tremaining: 1m 5s\n",
            "2244:\tlearn: 1038898.1813634\ttotal: 3m 10s\tremaining: 1m 5s\n",
            "2245:\tlearn: 1038447.2146622\ttotal: 3m 10s\tremaining: 1m 5s\n",
            "2246:\tlearn: 1038113.7446792\ttotal: 3m 10s\tremaining: 1m 4s\n",
            "2247:\tlearn: 1037775.4858207\ttotal: 3m 10s\tremaining: 1m 4s\n",
            "2248:\tlearn: 1037512.3585483\ttotal: 3m 10s\tremaining: 1m 4s\n",
            "2249:\tlearn: 1037194.2021598\ttotal: 3m 10s\tremaining: 1m 4s\n",
            "2250:\tlearn: 1036526.0421172\ttotal: 3m 10s\tremaining: 1m 4s\n",
            "2251:\tlearn: 1036144.3931049\ttotal: 3m 10s\tremaining: 1m 4s\n",
            "2252:\tlearn: 1035942.9377469\ttotal: 3m 10s\tremaining: 1m 4s\n",
            "2253:\tlearn: 1035508.5393785\ttotal: 3m 10s\tremaining: 1m 4s\n",
            "2254:\tlearn: 1035330.9157979\ttotal: 3m 10s\tremaining: 1m 4s\n",
            "2255:\tlearn: 1034820.2919968\ttotal: 3m 10s\tremaining: 1m 4s\n",
            "2256:\tlearn: 1034399.0084057\ttotal: 3m 10s\tremaining: 1m 3s\n",
            "2257:\tlearn: 1033934.7351622\ttotal: 3m 10s\tremaining: 1m 3s\n",
            "2258:\tlearn: 1033700.8672710\ttotal: 3m 10s\tremaining: 1m 3s\n",
            "2259:\tlearn: 1033188.2988335\ttotal: 3m 10s\tremaining: 1m 3s\n",
            "2260:\tlearn: 1032755.9020974\ttotal: 3m 10s\tremaining: 1m 3s\n",
            "2261:\tlearn: 1032204.7472619\ttotal: 3m 10s\tremaining: 1m 3s\n",
            "2262:\tlearn: 1031960.5288484\ttotal: 3m 10s\tremaining: 1m 3s\n",
            "2263:\tlearn: 1031673.0319284\ttotal: 3m 11s\tremaining: 1m 3s\n",
            "2264:\tlearn: 1030603.8022410\ttotal: 3m 11s\tremaining: 1m 3s\n",
            "2265:\tlearn: 1030247.0930442\ttotal: 3m 11s\tremaining: 1m 2s\n",
            "2266:\tlearn: 1030025.3661041\ttotal: 3m 11s\tremaining: 1m 2s\n",
            "2267:\tlearn: 1029801.5306165\ttotal: 3m 11s\tremaining: 1m 2s\n",
            "2268:\tlearn: 1029426.6061940\ttotal: 3m 11s\tremaining: 1m 2s\n",
            "2269:\tlearn: 1028873.4039759\ttotal: 3m 11s\tremaining: 1m 2s\n",
            "2270:\tlearn: 1028512.2432712\ttotal: 3m 11s\tremaining: 1m 2s\n",
            "2271:\tlearn: 1028150.0869284\ttotal: 3m 11s\tremaining: 1m 2s\n",
            "2272:\tlearn: 1027928.5414086\ttotal: 3m 11s\tremaining: 1m 2s\n",
            "2273:\tlearn: 1027705.5884866\ttotal: 3m 11s\tremaining: 1m 2s\n",
            "2274:\tlearn: 1027538.4589527\ttotal: 3m 11s\tremaining: 1m 2s\n",
            "2275:\tlearn: 1027230.4188053\ttotal: 3m 11s\tremaining: 1m 1s\n",
            "2276:\tlearn: 1026836.9266413\ttotal: 3m 11s\tremaining: 1m 1s\n",
            "2277:\tlearn: 1026564.6744519\ttotal: 3m 11s\tremaining: 1m 1s\n",
            "2278:\tlearn: 1026028.4140465\ttotal: 3m 11s\tremaining: 1m 1s\n",
            "2279:\tlearn: 1025701.0721959\ttotal: 3m 11s\tremaining: 1m 1s\n",
            "2280:\tlearn: 1025172.7415718\ttotal: 3m 11s\tremaining: 1m 1s\n",
            "2281:\tlearn: 1024925.6354802\ttotal: 3m 11s\tremaining: 1m 1s\n",
            "2282:\tlearn: 1024549.3162923\ttotal: 3m 11s\tremaining: 1m 1s\n",
            "2283:\tlearn: 1024086.7629528\ttotal: 3m 11s\tremaining: 1m 1s\n",
            "2284:\tlearn: 1023750.8589247\ttotal: 3m 11s\tremaining: 1m 1s\n",
            "2285:\tlearn: 1023419.4038612\ttotal: 3m 11s\tremaining: 1m\n",
            "2286:\tlearn: 1023028.0994812\ttotal: 3m 11s\tremaining: 1m\n",
            "2287:\tlearn: 1022617.6183819\ttotal: 3m 11s\tremaining: 1m\n",
            "2288:\tlearn: 1022173.4167391\ttotal: 3m 11s\tremaining: 1m\n",
            "2289:\tlearn: 1021784.8907980\ttotal: 3m 12s\tremaining: 1m\n",
            "2290:\tlearn: 1021234.8252573\ttotal: 3m 12s\tremaining: 1m\n",
            "2291:\tlearn: 1020844.8500922\ttotal: 3m 12s\tremaining: 1m\n",
            "2292:\tlearn: 1020537.5621377\ttotal: 3m 12s\tremaining: 1m\n",
            "2293:\tlearn: 1020105.6937237\ttotal: 3m 12s\tremaining: 1m\n",
            "2294:\tlearn: 1019830.7910585\ttotal: 3m 12s\tremaining: 1m\n",
            "2295:\tlearn: 1019549.0192858\ttotal: 3m 12s\tremaining: 60s\n",
            "2296:\tlearn: 1018995.6272026\ttotal: 3m 12s\tremaining: 59.9s\n",
            "2297:\tlearn: 1018800.0735130\ttotal: 3m 12s\tremaining: 59.8s\n",
            "2298:\tlearn: 1018555.5722234\ttotal: 3m 12s\tremaining: 59.7s\n",
            "2299:\tlearn: 1018008.4942603\ttotal: 3m 12s\tremaining: 59.6s\n",
            "2300:\tlearn: 1017766.6385020\ttotal: 3m 12s\tremaining: 59.5s\n",
            "2301:\tlearn: 1017559.6258624\ttotal: 3m 12s\tremaining: 59.4s\n",
            "2302:\tlearn: 1017150.9874108\ttotal: 3m 12s\tremaining: 59.3s\n",
            "2303:\tlearn: 1016588.4748266\ttotal: 3m 12s\tremaining: 59.2s\n",
            "2304:\tlearn: 1016268.5009330\ttotal: 3m 12s\tremaining: 59.1s\n",
            "2305:\tlearn: 1015962.0353291\ttotal: 3m 12s\tremaining: 59s\n",
            "2306:\tlearn: 1015537.9758030\ttotal: 3m 12s\tremaining: 58.9s\n",
            "2307:\tlearn: 1014916.2849780\ttotal: 3m 12s\tremaining: 58.8s\n",
            "2308:\tlearn: 1014276.4112079\ttotal: 3m 12s\tremaining: 58.7s\n",
            "2309:\tlearn: 1013689.8040765\ttotal: 3m 12s\tremaining: 58.6s\n",
            "2310:\tlearn: 1013234.6347717\ttotal: 3m 12s\tremaining: 58.5s\n",
            "2311:\tlearn: 1012306.8156709\ttotal: 3m 12s\tremaining: 58.4s\n",
            "2312:\tlearn: 1011991.7560186\ttotal: 3m 12s\tremaining: 58.3s\n",
            "2313:\tlearn: 1011738.8112968\ttotal: 3m 12s\tremaining: 58.2s\n",
            "2314:\tlearn: 1011326.8161658\ttotal: 3m 12s\tremaining: 58.1s\n",
            "2315:\tlearn: 1010601.0278758\ttotal: 3m 12s\tremaining: 58s\n",
            "2316:\tlearn: 1010030.6597361\ttotal: 3m 13s\tremaining: 57.9s\n",
            "2317:\tlearn: 1009632.4769706\ttotal: 3m 13s\tremaining: 57.8s\n",
            "2318:\tlearn: 1009343.2336438\ttotal: 3m 13s\tremaining: 57.7s\n",
            "2319:\tlearn: 1009078.5160853\ttotal: 3m 13s\tremaining: 57.6s\n",
            "2320:\tlearn: 1008751.7592522\ttotal: 3m 13s\tremaining: 57.5s\n",
            "2321:\tlearn: 1008226.3900210\ttotal: 3m 13s\tremaining: 57.4s\n",
            "2322:\tlearn: 1007516.7732910\ttotal: 3m 13s\tremaining: 57.3s\n",
            "2323:\tlearn: 1007291.2000671\ttotal: 3m 13s\tremaining: 57.2s\n",
            "2324:\tlearn: 1006764.1578712\ttotal: 3m 13s\tremaining: 57.1s\n",
            "2325:\tlearn: 1006219.6767402\ttotal: 3m 13s\tremaining: 57s\n",
            "2326:\tlearn: 1005946.0214637\ttotal: 3m 13s\tremaining: 56.9s\n",
            "2327:\tlearn: 1005472.1132769\ttotal: 3m 13s\tremaining: 56.8s\n",
            "2328:\tlearn: 1004865.7501868\ttotal: 3m 13s\tremaining: 56.7s\n",
            "2329:\tlearn: 1004555.3580796\ttotal: 3m 13s\tremaining: 56.6s\n",
            "2330:\tlearn: 1004026.3887673\ttotal: 3m 13s\tremaining: 56.5s\n",
            "2331:\tlearn: 1003884.9559628\ttotal: 3m 13s\tremaining: 56.4s\n",
            "2332:\tlearn: 1003530.8393284\ttotal: 3m 13s\tremaining: 56.3s\n",
            "2333:\tlearn: 1003189.9807425\ttotal: 3m 13s\tremaining: 56.2s\n",
            "2334:\tlearn: 1002992.6733831\ttotal: 3m 13s\tremaining: 56.1s\n",
            "2335:\tlearn: 1002642.7743458\ttotal: 3m 13s\tremaining: 56s\n",
            "2336:\tlearn: 1002383.7527880\ttotal: 3m 13s\tremaining: 55.9s\n",
            "2337:\tlearn: 1001964.0819755\ttotal: 3m 13s\tremaining: 55.8s\n",
            "2338:\tlearn: 1001442.3577311\ttotal: 3m 13s\tremaining: 55.7s\n",
            "2339:\tlearn: 1001066.9556271\ttotal: 3m 13s\tremaining: 55.6s\n",
            "2340:\tlearn: 1000686.9076858\ttotal: 3m 13s\tremaining: 55.5s\n",
            "2341:\tlearn: 1000331.0741666\ttotal: 3m 13s\tremaining: 55.4s\n",
            "2342:\tlearn: 999907.6285448\ttotal: 3m 14s\tremaining: 55.3s\n",
            "2343:\tlearn: 999475.7329853\ttotal: 3m 14s\tremaining: 55.2s\n",
            "2344:\tlearn: 999047.1628363\ttotal: 3m 14s\tremaining: 55.1s\n",
            "2345:\tlearn: 998590.2361769\ttotal: 3m 14s\tremaining: 55s\n",
            "2346:\tlearn: 998163.0736534\ttotal: 3m 14s\tremaining: 54.9s\n",
            "2347:\tlearn: 997877.7507415\ttotal: 3m 14s\tremaining: 54.8s\n",
            "2348:\tlearn: 997545.4175625\ttotal: 3m 14s\tremaining: 54.7s\n",
            "2349:\tlearn: 997291.6933514\ttotal: 3m 14s\tremaining: 54.6s\n",
            "2350:\tlearn: 996945.5838349\ttotal: 3m 14s\tremaining: 54.5s\n",
            "2351:\tlearn: 996503.3900114\ttotal: 3m 14s\tremaining: 54.4s\n",
            "2352:\tlearn: 996083.0900747\ttotal: 3m 14s\tremaining: 54.4s\n",
            "2353:\tlearn: 995658.9685317\ttotal: 3m 14s\tremaining: 54.3s\n",
            "2354:\tlearn: 995372.9729342\ttotal: 3m 14s\tremaining: 54.2s\n",
            "2355:\tlearn: 994621.6528152\ttotal: 3m 14s\tremaining: 54.1s\n",
            "2356:\tlearn: 994074.0602671\ttotal: 3m 14s\tremaining: 54s\n",
            "2357:\tlearn: 993867.7614356\ttotal: 3m 14s\tremaining: 53.9s\n",
            "2358:\tlearn: 993178.0727487\ttotal: 3m 14s\tremaining: 53.8s\n",
            "2359:\tlearn: 992358.5486248\ttotal: 3m 14s\tremaining: 53.7s\n",
            "2360:\tlearn: 992029.7295588\ttotal: 3m 14s\tremaining: 53.6s\n",
            "2361:\tlearn: 991634.5572876\ttotal: 3m 14s\tremaining: 53.5s\n",
            "2362:\tlearn: 991079.3119038\ttotal: 3m 14s\tremaining: 53.4s\n",
            "2363:\tlearn: 990565.6180971\ttotal: 3m 14s\tremaining: 53.3s\n",
            "2364:\tlearn: 990260.6145352\ttotal: 3m 14s\tremaining: 53.2s\n",
            "2365:\tlearn: 989701.6212775\ttotal: 3m 14s\tremaining: 53.1s\n",
            "2366:\tlearn: 989365.1470913\ttotal: 3m 14s\tremaining: 53s\n",
            "2367:\tlearn: 988953.0795110\ttotal: 3m 14s\tremaining: 52.9s\n",
            "2368:\tlearn: 988622.6573647\ttotal: 3m 14s\tremaining: 52.8s\n",
            "2369:\tlearn: 988295.4284686\ttotal: 3m 15s\tremaining: 52.7s\n",
            "2370:\tlearn: 987856.2030863\ttotal: 3m 15s\tremaining: 52.6s\n",
            "2371:\tlearn: 987521.7744605\ttotal: 3m 15s\tremaining: 52.5s\n",
            "2372:\tlearn: 987330.7944862\ttotal: 3m 15s\tremaining: 52.4s\n",
            "2373:\tlearn: 987033.0142002\ttotal: 3m 15s\tremaining: 52.3s\n",
            "2374:\tlearn: 986774.3492622\ttotal: 3m 15s\tremaining: 52.2s\n",
            "2375:\tlearn: 986355.5354159\ttotal: 3m 15s\tremaining: 52.1s\n",
            "2376:\tlearn: 985814.2201622\ttotal: 3m 15s\tremaining: 52s\n",
            "2377:\tlearn: 985657.0217078\ttotal: 3m 15s\tremaining: 51.9s\n",
            "2378:\tlearn: 985391.3704554\ttotal: 3m 15s\tremaining: 51.8s\n",
            "2379:\tlearn: 985048.3349044\ttotal: 3m 15s\tremaining: 51.8s\n",
            "2380:\tlearn: 984677.3932768\ttotal: 3m 15s\tremaining: 51.7s\n",
            "2381:\tlearn: 984243.9330371\ttotal: 3m 15s\tremaining: 51.6s\n",
            "2382:\tlearn: 983996.0483947\ttotal: 3m 15s\tremaining: 51.5s\n",
            "2383:\tlearn: 983543.7458433\ttotal: 3m 15s\tremaining: 51.4s\n",
            "2384:\tlearn: 983169.0006296\ttotal: 3m 15s\tremaining: 51.3s\n",
            "2385:\tlearn: 982614.5977492\ttotal: 3m 15s\tremaining: 51.2s\n",
            "2386:\tlearn: 982186.1735342\ttotal: 3m 15s\tremaining: 51.1s\n",
            "2387:\tlearn: 981910.2142283\ttotal: 3m 15s\tremaining: 51s\n",
            "2388:\tlearn: 981405.0778695\ttotal: 3m 15s\tremaining: 50.9s\n",
            "2389:\tlearn: 981051.8110579\ttotal: 3m 15s\tremaining: 50.8s\n",
            "2390:\tlearn: 980883.6498595\ttotal: 3m 15s\tremaining: 50.7s\n",
            "2391:\tlearn: 980462.0423984\ttotal: 3m 15s\tremaining: 50.6s\n",
            "2392:\tlearn: 980025.4406703\ttotal: 3m 15s\tremaining: 50.5s\n",
            "2393:\tlearn: 979749.3682641\ttotal: 3m 15s\tremaining: 50.4s\n",
            "2394:\tlearn: 979455.3948167\ttotal: 3m 15s\tremaining: 50.3s\n",
            "2395:\tlearn: 979120.5002472\ttotal: 3m 15s\tremaining: 50.2s\n",
            "2396:\tlearn: 978643.8055427\ttotal: 3m 16s\tremaining: 50.1s\n",
            "2397:\tlearn: 978317.0283108\ttotal: 3m 16s\tremaining: 50s\n",
            "2398:\tlearn: 977837.0475888\ttotal: 3m 16s\tremaining: 49.9s\n",
            "2399:\tlearn: 977562.6102520\ttotal: 3m 16s\tremaining: 49.8s\n",
            "2400:\tlearn: 977232.2511082\ttotal: 3m 16s\tremaining: 49.8s\n",
            "2401:\tlearn: 976874.6386761\ttotal: 3m 16s\tremaining: 49.8s\n",
            "2402:\tlearn: 976512.4362204\ttotal: 3m 18s\tremaining: 50.2s\n",
            "2403:\tlearn: 976004.9512515\ttotal: 3m 18s\tremaining: 50.1s\n",
            "2404:\tlearn: 975507.7155025\ttotal: 3m 18s\tremaining: 50s\n",
            "2405:\tlearn: 975227.4173763\ttotal: 3m 18s\tremaining: 50s\n",
            "2406:\tlearn: 974947.3474116\ttotal: 3m 19s\tremaining: 49.9s\n",
            "2407:\tlearn: 974562.4018668\ttotal: 3m 19s\tremaining: 49.8s\n",
            "2408:\tlearn: 974152.6860817\ttotal: 3m 19s\tremaining: 49.7s\n",
            "2409:\tlearn: 973813.7541373\ttotal: 3m 19s\tremaining: 49.6s\n",
            "2410:\tlearn: 973661.1270279\ttotal: 3m 19s\tremaining: 49.5s\n",
            "2411:\tlearn: 973164.7861417\ttotal: 3m 19s\tremaining: 49.4s\n",
            "2412:\tlearn: 972883.9186712\ttotal: 3m 19s\tremaining: 49.3s\n",
            "2413:\tlearn: 972353.3876065\ttotal: 3m 19s\tremaining: 49.2s\n",
            "2414:\tlearn: 972011.1600539\ttotal: 3m 19s\tremaining: 49.1s\n",
            "2415:\tlearn: 971669.8659639\ttotal: 3m 19s\tremaining: 49s\n",
            "2416:\tlearn: 971220.8639350\ttotal: 3m 19s\tremaining: 48.9s\n",
            "2417:\tlearn: 970314.1821641\ttotal: 3m 19s\tremaining: 48.8s\n",
            "2418:\tlearn: 970006.9607022\ttotal: 3m 19s\tremaining: 48.7s\n",
            "2419:\tlearn: 969561.3707992\ttotal: 3m 19s\tremaining: 48.6s\n",
            "2420:\tlearn: 969198.7128206\ttotal: 3m 19s\tremaining: 48.5s\n",
            "2421:\tlearn: 968943.2718417\ttotal: 3m 19s\tremaining: 48.4s\n",
            "2422:\tlearn: 968595.3140629\ttotal: 3m 19s\tremaining: 48.3s\n",
            "2423:\tlearn: 968339.7511619\ttotal: 3m 19s\tremaining: 48.2s\n",
            "2424:\tlearn: 968008.8311140\ttotal: 3m 19s\tremaining: 48.1s\n",
            "2425:\tlearn: 967826.6226849\ttotal: 3m 19s\tremaining: 48s\n",
            "2426:\tlearn: 967126.4021573\ttotal: 3m 19s\tremaining: 47.9s\n",
            "2427:\tlearn: 966763.6314194\ttotal: 3m 19s\tremaining: 47.9s\n",
            "2428:\tlearn: 966174.4871023\ttotal: 3m 19s\tremaining: 47.8s\n",
            "2429:\tlearn: 965813.7667566\ttotal: 3m 19s\tremaining: 47.7s\n",
            "2430:\tlearn: 965385.6733958\ttotal: 3m 19s\tremaining: 47.6s\n",
            "2431:\tlearn: 965209.9410028\ttotal: 3m 19s\tremaining: 47.5s\n",
            "2432:\tlearn: 965057.1793276\ttotal: 3m 19s\tremaining: 47.4s\n",
            "2433:\tlearn: 964614.2445368\ttotal: 3m 20s\tremaining: 47.3s\n",
            "2434:\tlearn: 964332.5792838\ttotal: 3m 20s\tremaining: 47.2s\n",
            "2435:\tlearn: 964012.7321049\ttotal: 3m 20s\tremaining: 47.1s\n",
            "2436:\tlearn: 963638.7496820\ttotal: 3m 20s\tremaining: 47s\n",
            "2437:\tlearn: 963177.5399669\ttotal: 3m 20s\tremaining: 46.9s\n",
            "2438:\tlearn: 962650.2021695\ttotal: 3m 20s\tremaining: 46.8s\n",
            "2439:\tlearn: 961925.5536625\ttotal: 3m 20s\tremaining: 46.7s\n",
            "2440:\tlearn: 961484.4799004\ttotal: 3m 20s\tremaining: 46.6s\n",
            "2441:\tlearn: 961061.2355606\ttotal: 3m 20s\tremaining: 46.5s\n",
            "2442:\tlearn: 960742.7618894\ttotal: 3m 20s\tremaining: 46.4s\n",
            "2443:\tlearn: 960422.0357969\ttotal: 3m 20s\tremaining: 46.3s\n",
            "2444:\tlearn: 959992.5958538\ttotal: 3m 20s\tremaining: 46.2s\n",
            "2445:\tlearn: 959700.3525909\ttotal: 3m 20s\tremaining: 46.2s\n",
            "2446:\tlearn: 959521.8311185\ttotal: 3m 20s\tremaining: 46.1s\n",
            "2447:\tlearn: 959236.2444957\ttotal: 3m 20s\tremaining: 46s\n",
            "2448:\tlearn: 958993.7150748\ttotal: 3m 20s\tremaining: 45.9s\n",
            "2449:\tlearn: 958770.7681287\ttotal: 3m 20s\tremaining: 45.8s\n",
            "2450:\tlearn: 958578.1854882\ttotal: 3m 20s\tremaining: 45.7s\n",
            "2451:\tlearn: 958112.9931710\ttotal: 3m 20s\tremaining: 45.6s\n",
            "2452:\tlearn: 957611.2070496\ttotal: 3m 20s\tremaining: 45.5s\n",
            "2453:\tlearn: 957225.5873344\ttotal: 3m 20s\tremaining: 45.4s\n",
            "2454:\tlearn: 956931.4826149\ttotal: 3m 20s\tremaining: 45.3s\n",
            "2455:\tlearn: 956593.2293412\ttotal: 3m 20s\tremaining: 45.2s\n",
            "2456:\tlearn: 956343.1569858\ttotal: 3m 20s\tremaining: 45.1s\n",
            "2457:\tlearn: 955965.6913615\ttotal: 3m 20s\tremaining: 45s\n",
            "2458:\tlearn: 955499.0944745\ttotal: 3m 20s\tremaining: 44.9s\n",
            "2459:\tlearn: 955168.5806858\ttotal: 3m 20s\tremaining: 44.8s\n",
            "2460:\tlearn: 954618.5652738\ttotal: 3m 21s\tremaining: 44.8s\n",
            "2461:\tlearn: 954207.7545250\ttotal: 3m 21s\tremaining: 44.7s\n",
            "2462:\tlearn: 953735.1305732\ttotal: 3m 21s\tremaining: 44.6s\n",
            "2463:\tlearn: 953365.3267658\ttotal: 3m 21s\tremaining: 44.5s\n",
            "2464:\tlearn: 952899.8901364\ttotal: 3m 21s\tremaining: 44.4s\n",
            "2465:\tlearn: 952382.7371635\ttotal: 3m 21s\tremaining: 44.3s\n",
            "2466:\tlearn: 951954.2884644\ttotal: 3m 21s\tremaining: 44.2s\n",
            "2467:\tlearn: 951625.4527824\ttotal: 3m 21s\tremaining: 44.1s\n",
            "2468:\tlearn: 951102.8733015\ttotal: 3m 21s\tremaining: 44s\n",
            "2469:\tlearn: 950850.1414225\ttotal: 3m 21s\tremaining: 43.9s\n",
            "2470:\tlearn: 950381.0843708\ttotal: 3m 21s\tremaining: 43.8s\n",
            "2471:\tlearn: 949825.8071398\ttotal: 3m 21s\tremaining: 43.7s\n",
            "2472:\tlearn: 949654.0125436\ttotal: 3m 21s\tremaining: 43.6s\n",
            "2473:\tlearn: 948924.1023901\ttotal: 3m 21s\tremaining: 43.5s\n",
            "2474:\tlearn: 948637.5857897\ttotal: 3m 21s\tremaining: 43.5s\n",
            "2475:\tlearn: 948374.0032765\ttotal: 3m 21s\tremaining: 43.4s\n",
            "2476:\tlearn: 948080.9118138\ttotal: 3m 21s\tremaining: 43.3s\n",
            "2477:\tlearn: 947716.0769690\ttotal: 3m 21s\tremaining: 43.2s\n",
            "2478:\tlearn: 947438.3357848\ttotal: 3m 21s\tremaining: 43.1s\n",
            "2479:\tlearn: 947090.9155664\ttotal: 3m 21s\tremaining: 43s\n",
            "2480:\tlearn: 946699.5661504\ttotal: 3m 21s\tremaining: 42.9s\n",
            "2481:\tlearn: 946436.9977423\ttotal: 3m 21s\tremaining: 42.8s\n",
            "2482:\tlearn: 946143.8326898\ttotal: 3m 21s\tremaining: 42.7s\n",
            "2483:\tlearn: 945819.1280916\ttotal: 3m 21s\tremaining: 42.6s\n",
            "2484:\tlearn: 945451.8580240\ttotal: 3m 21s\tremaining: 42.5s\n",
            "2485:\tlearn: 945238.9715878\ttotal: 3m 21s\tremaining: 42.4s\n",
            "2486:\tlearn: 945099.2753575\ttotal: 3m 22s\tremaining: 42.4s\n",
            "2487:\tlearn: 944734.9669848\ttotal: 3m 22s\tremaining: 42.3s\n",
            "2488:\tlearn: 944275.2986545\ttotal: 3m 22s\tremaining: 42.2s\n",
            "2489:\tlearn: 943762.1747803\ttotal: 3m 22s\tremaining: 42.1s\n",
            "2490:\tlearn: 943403.3554871\ttotal: 3m 22s\tremaining: 42s\n",
            "2491:\tlearn: 943106.7700309\ttotal: 3m 22s\tremaining: 41.9s\n",
            "2492:\tlearn: 942767.1736571\ttotal: 3m 22s\tremaining: 41.8s\n",
            "2493:\tlearn: 942385.9756301\ttotal: 3m 22s\tremaining: 41.7s\n",
            "2494:\tlearn: 941971.5238906\ttotal: 3m 22s\tremaining: 41.6s\n",
            "2495:\tlearn: 941797.6641270\ttotal: 3m 22s\tremaining: 41.5s\n",
            "2496:\tlearn: 941409.4516413\ttotal: 3m 22s\tremaining: 41.4s\n",
            "2497:\tlearn: 941192.1450881\ttotal: 3m 22s\tremaining: 41.3s\n",
            "2498:\tlearn: 941026.0371727\ttotal: 3m 22s\tremaining: 41.3s\n",
            "2499:\tlearn: 940779.4585846\ttotal: 3m 22s\tremaining: 41.2s\n",
            "2500:\tlearn: 940598.2526148\ttotal: 3m 22s\tremaining: 41.1s\n",
            "2501:\tlearn: 940303.0408919\ttotal: 3m 22s\tremaining: 41s\n",
            "2502:\tlearn: 939954.8709311\ttotal: 3m 22s\tremaining: 40.9s\n",
            "2503:\tlearn: 939555.5665761\ttotal: 3m 22s\tremaining: 40.8s\n",
            "2504:\tlearn: 939014.8856994\ttotal: 3m 22s\tremaining: 40.7s\n",
            "2505:\tlearn: 938684.7815620\ttotal: 3m 22s\tremaining: 40.6s\n",
            "2506:\tlearn: 938436.0883841\ttotal: 3m 22s\tremaining: 40.5s\n",
            "2507:\tlearn: 938113.4726023\ttotal: 3m 22s\tremaining: 40.4s\n",
            "2508:\tlearn: 937543.9686779\ttotal: 3m 22s\tremaining: 40.3s\n",
            "2509:\tlearn: 937207.6058274\ttotal: 3m 22s\tremaining: 40.2s\n",
            "2510:\tlearn: 936743.3441570\ttotal: 3m 22s\tremaining: 40.2s\n",
            "2511:\tlearn: 936157.3456421\ttotal: 3m 22s\tremaining: 40.1s\n",
            "2512:\tlearn: 935948.2054517\ttotal: 3m 22s\tremaining: 40s\n",
            "2513:\tlearn: 935574.3918061\ttotal: 3m 23s\tremaining: 39.9s\n",
            "2514:\tlearn: 935070.0057135\ttotal: 3m 23s\tremaining: 39.8s\n",
            "2515:\tlearn: 934776.9428307\ttotal: 3m 23s\tremaining: 39.7s\n",
            "2516:\tlearn: 934420.4977013\ttotal: 3m 23s\tremaining: 39.6s\n",
            "2517:\tlearn: 934019.1853155\ttotal: 3m 23s\tremaining: 39.5s\n",
            "2518:\tlearn: 933671.1426675\ttotal: 3m 23s\tremaining: 39.4s\n",
            "2519:\tlearn: 933210.7794685\ttotal: 3m 23s\tremaining: 39.4s\n",
            "2520:\tlearn: 932923.1278697\ttotal: 3m 23s\tremaining: 39.3s\n",
            "2521:\tlearn: 932479.9629954\ttotal: 3m 23s\tremaining: 39.2s\n",
            "2522:\tlearn: 931956.6220362\ttotal: 3m 23s\tremaining: 39.1s\n",
            "2523:\tlearn: 931567.4322623\ttotal: 3m 23s\tremaining: 39s\n",
            "2524:\tlearn: 931112.4081859\ttotal: 3m 23s\tremaining: 38.9s\n",
            "2525:\tlearn: 930688.4784107\ttotal: 3m 23s\tremaining: 38.8s\n",
            "2526:\tlearn: 930470.0321128\ttotal: 3m 23s\tremaining: 38.7s\n",
            "2527:\tlearn: 930113.9497682\ttotal: 3m 23s\tremaining: 38.6s\n",
            "2528:\tlearn: 929951.5792176\ttotal: 3m 23s\tremaining: 38.5s\n",
            "2529:\tlearn: 929363.4118094\ttotal: 3m 23s\tremaining: 38.4s\n",
            "2530:\tlearn: 929104.8272345\ttotal: 3m 23s\tremaining: 38.4s\n",
            "2531:\tlearn: 928656.4188180\ttotal: 3m 23s\tremaining: 38.3s\n",
            "2532:\tlearn: 928490.7919172\ttotal: 3m 26s\tremaining: 38.7s\n",
            "2533:\tlearn: 928226.6532649\ttotal: 3m 26s\tremaining: 38.6s\n",
            "2534:\tlearn: 927985.3945109\ttotal: 3m 26s\tremaining: 38.5s\n",
            "2535:\tlearn: 927649.0365407\ttotal: 3m 26s\tremaining: 38.4s\n",
            "2536:\tlearn: 927193.0236025\ttotal: 3m 26s\tremaining: 38.3s\n",
            "2537:\tlearn: 926999.0295781\ttotal: 3m 26s\tremaining: 38.2s\n",
            "2538:\tlearn: 926285.9445487\ttotal: 3m 26s\tremaining: 38.1s\n",
            "2539:\tlearn: 925863.9068136\ttotal: 3m 26s\tremaining: 38s\n",
            "2540:\tlearn: 925049.7946405\ttotal: 3m 26s\tremaining: 37.9s\n",
            "2541:\tlearn: 924885.9846839\ttotal: 3m 26s\tremaining: 37.8s\n",
            "2542:\tlearn: 924484.3847247\ttotal: 3m 26s\tremaining: 37.8s\n",
            "2543:\tlearn: 924171.3968232\ttotal: 3m 26s\tremaining: 37.7s\n",
            "2544:\tlearn: 923699.4510672\ttotal: 3m 26s\tremaining: 37.6s\n",
            "2545:\tlearn: 923213.1975373\ttotal: 3m 26s\tremaining: 37.5s\n",
            "2546:\tlearn: 922901.3379442\ttotal: 3m 26s\tremaining: 37.4s\n",
            "2547:\tlearn: 922623.6729285\ttotal: 3m 26s\tremaining: 37.3s\n",
            "2548:\tlearn: 922383.1800803\ttotal: 3m 26s\tremaining: 37.2s\n",
            "2549:\tlearn: 921905.6896065\ttotal: 3m 26s\tremaining: 37.1s\n",
            "2550:\tlearn: 921649.5604318\ttotal: 3m 26s\tremaining: 37s\n",
            "2551:\tlearn: 921571.1988382\ttotal: 3m 27s\tremaining: 36.9s\n",
            "2552:\tlearn: 921140.8779369\ttotal: 3m 27s\tremaining: 36.8s\n",
            "2553:\tlearn: 920865.2296026\ttotal: 3m 27s\tremaining: 36.8s\n",
            "2554:\tlearn: 920778.2349536\ttotal: 3m 27s\tremaining: 36.7s\n",
            "2555:\tlearn: 920442.2403260\ttotal: 3m 27s\tremaining: 36.6s\n",
            "2556:\tlearn: 920237.7769131\ttotal: 3m 27s\tremaining: 36.5s\n",
            "2557:\tlearn: 919980.3829685\ttotal: 3m 27s\tremaining: 36.4s\n",
            "2558:\tlearn: 919693.6848585\ttotal: 3m 27s\tremaining: 36.3s\n",
            "2559:\tlearn: 919265.7567674\ttotal: 3m 27s\tremaining: 36.2s\n",
            "2560:\tlearn: 918843.0552425\ttotal: 3m 27s\tremaining: 36.1s\n",
            "2561:\tlearn: 918531.0093689\ttotal: 3m 27s\tremaining: 36s\n",
            "2562:\tlearn: 917886.2498750\ttotal: 3m 27s\tremaining: 35.9s\n",
            "2563:\tlearn: 917712.6087382\ttotal: 3m 27s\tremaining: 35.8s\n",
            "2564:\tlearn: 917536.1115414\ttotal: 3m 27s\tremaining: 35.8s\n",
            "2565:\tlearn: 917267.7513969\ttotal: 3m 27s\tremaining: 35.7s\n",
            "2566:\tlearn: 916756.0948962\ttotal: 3m 27s\tremaining: 35.6s\n",
            "2567:\tlearn: 916302.4729129\ttotal: 3m 27s\tremaining: 35.5s\n",
            "2568:\tlearn: 916121.6526150\ttotal: 3m 27s\tremaining: 35.4s\n",
            "2569:\tlearn: 915773.6735583\ttotal: 3m 27s\tremaining: 35.3s\n",
            "2570:\tlearn: 915309.3321725\ttotal: 3m 27s\tremaining: 35.2s\n",
            "2571:\tlearn: 915117.0498836\ttotal: 3m 27s\tremaining: 35.1s\n",
            "2572:\tlearn: 914682.3754829\ttotal: 3m 27s\tremaining: 35s\n",
            "2573:\tlearn: 913997.3655663\ttotal: 3m 27s\tremaining: 35s\n",
            "2574:\tlearn: 913487.1460092\ttotal: 3m 27s\tremaining: 34.9s\n",
            "2575:\tlearn: 913149.8639798\ttotal: 3m 27s\tremaining: 34.8s\n",
            "2576:\tlearn: 912809.5043480\ttotal: 3m 27s\tremaining: 34.7s\n",
            "2577:\tlearn: 912458.1368829\ttotal: 3m 27s\tremaining: 34.6s\n",
            "2578:\tlearn: 912195.0870331\ttotal: 3m 28s\tremaining: 34.5s\n",
            "2579:\tlearn: 911678.2986983\ttotal: 3m 28s\tremaining: 34.4s\n",
            "2580:\tlearn: 911258.8454379\ttotal: 3m 28s\tremaining: 34.3s\n",
            "2581:\tlearn: 911009.4312016\ttotal: 3m 28s\tremaining: 34.2s\n",
            "2582:\tlearn: 910770.1791756\ttotal: 3m 28s\tremaining: 34.1s\n",
            "2583:\tlearn: 910508.7604497\ttotal: 3m 28s\tremaining: 34.1s\n",
            "2584:\tlearn: 910299.6040066\ttotal: 3m 28s\tremaining: 34s\n",
            "2585:\tlearn: 909817.5648942\ttotal: 3m 28s\tremaining: 33.9s\n",
            "2586:\tlearn: 909617.6347641\ttotal: 3m 28s\tremaining: 33.8s\n",
            "2587:\tlearn: 909175.6921015\ttotal: 3m 28s\tremaining: 33.7s\n",
            "2588:\tlearn: 908990.0933603\ttotal: 3m 28s\tremaining: 33.6s\n",
            "2589:\tlearn: 908755.9050966\ttotal: 3m 28s\tremaining: 33.5s\n",
            "2590:\tlearn: 908459.8006146\ttotal: 3m 28s\tremaining: 33.4s\n",
            "2591:\tlearn: 908085.3323914\ttotal: 3m 28s\tremaining: 33.3s\n",
            "2592:\tlearn: 907586.7200479\ttotal: 3m 28s\tremaining: 33.3s\n",
            "2593:\tlearn: 907204.7428299\ttotal: 3m 28s\tremaining: 33.2s\n",
            "2594:\tlearn: 906871.3468924\ttotal: 3m 28s\tremaining: 33.1s\n",
            "2595:\tlearn: 906700.9178911\ttotal: 3m 28s\tremaining: 33s\n",
            "2596:\tlearn: 906441.0016813\ttotal: 3m 28s\tremaining: 32.9s\n",
            "2597:\tlearn: 906229.3822764\ttotal: 3m 28s\tremaining: 32.8s\n",
            "2598:\tlearn: 905716.4669745\ttotal: 3m 28s\tremaining: 32.7s\n",
            "2599:\tlearn: 905264.7172115\ttotal: 3m 28s\tremaining: 32.6s\n",
            "2600:\tlearn: 904821.8355553\ttotal: 3m 28s\tremaining: 32.6s\n",
            "2601:\tlearn: 904389.8937196\ttotal: 3m 28s\tremaining: 32.5s\n",
            "2602:\tlearn: 903942.0292986\ttotal: 3m 31s\tremaining: 32.8s\n",
            "2603:\tlearn: 903873.6755869\ttotal: 3m 31s\tremaining: 32.8s\n",
            "2604:\tlearn: 903538.8332787\ttotal: 3m 31s\tremaining: 32.6s\n",
            "2605:\tlearn: 903047.9127576\ttotal: 3m 31s\tremaining: 32.5s\n",
            "2606:\tlearn: 902918.9730938\ttotal: 3m 31s\tremaining: 32.4s\n",
            "2607:\tlearn: 902490.0584545\ttotal: 3m 34s\tremaining: 32.7s\n",
            "2608:\tlearn: 902239.3629736\ttotal: 3m 34s\tremaining: 32.7s\n",
            "2609:\tlearn: 902034.1593367\ttotal: 3m 34s\tremaining: 32.6s\n",
            "2610:\tlearn: 901849.6334389\ttotal: 3m 34s\tremaining: 32.5s\n",
            "2611:\tlearn: 901508.4707078\ttotal: 3m 34s\tremaining: 32.4s\n",
            "2612:\tlearn: 901373.7170436\ttotal: 3m 34s\tremaining: 32.3s\n",
            "2613:\tlearn: 901021.9283099\ttotal: 3m 34s\tremaining: 32.2s\n",
            "2614:\tlearn: 900569.3095539\ttotal: 3m 34s\tremaining: 32.1s\n",
            "2615:\tlearn: 900220.6552401\ttotal: 3m 34s\tremaining: 32.1s\n",
            "2616:\tlearn: 899719.4364964\ttotal: 3m 34s\tremaining: 31.9s\n",
            "2617:\tlearn: 899369.8008936\ttotal: 3m 34s\tremaining: 31.8s\n",
            "2618:\tlearn: 899147.4110373\ttotal: 3m 34s\tremaining: 31.8s\n",
            "2619:\tlearn: 898767.1617574\ttotal: 3m 37s\tremaining: 32.1s\n",
            "2620:\tlearn: 898483.2278208\ttotal: 3m 37s\tremaining: 32s\n",
            "2621:\tlearn: 897952.7847703\ttotal: 3m 37s\tremaining: 32s\n",
            "2622:\tlearn: 897398.3840067\ttotal: 3m 37s\tremaining: 31.8s\n",
            "2623:\tlearn: 896712.7352455\ttotal: 3m 37s\tremaining: 31.7s\n",
            "2624:\tlearn: 896345.6204918\ttotal: 3m 40s\tremaining: 32s\n",
            "2625:\tlearn: 896042.6461638\ttotal: 3m 40s\tremaining: 31.9s\n",
            "2626:\tlearn: 895826.7621105\ttotal: 3m 40s\tremaining: 31.8s\n",
            "2627:\tlearn: 895632.8600718\ttotal: 3m 40s\tremaining: 31.7s\n",
            "2628:\tlearn: 895372.0207376\ttotal: 3m 40s\tremaining: 31.6s\n",
            "2629:\tlearn: 894980.2764205\ttotal: 3m 40s\tremaining: 31.5s\n",
            "2630:\tlearn: 894730.8407372\ttotal: 3m 40s\tremaining: 31.4s\n",
            "2631:\tlearn: 894376.3434352\ttotal: 3m 40s\tremaining: 31.3s\n",
            "2632:\tlearn: 894101.5298791\ttotal: 3m 40s\tremaining: 31.3s\n",
            "2633:\tlearn: 893522.7642902\ttotal: 3m 40s\tremaining: 31.2s\n",
            "2634:\tlearn: 893333.3117798\ttotal: 3m 40s\tremaining: 31.1s\n",
            "2635:\tlearn: 892650.0049490\ttotal: 3m 40s\tremaining: 31s\n",
            "2636:\tlearn: 892480.0871508\ttotal: 3m 40s\tremaining: 30.9s\n",
            "2637:\tlearn: 892174.0382223\ttotal: 3m 40s\tremaining: 30.8s\n",
            "2638:\tlearn: 891891.6556174\ttotal: 3m 40s\tremaining: 30.7s\n",
            "2639:\tlearn: 891521.4561458\ttotal: 3m 40s\tremaining: 30.6s\n",
            "2640:\tlearn: 891164.9994492\ttotal: 3m 40s\tremaining: 30.5s\n",
            "2641:\tlearn: 890856.3276257\ttotal: 3m 40s\tremaining: 30.4s\n",
            "2642:\tlearn: 890651.4035487\ttotal: 3m 40s\tremaining: 30.3s\n",
            "2643:\tlearn: 890213.5281903\ttotal: 3m 40s\tremaining: 30.2s\n",
            "2644:\tlearn: 889704.1301887\ttotal: 3m 40s\tremaining: 30.2s\n",
            "2645:\tlearn: 889416.7140841\ttotal: 3m 40s\tremaining: 30.1s\n",
            "2646:\tlearn: 889260.5357243\ttotal: 3m 40s\tremaining: 30s\n",
            "2647:\tlearn: 888924.4797655\ttotal: 3m 40s\tremaining: 29.9s\n",
            "2648:\tlearn: 888507.4679762\ttotal: 3m 41s\tremaining: 29.8s\n",
            "2649:\tlearn: 888247.3601179\ttotal: 3m 41s\tremaining: 29.7s\n",
            "2650:\tlearn: 887964.2697812\ttotal: 3m 41s\tremaining: 29.6s\n",
            "2651:\tlearn: 887356.9608105\ttotal: 3m 41s\tremaining: 29.5s\n",
            "2652:\tlearn: 886965.0659498\ttotal: 3m 41s\tremaining: 29.4s\n",
            "2653:\tlearn: 886730.8694113\ttotal: 3m 41s\tremaining: 29.3s\n",
            "2654:\tlearn: 886379.4113224\ttotal: 3m 41s\tremaining: 29.2s\n",
            "2655:\tlearn: 885883.8429626\ttotal: 3m 41s\tremaining: 29.2s\n",
            "2656:\tlearn: 885607.6971793\ttotal: 3m 41s\tremaining: 29.1s\n",
            "2657:\tlearn: 885342.6525743\ttotal: 3m 41s\tremaining: 29s\n",
            "2658:\tlearn: 885170.1562122\ttotal: 3m 41s\tremaining: 28.9s\n",
            "2659:\tlearn: 884957.1067855\ttotal: 3m 41s\tremaining: 28.8s\n",
            "2660:\tlearn: 884790.3177636\ttotal: 3m 41s\tremaining: 28.7s\n",
            "2661:\tlearn: 884439.6792145\ttotal: 3m 41s\tremaining: 28.6s\n",
            "2662:\tlearn: 884041.9156845\ttotal: 3m 41s\tremaining: 28.5s\n",
            "2663:\tlearn: 883514.0445607\ttotal: 3m 41s\tremaining: 28.4s\n",
            "2664:\tlearn: 883123.2767757\ttotal: 3m 41s\tremaining: 28.3s\n",
            "2665:\tlearn: 882952.4264415\ttotal: 3m 41s\tremaining: 28.2s\n",
            "2666:\tlearn: 882240.0701047\ttotal: 3m 41s\tremaining: 28.2s\n",
            "2667:\tlearn: 881890.1716112\ttotal: 3m 41s\tremaining: 28.1s\n",
            "2668:\tlearn: 881362.5866489\ttotal: 3m 41s\tremaining: 28s\n",
            "2669:\tlearn: 881079.2066785\ttotal: 3m 41s\tremaining: 27.9s\n",
            "2670:\tlearn: 880757.6870748\ttotal: 3m 41s\tremaining: 27.8s\n",
            "2671:\tlearn: 880658.3421097\ttotal: 3m 41s\tremaining: 27.7s\n",
            "2672:\tlearn: 880276.4543204\ttotal: 3m 41s\tremaining: 27.6s\n",
            "2673:\tlearn: 880035.1673479\ttotal: 3m 41s\tremaining: 27.5s\n",
            "2674:\tlearn: 879720.4572983\ttotal: 3m 41s\tremaining: 27.4s\n",
            "2675:\tlearn: 879433.5829672\ttotal: 3m 41s\tremaining: 27.3s\n",
            "2676:\tlearn: 879114.5922514\ttotal: 3m 42s\tremaining: 27.2s\n",
            "2677:\tlearn: 878785.4401004\ttotal: 3m 42s\tremaining: 27.2s\n",
            "2678:\tlearn: 878470.6197951\ttotal: 3m 42s\tremaining: 27.1s\n",
            "2679:\tlearn: 878169.5522505\ttotal: 3m 42s\tremaining: 27s\n",
            "2680:\tlearn: 878034.9602681\ttotal: 3m 42s\tremaining: 26.9s\n",
            "2681:\tlearn: 877842.2009159\ttotal: 3m 42s\tremaining: 26.8s\n",
            "2682:\tlearn: 877721.7141503\ttotal: 3m 42s\tremaining: 26.7s\n",
            "2683:\tlearn: 877363.3194583\ttotal: 3m 42s\tremaining: 26.6s\n",
            "2684:\tlearn: 876715.4639389\ttotal: 3m 42s\tremaining: 26.5s\n",
            "2685:\tlearn: 876170.1202944\ttotal: 3m 42s\tremaining: 26.4s\n",
            "2686:\tlearn: 875828.7431522\ttotal: 3m 42s\tremaining: 26.3s\n",
            "2687:\tlearn: 875372.2846337\ttotal: 3m 42s\tremaining: 26.3s\n",
            "2688:\tlearn: 875228.9925182\ttotal: 3m 42s\tremaining: 26.2s\n",
            "2689:\tlearn: 874965.8789428\ttotal: 3m 42s\tremaining: 26.1s\n",
            "2690:\tlearn: 874673.0554957\ttotal: 3m 42s\tremaining: 26s\n",
            "2691:\tlearn: 874413.1317759\ttotal: 3m 45s\tremaining: 26.2s\n",
            "2692:\tlearn: 873936.7840036\ttotal: 3m 45s\tremaining: 26.2s\n",
            "2693:\tlearn: 873706.6120308\ttotal: 3m 45s\tremaining: 26s\n",
            "2694:\tlearn: 873487.5933166\ttotal: 3m 45s\tremaining: 25.9s\n",
            "2695:\tlearn: 873228.3127174\ttotal: 3m 47s\tremaining: 26.2s\n",
            "2696:\tlearn: 872951.4715456\ttotal: 3m 47s\tremaining: 26.1s\n",
            "2697:\tlearn: 872450.7950390\ttotal: 3m 47s\tremaining: 26s\n",
            "2698:\tlearn: 872126.5757557\ttotal: 3m 48s\tremaining: 25.9s\n",
            "2699:\tlearn: 871877.9338401\ttotal: 3m 48s\tremaining: 25.8s\n",
            "2700:\tlearn: 871522.8386086\ttotal: 3m 48s\tremaining: 25.7s\n",
            "2701:\tlearn: 871277.8563186\ttotal: 3m 48s\tremaining: 25.6s\n",
            "2702:\tlearn: 871087.2995169\ttotal: 3m 48s\tremaining: 25.5s\n",
            "2703:\tlearn: 870889.7976359\ttotal: 3m 48s\tremaining: 25.4s\n",
            "2704:\tlearn: 870743.4135324\ttotal: 3m 48s\tremaining: 25.3s\n",
            "2705:\tlearn: 870444.3521747\ttotal: 3m 48s\tremaining: 25.2s\n",
            "2706:\tlearn: 869812.2058711\ttotal: 3m 48s\tremaining: 25.1s\n",
            "2707:\tlearn: 869464.6997779\ttotal: 3m 48s\tremaining: 25.1s\n",
            "2708:\tlearn: 869008.6576265\ttotal: 3m 48s\tremaining: 25.1s\n",
            "2709:\tlearn: 868748.2789905\ttotal: 3m 48s\tremaining: 24.9s\n",
            "2710:\tlearn: 868643.3388752\ttotal: 3m 48s\tremaining: 24.8s\n",
            "2711:\tlearn: 868355.3934531\ttotal: 3m 48s\tremaining: 24.7s\n",
            "2712:\tlearn: 868185.0651851\ttotal: 3m 51s\tremaining: 24.9s\n",
            "2713:\tlearn: 868006.1637807\ttotal: 3m 51s\tremaining: 24.8s\n",
            "2714:\tlearn: 867712.7665020\ttotal: 3m 51s\tremaining: 24.7s\n",
            "2715:\tlearn: 867041.9023673\ttotal: 3m 51s\tremaining: 24.6s\n",
            "2716:\tlearn: 866689.5920612\ttotal: 3m 51s\tremaining: 24.5s\n",
            "2717:\tlearn: 866437.6384312\ttotal: 3m 51s\tremaining: 24.4s\n",
            "2718:\tlearn: 866104.6895979\ttotal: 3m 51s\tremaining: 24.3s\n",
            "2719:\tlearn: 865997.5122352\ttotal: 3m 51s\tremaining: 24.2s\n",
            "2720:\tlearn: 865819.9634194\ttotal: 3m 51s\tremaining: 24.2s\n",
            "2721:\tlearn: 865678.3686307\ttotal: 3m 51s\tremaining: 24.1s\n",
            "2722:\tlearn: 865385.7868549\ttotal: 3m 51s\tremaining: 24s\n",
            "2723:\tlearn: 865116.0003242\ttotal: 3m 51s\tremaining: 23.9s\n",
            "2724:\tlearn: 864780.8583985\ttotal: 3m 51s\tremaining: 23.8s\n",
            "2725:\tlearn: 864448.4040776\ttotal: 3m 51s\tremaining: 23.7s\n",
            "2726:\tlearn: 864058.7734090\ttotal: 3m 51s\tremaining: 23.6s\n",
            "2727:\tlearn: 863868.2743770\ttotal: 3m 51s\tremaining: 23.5s\n",
            "2728:\tlearn: 863562.2763599\ttotal: 3m 51s\tremaining: 23.4s\n",
            "2729:\tlearn: 863300.5012486\ttotal: 3m 51s\tremaining: 23.3s\n",
            "2730:\tlearn: 862896.6455515\ttotal: 3m 51s\tremaining: 23.2s\n",
            "2731:\tlearn: 862645.0348821\ttotal: 3m 51s\tremaining: 23.1s\n",
            "2732:\tlearn: 862299.6489571\ttotal: 3m 51s\tremaining: 23.1s\n",
            "2733:\tlearn: 861826.6442121\ttotal: 3m 51s\tremaining: 23s\n",
            "2734:\tlearn: 861584.3246347\ttotal: 3m 51s\tremaining: 22.9s\n",
            "2735:\tlearn: 861370.4290600\ttotal: 3m 52s\tremaining: 22.8s\n",
            "2736:\tlearn: 861199.1879733\ttotal: 3m 52s\tremaining: 22.8s\n",
            "2737:\tlearn: 860852.2581094\ttotal: 3m 52s\tremaining: 22.6s\n",
            "2738:\tlearn: 860581.0272967\ttotal: 3m 52s\tremaining: 22.5s\n",
            "2739:\tlearn: 860283.0065125\ttotal: 3m 54s\tremaining: 22.7s\n",
            "2740:\tlearn: 859981.7471860\ttotal: 3m 54s\tremaining: 22.6s\n",
            "2741:\tlearn: 859679.8049177\ttotal: 3m 54s\tremaining: 22.5s\n",
            "2742:\tlearn: 859417.0789125\ttotal: 3m 54s\tremaining: 22.4s\n",
            "2743:\tlearn: 859113.3363507\ttotal: 3m 54s\tremaining: 22.3s\n",
            "2744:\tlearn: 859009.0255258\ttotal: 3m 54s\tremaining: 22.2s\n",
            "2745:\tlearn: 858747.5690448\ttotal: 3m 54s\tremaining: 22.1s\n",
            "2746:\tlearn: 858499.2031806\ttotal: 3m 54s\tremaining: 22s\n",
            "2747:\tlearn: 858233.8434506\ttotal: 3m 55s\tremaining: 21.9s\n",
            "2748:\tlearn: 857943.9965892\ttotal: 3m 55s\tremaining: 21.9s\n",
            "2749:\tlearn: 857493.6154308\ttotal: 3m 55s\tremaining: 21.8s\n",
            "2750:\tlearn: 857324.2009032\ttotal: 3m 55s\tremaining: 21.7s\n",
            "2751:\tlearn: 856937.9243781\ttotal: 3m 55s\tremaining: 21.7s\n",
            "2752:\tlearn: 856712.4820169\ttotal: 3m 57s\tremaining: 21.7s\n",
            "2753:\tlearn: 856485.3726133\ttotal: 3m 57s\tremaining: 21.6s\n",
            "2754:\tlearn: 856322.0832511\ttotal: 3m 57s\tremaining: 21.5s\n",
            "2755:\tlearn: 856053.8594612\ttotal: 3m 57s\tremaining: 21.5s\n",
            "2756:\tlearn: 855670.5518389\ttotal: 3m 57s\tremaining: 21.4s\n",
            "2757:\tlearn: 855385.6155354\ttotal: 3m 57s\tremaining: 21.3s\n",
            "2758:\tlearn: 855095.4336843\ttotal: 3m 58s\tremaining: 21.2s\n",
            "2759:\tlearn: 854721.5743058\ttotal: 3m 58s\tremaining: 21.1s\n",
            "2760:\tlearn: 854333.0253303\ttotal: 3m 58s\tremaining: 21.1s\n",
            "2761:\tlearn: 853950.7315403\ttotal: 4m\tremaining: 21.1s\n",
            "2762:\tlearn: 853591.1809771\ttotal: 4m\tremaining: 21s\n",
            "2763:\tlearn: 853371.6927434\ttotal: 4m\tremaining: 20.9s\n",
            "2764:\tlearn: 853084.8571055\ttotal: 4m\tremaining: 20.9s\n",
            "2765:\tlearn: 852642.2934093\ttotal: 4m\tremaining: 20.8s\n",
            "2766:\tlearn: 852480.6103214\ttotal: 4m\tremaining: 20.7s\n",
            "2767:\tlearn: 852114.3301531\ttotal: 4m\tremaining: 20.6s\n",
            "2768:\tlearn: 851559.7888196\ttotal: 4m 1s\tremaining: 20.5s\n",
            "2769:\tlearn: 851117.6746747\ttotal: 4m 1s\tremaining: 20.4s\n",
            "2770:\tlearn: 850890.3748906\ttotal: 4m 1s\tremaining: 20.3s\n",
            "2771:\tlearn: 850772.7642313\ttotal: 4m 1s\tremaining: 20.2s\n",
            "2772:\tlearn: 850451.6180034\ttotal: 4m 1s\tremaining: 20.1s\n",
            "2773:\tlearn: 850030.2489029\ttotal: 4m 1s\tremaining: 20s\n",
            "2774:\tlearn: 849365.8662529\ttotal: 4m 1s\tremaining: 19.9s\n",
            "2775:\tlearn: 849056.0379608\ttotal: 4m 1s\tremaining: 19.8s\n",
            "2776:\tlearn: 848652.8414135\ttotal: 4m 1s\tremaining: 19.7s\n",
            "2777:\tlearn: 848434.2684156\ttotal: 4m 1s\tremaining: 19.6s\n",
            "2778:\tlearn: 848069.0492505\ttotal: 4m 1s\tremaining: 19.6s\n",
            "2779:\tlearn: 847906.8149523\ttotal: 4m 1s\tremaining: 19.5s\n",
            "2780:\tlearn: 847644.7343870\ttotal: 4m 1s\tremaining: 19.4s\n",
            "2781:\tlearn: 847285.3709705\ttotal: 4m 1s\tremaining: 19.3s\n",
            "2782:\tlearn: 846812.4630645\ttotal: 4m 1s\tremaining: 19.2s\n",
            "2783:\tlearn: 846534.2384393\ttotal: 4m 1s\tremaining: 19.2s\n",
            "2784:\tlearn: 846333.0684484\ttotal: 4m 4s\tremaining: 19.2s\n",
            "2785:\tlearn: 845567.0899211\ttotal: 4m 4s\tremaining: 19.1s\n",
            "2786:\tlearn: 845493.8116587\ttotal: 4m 4s\tremaining: 19s\n",
            "2787:\tlearn: 845268.8983162\ttotal: 4m 4s\tremaining: 18.9s\n",
            "2788:\tlearn: 845068.5729468\ttotal: 4m 4s\tremaining: 18.8s\n",
            "2789:\tlearn: 844919.3432856\ttotal: 4m 4s\tremaining: 18.7s\n",
            "2790:\tlearn: 844524.1330276\ttotal: 4m 4s\tremaining: 18.6s\n",
            "2791:\tlearn: 844376.8002525\ttotal: 4m 4s\tremaining: 18.6s\n",
            "2792:\tlearn: 844212.9119160\ttotal: 4m 4s\tremaining: 18.5s\n",
            "2793:\tlearn: 843982.7439338\ttotal: 4m 4s\tremaining: 18.4s\n",
            "2794:\tlearn: 843715.3671817\ttotal: 4m 4s\tremaining: 18.3s\n",
            "2795:\tlearn: 843362.5086937\ttotal: 4m 4s\tremaining: 18.2s\n",
            "2796:\tlearn: 842921.0532571\ttotal: 4m 4s\tremaining: 18.1s\n",
            "2797:\tlearn: 842580.0595965\ttotal: 4m 4s\tremaining: 18s\n",
            "2798:\tlearn: 842361.4324794\ttotal: 4m 4s\tremaining: 17.9s\n",
            "2799:\tlearn: 842119.6889521\ttotal: 4m 4s\tremaining: 17.8s\n",
            "2800:\tlearn: 841787.4214866\ttotal: 4m 4s\tremaining: 17.7s\n",
            "2801:\tlearn: 841216.4554695\ttotal: 4m 4s\tremaining: 17.6s\n",
            "2802:\tlearn: 840929.0562740\ttotal: 4m 4s\tremaining: 17.5s\n",
            "2803:\tlearn: 840605.3522838\ttotal: 4m 4s\tremaining: 17.4s\n",
            "2804:\tlearn: 840330.5058364\ttotal: 4m 4s\tremaining: 17.4s\n",
            "2805:\tlearn: 840160.3516150\ttotal: 4m 4s\tremaining: 17.3s\n",
            "2806:\tlearn: 839772.5646765\ttotal: 4m 5s\tremaining: 17.2s\n",
            "2807:\tlearn: 839501.6234480\ttotal: 4m 5s\tremaining: 17.1s\n",
            "2808:\tlearn: 839060.0050929\ttotal: 4m 5s\tremaining: 17s\n",
            "2809:\tlearn: 838743.4085039\ttotal: 4m 5s\tremaining: 16.9s\n",
            "2810:\tlearn: 838413.5041908\ttotal: 4m 5s\tremaining: 16.8s\n",
            "2811:\tlearn: 838203.1850636\ttotal: 4m 5s\tremaining: 16.7s\n",
            "2812:\tlearn: 838071.1257150\ttotal: 4m 5s\tremaining: 16.6s\n",
            "2813:\tlearn: 837753.2238160\ttotal: 4m 5s\tremaining: 16.5s\n",
            "2814:\tlearn: 837619.9090579\ttotal: 4m 5s\tremaining: 16.4s\n",
            "2815:\tlearn: 836969.7085840\ttotal: 4m 5s\tremaining: 16.3s\n",
            "2816:\tlearn: 836717.9718980\ttotal: 4m 5s\tremaining: 16.2s\n",
            "2817:\tlearn: 836597.8140882\ttotal: 4m 5s\tremaining: 16.1s\n",
            "2818:\tlearn: 836410.6801296\ttotal: 4m 5s\tremaining: 16.1s\n",
            "2819:\tlearn: 836125.8065612\ttotal: 4m 5s\tremaining: 16s\n",
            "2820:\tlearn: 835916.5889389\ttotal: 4m 5s\tremaining: 15.9s\n",
            "2821:\tlearn: 835723.6792924\ttotal: 4m 5s\tremaining: 15.8s\n",
            "2822:\tlearn: 835533.7315353\ttotal: 4m 5s\tremaining: 15.7s\n",
            "2823:\tlearn: 835295.7406167\ttotal: 4m 5s\tremaining: 15.6s\n",
            "2824:\tlearn: 835119.5562883\ttotal: 4m 5s\tremaining: 15.5s\n",
            "2825:\tlearn: 834822.7637178\ttotal: 4m 5s\tremaining: 15.4s\n",
            "2826:\tlearn: 834563.8967507\ttotal: 4m 5s\tremaining: 15.3s\n",
            "2827:\tlearn: 834401.5745947\ttotal: 4m 5s\tremaining: 15.2s\n",
            "2828:\tlearn: 834058.2958983\ttotal: 4m 5s\tremaining: 15.1s\n",
            "2829:\tlearn: 833854.1031981\ttotal: 4m 5s\tremaining: 15s\n",
            "2830:\tlearn: 833646.2351312\ttotal: 4m 5s\tremaining: 15s\n",
            "2831:\tlearn: 833371.6787371\ttotal: 4m 5s\tremaining: 14.9s\n",
            "2832:\tlearn: 832785.0683129\ttotal: 4m 6s\tremaining: 14.8s\n",
            "2833:\tlearn: 832459.2658131\ttotal: 4m 6s\tremaining: 14.7s\n",
            "2834:\tlearn: 832185.1387660\ttotal: 4m 6s\tremaining: 14.6s\n",
            "2835:\tlearn: 831904.2623513\ttotal: 4m 6s\tremaining: 14.5s\n",
            "2836:\tlearn: 831600.6029163\ttotal: 4m 6s\tremaining: 14.4s\n",
            "2837:\tlearn: 831368.3302620\ttotal: 4m 6s\tremaining: 14.3s\n",
            "2838:\tlearn: 830888.8996526\ttotal: 4m 6s\tremaining: 14.2s\n",
            "2839:\tlearn: 830578.4557524\ttotal: 4m 6s\tremaining: 14.1s\n",
            "2840:\tlearn: 830217.4430786\ttotal: 4m 6s\tremaining: 14s\n",
            "2841:\tlearn: 829793.5110995\ttotal: 4m 6s\tremaining: 13.9s\n",
            "2842:\tlearn: 829587.4359438\ttotal: 4m 6s\tremaining: 13.9s\n",
            "2843:\tlearn: 829242.1249190\ttotal: 4m 6s\tremaining: 13.8s\n",
            "2844:\tlearn: 828707.2386846\ttotal: 4m 6s\tremaining: 13.7s\n",
            "2845:\tlearn: 828521.6545856\ttotal: 4m 6s\tremaining: 13.6s\n",
            "2846:\tlearn: 828282.1969173\ttotal: 4m 6s\tremaining: 13.5s\n",
            "2847:\tlearn: 828069.8355619\ttotal: 4m 6s\tremaining: 13.4s\n",
            "2848:\tlearn: 828013.3095807\ttotal: 4m 6s\tremaining: 13.3s\n",
            "2849:\tlearn: 827751.6757915\ttotal: 4m 6s\tremaining: 13.2s\n",
            "2850:\tlearn: 827517.7582421\ttotal: 4m 6s\tremaining: 13.1s\n",
            "2851:\tlearn: 827366.2599972\ttotal: 4m 6s\tremaining: 13s\n",
            "2852:\tlearn: 827016.4868104\ttotal: 4m 6s\tremaining: 12.9s\n",
            "2853:\tlearn: 826546.9667081\ttotal: 4m 6s\tremaining: 12.9s\n",
            "2854:\tlearn: 826076.1279583\ttotal: 4m 6s\tremaining: 12.8s\n",
            "2855:\tlearn: 825820.5153644\ttotal: 4m 6s\tremaining: 12.7s\n",
            "2856:\tlearn: 825171.7156111\ttotal: 4m 6s\tremaining: 12.6s\n",
            "2857:\tlearn: 824877.4776204\ttotal: 4m 6s\tremaining: 12.5s\n",
            "2858:\tlearn: 824656.2018328\ttotal: 4m 6s\tremaining: 12.4s\n",
            "2859:\tlearn: 824304.0508024\ttotal: 4m 7s\tremaining: 12.3s\n",
            "2860:\tlearn: 823833.9127872\ttotal: 4m 7s\tremaining: 12.2s\n",
            "2861:\tlearn: 823677.5843622\ttotal: 4m 7s\tremaining: 12.1s\n",
            "2862:\tlearn: 823410.3070624\ttotal: 4m 7s\tremaining: 12s\n",
            "2863:\tlearn: 823315.3046288\ttotal: 4m 7s\tremaining: 12s\n",
            "2864:\tlearn: 822951.3465762\ttotal: 4m 7s\tremaining: 11.9s\n",
            "2865:\tlearn: 822391.2736762\ttotal: 4m 7s\tremaining: 11.8s\n",
            "2866:\tlearn: 822077.9711940\ttotal: 4m 7s\tremaining: 11.7s\n",
            "2867:\tlearn: 821707.1559637\ttotal: 4m 7s\tremaining: 11.6s\n",
            "2868:\tlearn: 821402.9226038\ttotal: 4m 7s\tremaining: 11.5s\n",
            "2869:\tlearn: 821274.0899562\ttotal: 4m 7s\tremaining: 11.4s\n",
            "2870:\tlearn: 820855.0004307\ttotal: 4m 7s\tremaining: 11.3s\n",
            "2871:\tlearn: 820568.5631878\ttotal: 4m 7s\tremaining: 11.2s\n",
            "2872:\tlearn: 820301.1237305\ttotal: 4m 7s\tremaining: 11.1s\n",
            "2873:\tlearn: 819989.5248665\ttotal: 4m 7s\tremaining: 11.1s\n",
            "2874:\tlearn: 819799.8717560\ttotal: 4m 7s\tremaining: 11s\n",
            "2875:\tlearn: 819599.2983649\ttotal: 4m 7s\tremaining: 10.9s\n",
            "2876:\tlearn: 819035.2902483\ttotal: 4m 7s\tremaining: 10.8s\n",
            "2877:\tlearn: 818762.2644822\ttotal: 4m 7s\tremaining: 10.7s\n",
            "2878:\tlearn: 818267.5422262\ttotal: 4m 7s\tremaining: 10.6s\n",
            "2879:\tlearn: 818001.8061469\ttotal: 4m 7s\tremaining: 10.5s\n",
            "2880:\tlearn: 817578.4924420\ttotal: 4m 7s\tremaining: 10.4s\n",
            "2881:\tlearn: 817219.2730079\ttotal: 4m 7s\tremaining: 10.3s\n",
            "2882:\tlearn: 816919.4169625\ttotal: 4m 7s\tremaining: 10.2s\n",
            "2883:\tlearn: 816630.5936832\ttotal: 4m 7s\tremaining: 10.2s\n",
            "2884:\tlearn: 816178.6262945\ttotal: 4m 7s\tremaining: 10.1s\n",
            "2885:\tlearn: 815897.5188906\ttotal: 4m 7s\tremaining: 9.97s\n",
            "2886:\tlearn: 815578.8109409\ttotal: 4m 8s\tremaining: 9.88s\n",
            "2887:\tlearn: 815164.4534510\ttotal: 4m 8s\tremaining: 9.8s\n",
            "2888:\tlearn: 814603.8108047\ttotal: 4m 8s\tremaining: 9.71s\n",
            "2889:\tlearn: 814217.5861065\ttotal: 4m 8s\tremaining: 9.62s\n",
            "2890:\tlearn: 813891.4313044\ttotal: 4m 8s\tremaining: 9.53s\n",
            "2891:\tlearn: 813607.0996712\ttotal: 4m 8s\tremaining: 9.44s\n",
            "2892:\tlearn: 812968.9430029\ttotal: 4m 8s\tremaining: 9.35s\n",
            "2893:\tlearn: 812745.2815743\ttotal: 4m 8s\tremaining: 9.26s\n",
            "2894:\tlearn: 812437.0905960\ttotal: 4m 8s\tremaining: 9.17s\n",
            "2895:\tlearn: 812152.4736460\ttotal: 4m 8s\tremaining: 9.08s\n",
            "2896:\tlearn: 811852.3745668\ttotal: 4m 8s\tremaining: 8.99s\n",
            "2897:\tlearn: 811564.4510263\ttotal: 4m 8s\tremaining: 8.9s\n",
            "2898:\tlearn: 811371.3763283\ttotal: 4m 8s\tremaining: 8.81s\n",
            "2899:\tlearn: 811005.7507068\ttotal: 4m 8s\tremaining: 8.72s\n",
            "2900:\tlearn: 810549.5760360\ttotal: 4m 8s\tremaining: 8.64s\n",
            "2901:\tlearn: 810100.5983413\ttotal: 4m 8s\tremaining: 8.55s\n",
            "2902:\tlearn: 809721.8888235\ttotal: 4m 8s\tremaining: 8.46s\n",
            "2903:\tlearn: 809258.3923239\ttotal: 4m 8s\tremaining: 8.37s\n",
            "2904:\tlearn: 808998.9155433\ttotal: 4m 8s\tremaining: 8.28s\n",
            "2905:\tlearn: 808741.2552843\ttotal: 4m 8s\tremaining: 8.19s\n",
            "2906:\tlearn: 808438.7530896\ttotal: 4m 8s\tremaining: 8.1s\n",
            "2907:\tlearn: 808174.1161899\ttotal: 4m 8s\tremaining: 8.01s\n",
            "2908:\tlearn: 807748.2290547\ttotal: 4m 8s\tremaining: 7.93s\n",
            "2909:\tlearn: 807291.6920174\ttotal: 4m 8s\tremaining: 7.84s\n",
            "2910:\tlearn: 807057.3172419\ttotal: 4m 8s\tremaining: 7.75s\n",
            "2911:\tlearn: 806991.1881864\ttotal: 4m 8s\tremaining: 7.66s\n",
            "2912:\tlearn: 806775.0366734\ttotal: 4m 8s\tremaining: 7.57s\n",
            "2913:\tlearn: 806542.9109695\ttotal: 4m 9s\tremaining: 7.48s\n",
            "2914:\tlearn: 806191.4886676\ttotal: 4m 9s\tremaining: 7.39s\n",
            "2915:\tlearn: 805931.7897307\ttotal: 4m 9s\tremaining: 7.39s\n",
            "2916:\tlearn: 805590.8814774\ttotal: 4m 11s\tremaining: 7.29s\n",
            "2917:\tlearn: 805428.1845844\ttotal: 4m 11s\tremaining: 7.21s\n",
            "2918:\tlearn: 804833.3592932\ttotal: 4m 11s\tremaining: 7.12s\n",
            "2919:\tlearn: 804521.8284153\ttotal: 4m 11s\tremaining: 7.03s\n",
            "2920:\tlearn: 804104.0963702\ttotal: 4m 11s\tremaining: 7.03s\n",
            "2921:\tlearn: 803651.8036182\ttotal: 4m 11s\tremaining: 6.85s\n",
            "2922:\tlearn: 803398.3306468\ttotal: 4m 11s\tremaining: 6.76s\n",
            "2923:\tlearn: 803209.2841874\ttotal: 4m 11s\tremaining: 6.67s\n",
            "2924:\tlearn: 802858.5424007\ttotal: 4m 11s\tremaining: 6.58s\n",
            "2925:\tlearn: 802625.9799307\ttotal: 4m 12s\tremaining: 6.49s\n",
            "2926:\tlearn: 802000.4841297\ttotal: 4m 12s\tremaining: 6.4s\n",
            "2927:\tlearn: 801827.6108067\ttotal: 4m 12s\tremaining: 6.32s\n",
            "2928:\tlearn: 801704.3451512\ttotal: 4m 12s\tremaining: 6.23s\n",
            "2929:\tlearn: 801395.7587494\ttotal: 4m 12s\tremaining: 6.14s\n",
            "2930:\tlearn: 801254.3620245\ttotal: 4m 12s\tremaining: 6.05s\n",
            "2931:\tlearn: 801031.7747213\ttotal: 4m 12s\tremaining: 5.96s\n",
            "2932:\tlearn: 800871.4606700\ttotal: 4m 14s\tremaining: 5.93s\n",
            "2933:\tlearn: 800596.7237767\ttotal: 4m 14s\tremaining: 5.93s\n",
            "2934:\tlearn: 799876.5262233\ttotal: 4m 17s\tremaining: 5.81s\n",
            "2935:\tlearn: 799612.8355627\ttotal: 4m 17s\tremaining: 5.72s\n",
            "2936:\tlearn: 799328.4340421\ttotal: 4m 17s\tremaining: 5.63s\n",
            "2937:\tlearn: 798915.4048573\ttotal: 4m 17s\tremaining: 5.54s\n",
            "2938:\tlearn: 798556.7756617\ttotal: 4m 17s\tremaining: 5.45s\n",
            "2939:\tlearn: 798242.3635996\ttotal: 4m 17s\tremaining: 5.45s\n",
            "2940:\tlearn: 798007.7539859\ttotal: 4m 17s\tremaining: 5.27s\n",
            "2941:\tlearn: 797645.3878423\ttotal: 4m 20s\tremaining: 5.23s\n",
            "2942:\tlearn: 797384.5859819\ttotal: 4m 20s\tremaining: 5.14s\n",
            "2943:\tlearn: 797304.3157013\ttotal: 4m 20s\tremaining: 5.05s\n",
            "2944:\tlearn: 797115.9674950\ttotal: 4m 20s\tremaining: 5.05s\n",
            "2945:\tlearn: 796841.0137454\ttotal: 4m 23s\tremaining: 4.92s\n",
            "2946:\tlearn: 796545.3510428\ttotal: 4m 23s\tremaining: 4.83s\n",
            "2947:\tlearn: 796233.2305702\ttotal: 4m 23s\tremaining: 4.83s\n",
            "2948:\tlearn: 795951.1590214\ttotal: 4m 23s\tremaining: 4.64s\n",
            "2949:\tlearn: 795710.3845902\ttotal: 4m 25s\tremaining: 4.6s\n",
            "2950:\tlearn: 795476.0095195\ttotal: 4m 25s\tremaining: 4.5s\n",
            "2951:\tlearn: 795216.8348852\ttotal: 4m 25s\tremaining: 4.41s\n",
            "2952:\tlearn: 794924.4278473\ttotal: 4m 25s\tremaining: 4.32s\n",
            "2953:\tlearn: 794691.4669824\ttotal: 4m 26s\tremaining: 4.22s\n",
            "2954:\tlearn: 794400.1230468\ttotal: 4m 26s\tremaining: 4.13s\n",
            "2955:\tlearn: 794089.2085229\ttotal: 4m 26s\tremaining: 4.04s\n",
            "2956:\tlearn: 793792.3006535\ttotal: 4m 26s\tremaining: 3.95s\n",
            "2957:\tlearn: 793488.4147496\ttotal: 4m 26s\tremaining: 3.85s\n",
            "2958:\tlearn: 793162.2735381\ttotal: 4m 26s\tremaining: 3.76s\n",
            "2959:\tlearn: 793039.8168896\ttotal: 4m 26s\tremaining: 3.67s\n",
            "2960:\tlearn: 792682.5340974\ttotal: 4m 26s\tremaining: 3.58s\n",
            "2961:\tlearn: 792521.3047274\ttotal: 4m 26s\tremaining: 3.48s\n",
            "2962:\tlearn: 792254.5559912\ttotal: 4m 26s\tremaining: 3.39s\n",
            "2963:\tlearn: 792077.6207550\ttotal: 4m 26s\tremaining: 3.3s\n",
            "2964:\tlearn: 791733.8690770\ttotal: 4m 26s\tremaining: 3.21s\n",
            "2965:\tlearn: 791350.5150263\ttotal: 4m 26s\tremaining: 3.12s\n",
            "2966:\tlearn: 791149.7065032\ttotal: 4m 26s\tremaining: 3.02s\n",
            "2967:\tlearn: 790758.2850020\ttotal: 4m 26s\tremaining: 2.93s\n",
            "2968:\tlearn: 790456.9942048\ttotal: 4m 26s\tremaining: 2.84s\n",
            "2969:\tlearn: 790304.3359557\ttotal: 4m 26s\tremaining: 2.75s\n",
            "2970:\tlearn: 790127.4737454\ttotal: 4m 26s\tremaining: 2.65s\n",
            "2971:\tlearn: 789871.8197094\ttotal: 4m 26s\tremaining: 2.56s\n",
            "2972:\tlearn: 789611.3598148\ttotal: 4m 26s\tremaining: 2.47s\n",
            "2973:\tlearn: 789320.4132396\ttotal: 4m 26s\tremaining: 2.38s\n",
            "2974:\tlearn: 788836.2437907\ttotal: 4m 26s\tremaining: 2.29s\n",
            "2975:\tlearn: 788495.7339342\ttotal: 4m 26s\tremaining: 2.19s\n",
            "2976:\tlearn: 788173.6725014\ttotal: 4m 26s\tremaining: 2.1s\n",
            "2977:\tlearn: 787815.5893810\ttotal: 4m 26s\tremaining: 2.01s\n",
            "2978:\tlearn: 787613.6458473\ttotal: 4m 26s\tremaining: 1.92s\n",
            "2979:\tlearn: 787281.3569995\ttotal: 4m 27s\tremaining: 1.83s\n",
            "2980:\tlearn: 786953.7299717\ttotal: 4m 27s\tremaining: 1.74s\n",
            "2981:\tlearn: 786578.0800057\ttotal: 4m 27s\tremaining: 1.64s\n",
            "2982:\tlearn: 786493.9572626\ttotal: 4m 27s\tremaining: 1.55s\n",
            "2983:\tlearn: 786259.9780746\ttotal: 4m 27s\tremaining: 1.46s\n",
            "2984:\tlearn: 785735.1333334\ttotal: 4m 27s\tremaining: 1.37s\n",
            "2985:\tlearn: 784963.2467884\ttotal: 4m 27s\tremaining: 1.28s\n",
            "2986:\tlearn: 784597.7305371\ttotal: 4m 27s\tremaining: 1.19s\n",
            "2987:\tlearn: 784335.6075278\ttotal: 4m 27s\tremaining: 1.09s\n",
            "2988:\tlearn: 784048.6403721\ttotal: 4m 27s\tremaining: 1s\n",
            "2989:\tlearn: 783822.9204715\ttotal: 4m 27s\tremaining: 912ms\n",
            "2990:\tlearn: 783320.3248463\ttotal: 4m 27s\tremaining: 821ms\n",
            "2991:\tlearn: 783071.3240809\ttotal: 4m 27s\tremaining: 729ms\n",
            "2992:\tlearn: 782531.0115404\ttotal: 4m 27s\tremaining: 638ms\n",
            "2993:\tlearn: 782258.9895374\ttotal: 4m 27s\tremaining: 547ms\n",
            "2994:\tlearn: 782114.1841874\ttotal: 4m 27s\tremaining: 456ms\n",
            "2995:\tlearn: 781556.1105010\ttotal: 4m 27s\tremaining: 364ms\n",
            "2996:\tlearn: 781355.3812651\ttotal: 4m 27s\tremaining: 273ms\n",
            "2997:\tlearn: 781175.2052591\ttotal: 4m 27s\tremaining: 182ms\n",
            "2998:\tlearn: 780698.9533371\ttotal: 4m 27s\tremaining: 91ms\n",
            "2999:\tlearn: 780611.0261091\ttotal: 4m 27s\tremaining: 0us\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "steps = 30\n",
        "predictions = forecaster.predict(steps=steps, exog=X_test)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "predictions.plot()"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "submission = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "submission[\"forecast_value\"] = predictions.to_numpy()\n",
        "submission.to_csv(\"skforecast.csv\", index=False)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}
