{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "import os\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from open_clip import create_model_from_pretrained, get_tokenizer\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(14.2849, grad_fn=<ExpBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Initialize the model and tokenizer\n",
        "model, preprocess = create_model_from_pretrained(\"hf-hub:apple/DFN5B-CLIP-ViT-H-14-384\")\n",
        "tokenizer = get_tokenizer(\"ViT-H-14\")\n",
        "model.eval()\n",
        "# Define the folder containing the images\n",
        "image_folder = \"public_test\"\n",
        "\n",
        "labels_list = [\n",
        "    \"Hotel exterior, outdoor area, or building facade\",\n",
        "    \"Hotel room, living space, or bedroom with furniture\",\n",
        "    \"Swimming pool or hotel pool area\",\n",
        "    \"Billiard table, pool table, or game room\",\n",
        "    \"Bathroom with toilet, shower, sink, or bath amenities\",\n",
        "    \"Hotel restaurant, dining room, or eating area\",\n",
        "    \"Hotel lobby, reception area, or entrance hall\",\n",
        "    \"Beachfront, shoreline, or sandy beach area\",\n",
        "    \"Corridors, hallways, or staircases in the hotel\",\n",
        "    \"Food dishes, meals on plates, or table settings\",\n",
        "    \"Conference room, meeting room, or seminar space\",\n",
        "    \"Gym, fitness center, or exercise equipment area\",\n",
        "    \"Balcony view, outdoor balcony, or terrace\",\n",
        "    \"Terrace, patio, or outdoor courtyard\",\n",
        "    \"Spa, sauna, wellness center, or relaxation area\",\n",
        "]\n",
        "text = tokenizer(labels_list, context_length=model.context_length)\n",
        "text_features = model.encode_text(text)\n",
        "# List to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate over each image in the folder\n",
        "for image_file in tqdm(os.listdir(image_folder)):\n",
        "    if image_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "        try:\n",
        "            # Open and preprocess the image\n",
        "            image = Image.open(image_path)\n",
        "            image = preprocess(image).unsqueeze(0)\n",
        "\n",
        "            # Perform inference\n",
        "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "                image_features = model.encode_image(image)\n",
        "\n",
        "                image_features = F.normalize(image_features, dim=-1)\n",
        "                text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "                # Calculate probabilities\n",
        "                logits = model.logit_scale.exp() * image_features @ text_features.T\n",
        "                probs = logits.softmax(dim=-1)\n",
        "                # text_probs = torch.sigmoid(model.logit_scale.exp() *\n",
        "                # image_features @ text_features.T * )\n",
        "\n",
        "            # Zip the labels with their corresponding probabilities\n",
        "            # and store in the results list\n",
        "            zipped_list = list(zip(labels_list, [round(p.item(), 3) for p in probs[0]]))\n",
        "            results.append({\"image_file\": image_file, \"label_probabilities\": zipped_list})\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_file}: {e}\")\n",
        "\n",
        "# # At this point, `results` contains the label probabilities for all processed images\n",
        "# # Example usage: print the results\n",
        "# for result in results:\n",
        "#     print(f\"Label probabilities for {result['image_file']}: {result['label_probabilities']}\")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/seara/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/open_clip/factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf2e231171be4a2aa42ae175a291a35c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1124 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_76353/2471385165.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "results"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'image_file': 'public_test_432.jpg',\n",
              "  'label_probabilities': [('Hotel exterior, outdoor area, or building facade',\n",
              "    0.04),\n",
              "   ('Hotel room, living space, or bedroom with furniture', 0.158),\n",
              "   ('Swimming pool or hotel pool area', 0.033),\n",
              "   ('Billiard table, pool table, or game room', 0.026),\n",
              "   ('Bathroom with toilet, shower, sink, or bath amenities', 0.107),\n",
              "   ('Hotel restaurant, dining room, or eating area', 0.032),\n",
              "   ('Hotel lobby, reception area, or entrance hall', 0.081),\n",
              "   ('Beachfront, shoreline, or sandy beach area', 0.018),\n",
              "   ('Corridors, hallways, or staircases in the hotel', 0.143),\n",
              "   ('Food dishes, meals on plates, or table settings', 0.008),\n",
              "   ('Conference room, meeting room, or seminar space', 0.069),\n",
              "   ('Gym, fitness center, or exercise equipment area', 0.062),\n",
              "   ('Balcony view, outdoor balcony, or terrace', 0.067),\n",
              "   ('Terrace, patio, or outdoor courtyard', 0.048),\n",
              "   ('Spa, sauna, wellness center, or relaxation area', 0.109)]}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "results[\"label_probabilities\"]"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel_probabilities\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Initialize the model and tokenizer, and move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model, preprocess = create_model_from_pretrained(\"hf-hub:apple/DFN5B-CLIP-ViT-H-14-384\")\n",
        "model = model.to(device)\n",
        "tokenizer = get_tokenizer(\"ViT-H-14\")\n",
        "\n",
        "image_folder = \"public_test\"\n",
        "\n",
        "labels_list = [\n",
        "    \"Hotel exterior, outdoor area, or building facade\",\n",
        "    \"Hotel room, living space, or bedroom with furniture\",\n",
        "    \"Swimming pool or hotel pool area\",\n",
        "    \"Billiard table, pool table, or game room\",\n",
        "    \"Bathroom with toilet, shower, sink, or bath amenities\",\n",
        "    \"Hotel restaurant, dining room, or eating area\",\n",
        "    \"Hotel lobby, reception area, or entrance hall\",\n",
        "    \"Beachfront, shoreline, or sandy beach area\",\n",
        "    \"Corridors, hallways, or staircases in the hotel\",\n",
        "    \"Food dishes, meals on plates, or table settings\",\n",
        "    \"Conference room, meeting room, or seminar space\",\n",
        "    \"Gym, fitness center, or exercise equipment area\",\n",
        "    \"Balcony view, outdoor balcony, or terrace\",\n",
        "    \"Terrace, patio, or outdoor courtyard\",\n",
        "    \"Spa, sauna, wellness center, or relaxation area\",\n",
        "]\n",
        "text = tokenizer(labels_list, context_length=model.context_length).to(device)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# List to store the results\n",
        "results = []\n",
        "\n",
        "# Preprocess the images and prepare them in batches\n",
        "\n",
        "\n",
        "def load_images(image_folder):\n",
        "    image_files = []\n",
        "    images = []\n",
        "\n",
        "    for image_file in os.listdir(image_folder):\n",
        "        if image_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            image_path = os.path.join(image_folder, image_file)\n",
        "            try:\n",
        "                image = Image.open(image_path)\n",
        "                image = preprocess(image)\n",
        "                images.append(image)\n",
        "                image_files.append(image_file)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_file}: {e}\")\n",
        "\n",
        "    return image_files, images\n",
        "\n",
        "\n",
        "# Load images and preprocess\n",
        "image_files, images = load_images(image_folder)\n",
        "\n",
        "# Split the images into batches\n",
        "for i in tqdm(range(0, len(images), batch_size)):\n",
        "    batch_images = images[i : i + batch_size]\n",
        "    batch_files = image_files[i : i + batch_size]\n",
        "\n",
        "    # Stack images into a single tensor and move to GPU\n",
        "    batch_images = torch.stack(batch_images).to(device)\n",
        "\n",
        "    # Perform inference on the batch\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "        image_features = model.encode_image(batch_images)\n",
        "        text_features = model.encode_text(text)\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "        text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "        # Calculate probabilities\n",
        "        text_probs = torch.sigmoid(\n",
        "            image_features @ text_features.T * model.logit_scale.exp() + model.logit_bias\n",
        "        )\n",
        "\n",
        "    # Store the results for the current batch\n",
        "    for j, file_name in enumerate(batch_files):\n",
        "        zipped_list = list(zip(labels_list, [round(p.item(), 3) for p in text_probs[j]]))\n",
        "        results.append({\"image_file\": file_name, \"label_probabilities\": zipped_list})\n",
        "\n",
        "# # At this point, `results` contains the label probabilities for all processed images\n",
        "# # Example usage: print the results\n",
        "# for result in results:\n",
        "#     print(f\"Label probabilities for {result['image_file']}: {result['label_probabilities']}\")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/seara/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/open_clip/factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the model and tokenizer, and move model to GPU\u001b[39;00m\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m model, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhf-hub:apple/DFN5B-CLIP-ViT-H-14-384\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-H-14\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/open_clip/factory.py:452\u001b[0m, in \u001b[0;36mcreate_model_from_pretrained\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, return_transform, cache_dir, **model_kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_from_pretrained\u001b[39m(\n\u001b[1;32m    433\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    434\u001b[0m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    448\u001b[0m ):\n\u001b[1;32m    449\u001b[0m     force_preprocess_cfg \u001b[38;5;241m=\u001b[39m merge_preprocess_kwargs(\n\u001b[1;32m    450\u001b[0m         {}, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, interpolation\u001b[38;5;241m=\u001b[39mimage_interpolation, resize_mode\u001b[38;5;241m=\u001b[39mimage_resize_mode)\n\u001b[0;32m--> 452\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_image_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequire_pretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_transform:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m~/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/open_clip/factory.py:315\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, **model_kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_hf_hub_prefix:\n\u001b[1;32m    314\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading pretrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 315\u001b[0m     \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     pretrained_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m require_pretrained \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pretrained_loaded:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# callers of create_model_from_pretrained always expect pretrained weights\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/open_clip/factory.py:154\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(model, checkpoint_path, strict)\u001b[0m\n\u001b[1;32m    151\u001b[0m     load_big_vision_weights(model, checkpoint_path)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 154\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Detect & convert 3rd party state_dicts -> open_clip\u001b[39;00m\n\u001b[1;32m    157\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m convert_state_dict(model, state_dict)\n",
            "File \u001b[0;32m~/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/open_clip/factory.py:129\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_path, map_location)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_state_dict\u001b[39m(checkpoint_path: \u001b[38;5;28mstr\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 129\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint:\n\u001b[1;32m    131\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[0;32m~/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/torch/serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/torch/serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[1;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
            "File \u001b[0;32m~/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/torch/serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
            "File \u001b[0;32m~/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/torch/serialization.py:1457\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "model, preprocess = create_model_from_pretrained(\n",
        "    \"hf-hub:apple/DFN5B-CLIP-ViT-H-14-384\", device=\"cpu\"\n",
        ")\n",
        "tokenizer = get_tokenizer(\"ViT-H-14\")\n",
        "\n",
        "image = Image.open(\n",
        "    urlopen(\n",
        "        \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\"\n",
        "    )\n",
        ")\n",
        "image = preprocess(image).unsqueeze(0)\n",
        "\n",
        "labels_list = [\"a dog\", \"a cat\", \"a donut\", \"a beignet\"]\n",
        "text = tokenizer(labels_list, context_length=model.context_length)\n",
        "\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "    image_features = F.normalize(image_features, dim=-1)\n",
        "    text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "    print(model.logit_scale.exp())\n",
        "\n",
        "    # text_probs = torch.softmax(image_features @ text_features.T * model.logit_scale.exp())\n",
        "\n",
        "zipped_list = list(zip(labels_list, [round(p.item(), 3) for p in text_probs[0]]))\n",
        "print(\"Label probabilities: \", zipped_list)"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/seara/Desktop/Github/mtc-classification/.venv/lib/python3.10/site-packages/open_clip/factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
            "/tmp/ipykernel_58136/432745371.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(14.2849)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'text_probs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mlogit_scale\u001b[38;5;241m.\u001b[39mexp())\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# text_probs = torch.softmax(image_features @ text_features.T * model.logit_scale.exp())\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m zipped_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(labels_list, [\u001b[38;5;28mround\u001b[39m(p\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;241m3\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtext_probs\u001b[49m[\u001b[38;5;241m0\u001b[39m]]))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel probabilities: \u001b[39m\u001b[38;5;124m\"\u001b[39m, zipped_list)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_probs' is not defined"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}
