# RuCode-competition

- [Всероссийский фестиваль по искусственному интеллекту и алгоритмическому программированию](https://rucode.net/final-2024/)
- [Яндекс контест](https://official.contest.yandex.ru/contest/66744)
- Solved 3 out of 5 tasks:
  - Task C - Film genre text classification with the F1-macro evaluation metric
  - Task D - Turnover time-series forecasting for a hotel company
  - Task E - Zero-shot image classification of hotel images **(got third place)**

## Final solution

- Task C - [DeepPavlov/rubert-base-cased](https://huggingface.co/DeepPavlov/rubert-base-cased) fine-tune, with some text preparation
- Task D - Single Catboost model on lagged rolling and time features
- Task E - [Apple CLIP model](https://huggingface.co/apple/DFN5B-CLIP-ViT-H-14-378) with some hand made short prompts

## Data

- [Task C](https://drive.google.com/drive/folders/1kEQicOD3T7P_69_fNAsJKYqtJF8AG_w4)
- [Task D](https://drive.google.com/drive/folders/1Ybyl32vaxFtkD_XBho7j91VEWrBT4yJb?usp=sharing)
- [Task E](https://disk.yandex.ru/d/RzRkRn3Tym-FaA) and [this](https://disk.yandex.ru/d/jWWAhxGes_VJaw)

## Report and thoughts for each task in Russian

### Task C

Решение:

- Колонки сюжета и описание объединили в одну. (На один фильм получилось два объекта в обучающей выборке)
- Длинные тексты разбили на чанки по 150 слов. (На один фильм получилось ещё больше объектов в обучающей выборке)
- В качестве ответа брали средний результат по всем разбивкам для одного фильма
- C помощью spacy и модели "ru_core_news_lg" в тексте заменили различные города, страны и тд. на слово "локация", а имена персонажей на "герой". (Задача NER)
- В текстах заменили название фильмов на "фильм".
- Финальная модель - ai-forever/ruBert-base с max_length = 256, batch_size = 8, num_epochs = 3, lr=0.00001

Что ещё пробовали, но это не улучшило результат:

- Аугментации. Переводили текст на английский и обратно, чтобы увеличить количество объектов для обучения. Модели Helsinki-NLP/opus-mt-ru-en и Helsinki-NLP/opus-mt-en-ru.
- Разное количество слов в чанках 50-300.
- Разный max_length от 128 до 512.
- Разные модели:
  - cointegrated/rubert-tiny2
  - ai-forever/ruRoberta-large
  - ai-forever/ruBert-large
  - DeepPavlov/rubert-base-cased
- Персональный threshold для каждого класса, чтобы распределение классов на тесте совпало с трейном.
- Пробовали применять модели суммаризации текстов, чтобы текст влезал в лимит 512 токенов.
- Разбивать текст не на чанки, а применить метод sliding window (двигаться по тексту окном и классифицировать) и также усреднять.

Общие выводы:

- Сильный дисбаланс классов в задаче
- Мало объектов для тренировки (500) для таких больших моделей как BERTы.
- Метрика F1-macro при дисбалансе классов сильно меняется при небольшом изменении пайплайна. Существуют классы типа "мюзикл", доля которых очень мала в обучающей выборке. При этом в метрике F1-macro этот класс оценивается наравне со всеми. В совокупности с малым размером публичного теста (100) скор прыгал от 0.45 до 0.62 при смене threshold на 0.1.
- Сами фильмы очень тяжело классифицировать даже человеку)). Например, по сюжету иногда невозможно понять, что это мультфильм.

Что ещё можно было попробовать:

- Ещё больший подбор гиперпараметров?
- Lr scheduler, чтобы стабилизировать обучение?
- Аугментации с помощью синонимов?
- Ещё сильнее почистить данные: классические лемматизация, стемминг, удаление стоп-слов?? хотя для BERT моделей это обычно не нужно.
- Ансамбли моделей? Взять BERTы для русского языка, а также перевести тексты на английский и взять BERTы для английского, и всё объединить с весами в зависимости от валидации?
- Применить longformer модели для классификации длинных текстов?

### Task D

Что дало лучший результат на публичном лидерборде:

- Минимальный набор признаков (lagged rolling features и time features) и дефолтный CatBoost, direct предсказание сразу на 30 дней вперед.

Что пробовали:

- Простое экспоненциальное сглаживание
- Арима и Авто-арима с предварительных дифференцированием ряда, удаления сезонностей и тренда, box-cox.
- Auto-ml библиотека AutoGluon для временных рядов
- Отбор признаков и подбор гиперпараметров на кросс-валидации
- Рекурсивное предсказание по одному дню вперёд с помощью skforecast
- Facebook Prophet
- sktime TBATS

Что можно было ещё попробовать:

- Использовать информацию из таблицы, возможно можно было создать признаки, например, для каждого месяца среднее количество заказов, средняя сумма или средняя "звёздочность" отеля
- Различные ансамбли моделей
- DL-модели
- Добавить tsfresh признаки

Что заметили:

- Недельная и годовая сезонность
- Мультипликативный тренд
- Очень маленький горизонт предсказания и всего один временной ряд. Соответственно очень рандомно и сильно меняется RMSE при визуально валидных предсказаниях.
- Ряд очень нестабильный, поэтому для обучения использовали информацию после 2022 года.
- Чем больше признаков, тем хуже RMSE))
- Результат на кросс валидации не коррелирует с лидербордом. Всё очень рандомно

### Task E

Решение:

- Заместо парсинга будем решать задачу zero-shot классификации
- Для каждого класса подобрали самый лучший prompt
- Использовали CLIP модель (apple/DFN5B-CLIP-ViT-H-14-378), чтобы выбрать картинки близкие к каждому промтпу-тексту
- Класс "Остальное" назначали тем картинкам, у которых softmax threshold <= 0.2

Что ещё пробовали:

- Перебирали различные промпты
- Пробовали добавлять доп. классы, чтобы лучше отсеивать 16-ый класс
- Модели:
  - apple/DFN5B-CLIP-ViT-H-14-378
  - openai/clip-vit-large-patch14
  - openai/clip-vit-base-patch16
  - openai/clip-vit-large-patch14-336
  - openai/clip-vit-base-patch32
  - laion/CLIP-ViT-H-14-laion2B-s32B-b79K
  - google/siglip-so400m-patch14-384

Что ещё можно было попробовать:

- Кластеризация-классификация изображений с помощью ResNet с опорными изображениями
- Реально спарсить данные, но на многих сайтах они даны врассыпную и без категорий. Тогда всё равно пришлось бы использовать CLIP.
- Взять тренировочный датасет из [этого репозитория](https://github.com/omega-rg/Hotelify-A-classifier-for-hotel-images), там совпадают многие классы, но не ясно - улучшиться ли качество по сравнению с CLIP.
